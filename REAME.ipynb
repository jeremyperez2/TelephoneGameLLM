{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When LLMs Play the Telephone Game: Cumulative Changes and Attractors in Iterated Cultural Transmissions\n",
    "![introduction_figure](https://github.com/jeremyperez2/TelephoneGameLLM/assets/152488508/6ec97899-f8fd-48bd-a5da-d8fac458bfe8)\n",
    "\n",
    "This repo contains code for the paper [_\"When LLMs Play the Telephone Game: Cumulative Changes and Attractors in Iterated Cultural Transmissions\"_](https://sites.google.com/view/telephone-game-llm)\n",
    "\n",
    "The paper is also accompanied by a [website](https://sites.google.com/view/telephone-game-llm) that features a Data Explorer tool, allowing to look at the simulated data used in the paper. \n",
    "\n",
    "# How to use\n",
    "```\n",
    "conda env create -f telephone_llm.yml\n",
    "conda activate telephone_llm\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## Set-up environment variables\n",
    "\n",
    "- To use OpenAI models with the OpenAI API, set the OPENAI_API_KEY env variable (e.g. in your .bashrc):\n",
    "```\n",
    "export OPENAI_API_KEY=\"<your_key>\"\n",
    "```\n",
    "\n",
    "- To use OpenAI model with the Azure API, set the variables for each model, for example:\n",
    "```\n",
    "export AZURE_OPENAI_ENDPOINT_gpt_35_turbo_0125=\"<your_endpoint>\"\n",
    "export AZURE_OPENAI_KEY_gpt_35_turbo_0125=\"<your_key>\"\n",
    "export AZURE_OPENAI_API_VERSION_gpt_35_turbo_0125=\"<your_version>\"\n",
    "```\n",
    "- To use huggingface models, set the HF_HOME env variable to define your cache directory:\n",
    "\n",
    "```\n",
    "export HF_HOME=\"$HOME/.cache/huggingface\"\n",
    "```\n",
    "\n",
    "- To use huggingface gated models, set the HF_TOKEN env variable\n",
    "\n",
    "```\n",
    "export HF_TOKEN=\"<your_token>\"\n",
    "```\n",
    "\n",
    "## Reproducing the simulations comparing different models and prompts\n",
    "\n",
    "### Run the simulations\n",
    "\n",
    "The script _slurm_script.sh_ takes in argument a _model name_ and a _prompt name_. The model name is the name of LLM that you want to use (e.g. meta-llama/Meta-Llama-3-8B). Prompt names are either \"_rephrase_\", \"_inspiration_\" or \"_continue_\". \n",
    "\n",
    "- Regular machine:\n",
    "```\n",
    "for i in {0..99}; do SLURM_ARRAY_TASK_ID=$i bash slurm_script.sh <model_name> <prompt_name> ; done\n",
    "```\n",
    "This will sequentially launch 100 chains (20 initial texts * 5 seeds) for the specified <_model_name_> and <_prompt_name_>\n",
    "\n",
    "\n",
    "- Slurm-based machine:\n",
    "```\n",
    "sbatch slurm_script.sh <model_name> <prompt_name>\n",
    "```\n",
    "\n",
    "Generated texts will be stored in Results/<_model_name_>/<_prompt_name_>\n",
    "\n",
    "### Run the analyses\n",
    "\n",
    "- Perform the analyses:\n",
    "  \n",
    "  After running simulations for several models, you can compare them by running:\n",
    "  \n",
    "  ```\n",
    "  python3 run_all_analyses.py -m <_model_name1_> <_model_name2_> <_model_name3_> <...> -sn data-for-plotting\n",
    "  ```\n",
    "\n",
    "  This will create a file 'data-for-plotting/all_data.pkl' that can be used for generating figures and running the statistical tests\n",
    "\n",
    "### Generate the figures\n",
    " ```\n",
    "cd plots\n",
    "python3 plot_all_figures.py\n",
    " ```\n",
    "Figures will be stored in Figures/figures_<current_date>\n",
    "\n",
    "- Run the statistical models\n",
    "\n",
    "```\n",
    "  python3 stat_models.py\n",
    "  ```\n",
    "\n",
    "  Figures and tables will be stored in Results/StatModels<current_date>\n",
    "\n",
    "\n",
    "\n",
    "## Reproducing the experiments on effect of temperature\n",
    "\n",
    "### Run the simulations\n",
    "- Regular machine:\n",
    "```\n",
    "for i in {0..99}; do SLURM_ARRAY_TASK_ID=$i bash slurm_script_temperature.sh <model_name> <prompt_name> ; done\n",
    "```\n",
    "This will sequentially launch 100 chains (20 initial texts * 5 seeds) for the specified <_model_name_> and <_prompt_name_>, with temperature taking values in [0, 0.4, 1.2, 1.6]\n",
    "\n",
    "\n",
    "- Slurm-based machine:\n",
    "```\n",
    "sbatch slurm_script_temperature.sh <model_name> <prompt_name>\n",
    "```\n",
    "\n",
    "Generated texts will be stored in Results/<_model_name_>_<_temperature_value_>/<_prompt_name_>\n",
    "\n",
    "### Run the analyses\n",
    "\n",
    " ```\n",
    "  python3 run_all_analyses.py -m <_model_name1_>_<_temperature_value1_> <_model_name1_>_<_temperature_value2_> <...> -sn data-for-plotting-temperature\n",
    "\n",
    "  ```\n",
    "\n",
    "  This will create a file 'data-for-plotting-temperature/all_data.pkl' that can be used for generating figures\n",
    "\n",
    "### Generate the figures\n",
    "\n",
    " ```\n",
    "  python3 plot_effect_of_temperature.py\n",
    "\n",
    "  ```\n",
    "\n",
    "  Figures will be stored in Figures_temperature/figures_<current_date>\n",
    "\n",
    "\n",
    "\n",
    "## Reproducing the experiments on effect of fine-tuning\n",
    "\n",
    "### Run the simulations\n",
    "\n",
    "To compare Base and Instruct models, follow to same steps as when comparing differnt models, but pass as <_model_name_> arguments both the Base and Instruct version. For instance, with \"Meta-Llama-3-70B-Base\" and \"Meta-Llama-3-70B-Instruct\"\n",
    "\n",
    "- Regular machine:\n",
    "```\n",
    "for i in {0..99}; do SLURM_ARRAY_TASK_ID=$i bash slurm_script_temperature.sh <model_name> <prompt_name> ; done\n",
    "```\n",
    "This will sequentially launch 100 chains (20 initial texts * 5 seeds) for the specified <_model_name_> and <_prompt_name_>, with temperature taking values in [0, 0.4, 1.2, 1.6]\n",
    "\n",
    "\n",
    "- Slurm-based machine:\n",
    "```\n",
    "sbatch slurm_script_temperature.sh <model_name> <prompt_name>\n",
    "```\n",
    "\n",
    "\n",
    "Generated texts will be stored in Results/<_model_name_>/<_prompt_name_>\n",
    "\n",
    "### Run the analyses\n",
    "\n",
    " ```\n",
    "  python3 run_all_analyses.py -m <_model_name1_> <_model_name2_> <...> -sn data-for-plotting-finetuning\n",
    "\n",
    "  ```\n",
    "\n",
    "  where <_model_name1_> is the Base version and <_model_name2_> is the Instruct version. \n",
    "\n",
    "  This will create a file 'data-for-plotting-finetuning/all_data.pkl' that can be used for generating figures\n",
    "\n",
    "### Generate the figures\n",
    "\n",
    " ```\n",
    "  python3 plot_effect_of_finetuning.py\n",
    "\n",
    "  ```\n",
    "\n",
    "  Figures will be stored in Figures_finetuning/figures_<current_date>\n",
    "\n",
    "  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
