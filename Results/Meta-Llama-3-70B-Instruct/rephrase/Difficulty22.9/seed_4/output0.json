{
    "adjacency_matrix": [
        [
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0
        ]
    ],
    "prompt_init": [
        "You will receive a text. Your task is to rephrase this text without modifying its meaning. Just output your new text, nothing else. Here is the text:"
    ],
    "prompt_update": [
        "You will receive a text. Your task is to rephrase this text without modifying its meaning. Just output your new text, nothing else. Here is the text:"
    ],
    "personality_list": [
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        ""
    ],
    "format_prompt": [
        ""
    ],
    "initial_story": "musculoskeletal diseases and cognitive impairments in patients lead todifficulties in movement as well as negative effects on their psychologicalhealth. clinical gait analysis, a vital tool for early diagnosis and treatment,traditionally relies on expensive optical motion capture systems. recentadvances in computer vision and deep learning have opened the door to moreaccessible and cost-effective alternatives. this paper introduces a novelspatio-temporal transformer network to estimate critical gait parameters fromrgb videos captured by a single-view camera. empirical evaluations on a publicdataset of cerebral palsy patients indicate that the proposed frameworksurpasses current state-of-the-art approaches and show significant improvementsin predicting general gait parameters (including walking speed, gait deviationindex - gdi, and knee flexion angle at maximum extension), while utilizingfewer parameters and alleviating the need for manual feature extraction. ",
    "stories": [
        [
            "Patients with musculoskeletal diseases and cognitive impairments often experience mobility issues and psychological distress. Clinical gait analysis is crucial for early diagnosis and treatment, but traditionally requires costly optical motion capture systems. However, recent breakthroughs in computer vision and deep learning have made it possible to develop more affordable and accessible alternatives. This study presents a novel spatio-temporal transformer network that can accurately estimate key gait parameters from RGB videos captured by a single camera. Experimental results using a public dataset of cerebral palsy patients demonstrate that our proposed framework outperforms current state-of-the-art approaches, achieving significant improvements in predicting general gait parameters (including walking speed, gait deviation index, and knee flexion angle at maximum extension), while requiring fewer parameters and eliminating the need for manual feature extraction."
        ],
        [
            "Individuals suffering from musculoskeletal diseases and cognitive impairments frequently face mobility limitations and emotional distress. Accurate diagnosis and treatment rely heavily on clinical gait analysis, but this typically involves expensive optical motion capture systems. Fortunately, advancements in computer vision and deep learning have enabled the development of more cost-effective and accessible solutions. This research introduces a novel spatio-temporal transformer network capable of precisely estimating critical gait parameters from single-camera RGB video recordings. Our experimental findings, based on a publicly available dataset of cerebral palsy patients, show that our proposed framework surpasses existing state-of-the-art methods, achieving notable improvements in predicting general gait parameters (including walking speed, gait deviation index, and knee flexion angle at maximum extension), all while requiring fewer parameters and eliminating manual feature extraction."
        ],
        [
            "People with musculoskeletal diseases and cognitive impairments often experience restricted mobility and emotional suffering. To diagnose and treat these conditions effectively, clinicians rely on thorough gait analysis, but this typically requires costly and complex motion capture systems. However, recent breakthroughs in computer vision and deep learning have made it possible to develop more affordable and accessible alternatives. This study presents a novel spatio-temporal transformer network that can accurately estimate key gait parameters from single-camera video recordings. Our results, based on a publicly available dataset of patients with cerebral palsy, demonstrate that our proposed framework outperforms existing state-of-the-art methods, achieving significant improvements in predicting essential gait parameters such as walking speed, gait deviation index, and knee flexion angle at maximum extension, while requiring fewer parameters and eliminating manual feature extraction."
        ],
        [
            "Individuals affected by musculoskeletal diseases and cognitive impairments frequently face limitations in mobility and emotional distress. Clinicians rely on comprehensive gait analysis to accurately diagnose and treat these conditions, but traditionally, this has necessitated the use of expensive and intricate motion capture systems. Nevertheless, recent advances in computer vision and deep learning have enabled the development of more cost-effective and accessible solutions. This study introduces a novel spatio-temporal transformer network capable of precisely estimating crucial gait parameters from single-camera video recordings. Our findings, based on a publicly available dataset of patients with cerebral palsy, show that our proposed framework surpasses existing state-of-the-art methods, achieving notable improvements in predicting vital gait parameters such as walking speed, gait deviation index, and knee flexion angle at maximum extension, while requiring fewer parameters and eliminating manual feature extraction."
        ],
        [
            "People with musculoskeletal diseases and cognitive impairments often experience restricted mobility and emotional suffering. To diagnose and treat these conditions, healthcare professionals rely on thorough gait analysis, but this typically requires the use of complex and costly motion capture systems. However, breakthroughs in computer vision and deep learning have made it possible to develop more affordable and accessible alternatives. This research presents a novel spatio-temporal transformer network that can accurately estimate key gait parameters from video recordings taken by a single camera. Our results, based on a publicly available dataset of patients with cerebral palsy, demonstrate that our proposed framework outperforms existing state-of-the-art methods, achieving significant improvements in predicting crucial gait parameters such as walking speed, gait deviation index, and knee flexion angle at maximum extension, while requiring fewer parameters and eliminating the need for manual feature extraction."
        ],
        [
            "Individuals affected by musculoskeletal diseases and cognitive impairments frequently face limitations in mobility and emotional distress. Healthcare professionals diagnose and treat these conditions by conducting in-depth analyses of gait patterns, which typically involves the use of sophisticated and expensive motion capture technology. However, advancements in computer vision and deep learning have enabled the development of more cost-effective and accessible alternatives. This study introduces a novel spatio-temporal transformer network capable of accurately estimating key gait parameters from video footage captured by a single camera. Our findings, based on a publicly available dataset of patients with cerebral palsy, show that our proposed framework surpasses existing state-of-the-art methods, achieving substantial improvements in predicting critical gait parameters such as walking speed, gait deviation index, and knee flexion angle at maximum extension, while requiring fewer parameters and eliminating manual feature extraction."
        ],
        [
            "People with musculoskeletal diseases and cognitive impairments often struggle with restricted mobility and emotional suffering. To diagnose and treat these conditions, healthcare professionals typically conduct detailed examinations of gait patterns, which usually involves using advanced and costly motion capture technology. Nevertheless, breakthroughs in computer vision and deep learning have made it possible to develop more affordable and accessible solutions. This research presents a new spatio-temporal transformer network that can accurately estimate key gait parameters from video recordings taken with a single camera. Our results, based on a publicly available dataset of patients with cerebral palsy, demonstrate that our proposed framework outperforms existing state-of-the-art methods, achieving significant improvements in predicting crucial gait parameters such as walking speed, gait deviation index, and knee flexion angle at maximum extension, while requiring fewer parameters and eliminating manual feature extraction."
        ],
        [
            "Individuals affected by musculoskeletal diseases and cognitive impairments frequently experience limited mobility and emotional distress. Healthcare providers typically diagnose and treat these conditions by conducting in-depth analyses of gait patterns, which often rely on sophisticated and expensive motion capture technology. However, recent advances in computer vision and deep learning have enabled the development of more cost-effective and accessible alternatives. This study introduces a novel spatio-temporal transformer network capable of accurately estimating key gait parameters from single-camera video recordings. Our findings, based on a publicly available dataset of patients with cerebral palsy, show that our proposed framework surpasses existing state-of-the-art methods, achieving substantial improvements in predicting critical gait parameters such as walking speed, gait deviation index, and knee flexion angle at maximum extension, while requiring fewer parameters and eliminating manual feature extraction."
        ],
        [
            "People with musculoskeletal diseases and cognitive impairments often struggle with restricted movement and emotional suffering. To diagnose and treat these conditions, healthcare professionals typically perform thorough examinations of walking patterns, which usually involve advanced and costly motion capture technology. Nevertheless, breakthroughs in computer vision and deep learning have led to the creation of more affordable and accessible alternatives. This research presents a new spatio-temporal transformer network that can precisely estimate crucial gait parameters from single-camera video recordings. Our results, based on a publicly available dataset of patients with cerebral palsy, demonstrate that our proposed framework outperforms existing state-of-the-art methods, achieving significant improvements in predicting vital gait parameters such as walking speed, gait deviation index, and knee flexion angle at maximum extension, while requiring fewer parameters and eliminating manual feature extraction."
        ],
        [
            "Individuals afflicted with musculoskeletal diseases and cognitive impairments frequently face limitations in mobility and emotional distress. In order to accurately diagnose and treat these conditions, medical professionals usually conduct comprehensive assessments of walking patterns, often relying on sophisticated and expensive motion capture systems. However, recent advancements in computer vision and deep learning have paved the way for more affordable and accessible solutions. This study introduces a novel spatio-temporal transformer network capable of accurately estimating critical gait parameters from video recordings captured by a single camera. Our findings, based on a publicly accessible dataset of patients with cerebral palsy, show that our proposed framework surpasses existing state-of-the-art methods, achieving substantial improvements in predicting essential gait parameters such as walking speed, gait deviation index, and knee flexion angle at maximum extension, while requiring fewer parameters and eliminating the need for manual feature extraction."
        ],
        [
            "People suffering from musculoskeletal diseases and cognitive impairments often experience restricted mobility and emotional suffering. To diagnose and treat these conditions effectively, healthcare professionals typically conduct thorough evaluations of walking patterns, which often involve the use of complex and costly motion capture systems. Nevertheless, breakthroughs in computer vision and deep learning have opened up opportunities for more affordable and accessible alternatives. This study presents a novel spatio-temporal transformer network that can accurately estimate key gait parameters from video recordings taken by a single camera. Our research, based on a publicly available dataset of patients with cerebral palsy, reveals that our proposed framework outperforms existing state-of-the-art methods, achieving significant improvements in predicting crucial gait parameters such as walking speed, gait deviation index, and knee flexion angle at maximum extension, all while requiring fewer parameters and eliminating the need for manual feature extraction."
        ],
        [
            "Individuals with musculoskeletal disorders and cognitive impairments frequently face limited mobility and emotional distress. To accurately diagnose and treat these conditions, healthcare providers typically conduct in-depth assessments of patients' gait patterns, often relying on expensive and complex motion capture systems. However, advances in computer vision and deep learning have paved the way for more affordable and accessible solutions. This study introduces a novel spatio-temporal transformer network capable of precisely estimating essential gait parameters from video footage captured by a single camera. Our findings, based on a publicly available dataset of patients with cerebral palsy, demonstrate that our proposed framework surpasses existing state-of-the-art methods, achieving notable improvements in predicting vital gait parameters such as walking speed, gait deviation index, and knee flexion angle at maximum extension, all while requiring fewer parameters and eliminating the need for manual feature extraction."
        ],
        [
            "People with musculoskeletal disorders and cognitive impairments often experience restricted mobility and emotional suffering. Healthcare professionals typically conduct thorough evaluations of patients' walking patterns to diagnose and treat these conditions, usually relying on costly and intricate motion capture systems. However, breakthroughs in computer vision and deep learning have made it possible to develop more affordable and accessible alternatives. This study presents a new spatio-temporal transformer network that can accurately estimate crucial gait parameters from video recordings taken with a single camera. Our results, based on a publicly available dataset of patients with cerebral palsy, show that our proposed framework outperforms existing state-of-the-art methods, achieving significant improvements in predicting key gait parameters such as walking speed, gait deviation index, and knee flexion angle at maximum extension, while requiring fewer parameters and eliminating the need for manual feature extraction."
        ],
        [
            "Individuals with musculoskeletal disorders and cognitive impairments frequently face limitations in mobility and emotional distress. To diagnose and treat these conditions, healthcare professionals typically conduct in-depth assessments of patients' walking patterns, often relying on complex and expensive motion capture systems. However, advances in computer vision and deep learning have enabled the development of more affordable and accessible solutions. This study introduces a novel spatio-temporal transformer network capable of accurately estimating critical gait parameters from video recordings captured with a single camera. Our findings, based on a publicly available dataset of patients with cerebral palsy, demonstrate that our proposed framework surpasses existing state-of-the-art methods, achieving notable improvements in predicting key gait parameters such as walking speed, gait deviation index, and knee flexion angle at maximum extension, while requiring fewer parameters and eliminating manual feature extraction."
        ],
        [
            "People with musculoskeletal disorders and cognitive impairments often struggle with limited mobility and emotional suffering. Healthcare professionals typically conduct thorough examinations of patients' walking patterns to diagnose and treat these conditions, usually relying on complex and costly motion capture systems. However, breakthroughs in computer vision and deep learning have made it possible to develop more affordable and accessible alternatives. This study presents a new spatio-temporal transformer network that can accurately estimate key gait parameters from video recordings taken with a single camera. Our results, based on a publicly available dataset of patients with cerebral palsy, show that our proposed framework outperforms existing state-of-the-art methods, achieving significant improvements in predicting crucial gait parameters such as walking speed, gait deviation index, and knee flexion angle at maximum extension, all while requiring fewer parameters and eliminating manual feature extraction."
        ],
        [
            "Individuals afflicted with musculoskeletal disorders and cognitive impairments frequently experience restricted mobility and emotional distress. To diagnose and treat these conditions, healthcare providers usually conduct comprehensive assessments of patients' gait patterns, often relying on intricate and expensive motion capture technology. However, recent advancements in computer vision and deep learning have paved the way for the development of more cost-effective and accessible solutions. This study introduces a novel spatio-temporal transformer network capable of accurately estimating critical gait parameters from video footage captured using a single camera. Our findings, based on a publicly available dataset of patients with cerebral palsy, demonstrate that our proposed framework surpasses existing state-of-the-art methods, achieving substantial improvements in predicting vital gait parameters such as walking speed, gait deviation index, and knee flexion angle at maximum extension, all while requiring fewer parameters and eliminating manual feature extraction."
        ],
        [
            "People with musculoskeletal disorders and cognitive impairments often struggle with limited mobility and emotional suffering. Healthcare professionals typically diagnose and treat these conditions by conducting thorough evaluations of patients' walking patterns, which often involve the use of complex and costly motion capture technology. However, breakthroughs in computer vision and deep learning have made it possible to develop more affordable and accessible solutions. This study presents a new spatio-temporal transformer network that can accurately estimate key gait parameters from video recordings taken with a single camera. Our results, based on a publicly available dataset of patients with cerebral palsy, show that our proposed framework outperforms existing state-of-the-art methods, achieving significant improvements in predicting crucial gait parameters such as walking speed, gait deviation index, and knee flexion angle at maximum extension, all while requiring fewer parameters and eliminating manual feature extraction."
        ],
        [
            "Individuals dealing with musculoskeletal disorders and cognitive impairments frequently face significant challenges related to restricted mobility and emotional distress. To diagnose and treat these conditions, healthcare professionals usually conduct comprehensive assessments of patients' walking patterns, often relying on complex and expensive motion capture technology. However, recent advancements in computer vision and deep learning have enabled the development of more affordable and accessible alternatives. This study introduces a novel spatio-temporal transformer network capable of accurately estimating key gait parameters from video footage captured using a single camera. Our findings, based on a publicly available dataset of patients with cerebral palsy, demonstrate that our proposed framework surpasses existing state-of-the-art methods, achieving substantial improvements in predicting crucial gait parameters such as walking speed, gait deviation index, and knee flexion angle at maximum extension, all while requiring fewer parameters and eliminating the need for manual feature extraction."
        ],
        [
            "People with musculoskeletal disorders and cognitive impairments often struggle with limited mobility and emotional suffering. Healthcare professionals typically perform thorough evaluations of patients' walking patterns to diagnose and treat these conditions, often relying on sophisticated and costly motion capture systems. However, breakthroughs in computer vision and deep learning have led to the creation of more affordable and accessible solutions. This study presents a new spatio-temporal transformer network that can accurately estimate key gait parameters from video footage recorded using a single camera. Our results, based on a publicly available dataset of patients with cerebral palsy, show that our proposed framework outperforms existing state-of-the-art methods, achieving significant improvements in predicting essential gait parameters such as walking speed, gait deviation index, and knee flexion angle at maximum extension, all while using fewer parameters and eliminating the need for manual feature extraction."
        ],
        [
            "Individuals affected by musculoskeletal disorders and cognitive impairments frequently face difficulties with restricted mobility and emotional distress. To diagnose and treat these conditions, healthcare providers typically conduct comprehensive assessments of patients' walking patterns, often utilizing advanced and expensive motion capture systems. Nevertheless, advancements in computer vision and deep learning have paved the way for more affordable and accessible alternatives. This study introduces a novel spatio-temporal transformer network capable of accurately estimating crucial gait parameters from video recordings captured by a single camera. Our findings, based on a publicly available dataset of patients with cerebral palsy, demonstrate that our proposed framework surpasses existing state-of-the-art methods, achieving notable improvements in predicting vital gait parameters such as walking speed, gait deviation index, and knee flexion angle at maximum extension, while requiring fewer parameters and eliminating manual feature extraction."
        ],
        [
            "People with musculoskeletal disorders and cognitive impairments often struggle with limited mobility and emotional distress. Healthcare professionals usually assess patients' walking patterns thoroughly to diagnose and treat these conditions, often relying on sophisticated and costly motion capture systems. However, breakthroughs in computer vision and deep learning have made it possible to develop more affordable and accessible solutions. This research presents a new spatio-temporal transformer network that can accurately estimate key gait parameters from video footage taken with a single camera. Our results, based on a publicly available dataset of patients with cerebral palsy, show that our proposed framework outperforms existing state-of-the-art methods, achieving significant improvements in predicting crucial gait parameters such as walking speed, gait deviation index, and knee flexion angle at maximum extension, while using fewer parameters and eliminating manual feature extraction."
        ],
        [
            "Individuals with musculoskeletal disorders and cognitive impairments frequently face challenges related to restricted mobility and emotional suffering. To diagnose and treat these conditions, healthcare professionals typically conduct a thorough examination of patients' walking patterns, often relying on advanced and expensive motion capture systems. However, recent advances in computer vision and deep learning have enabled the development of more affordable and accessible alternatives. This study introduces a novel spatio-temporal transformer network capable of accurately estimating key gait parameters from single-camera video recordings. Our findings, based on a publicly available dataset of patients with cerebral palsy, demonstrate that our proposed framework surpasses existing state-of-the-art methods, achieving significant improvements in predicting critical gait parameters such as walking speed, gait deviation index, and knee flexion angle at maximum extension, while requiring fewer parameters and eliminating manual feature extraction."
        ],
        [
            "People with musculoskeletal disorders and cognitive impairments often struggle with limited mobility and emotional distress. To identify and manage these conditions, healthcare providers usually conduct a comprehensive assessment of patients' walking patterns, typically relying on sophisticated and costly motion capture technology. However, recent breakthroughs in computer vision and deep learning have paved the way for more affordable and accessible solutions. This study presents a novel spatio-temporal transformer network that can accurately estimate key gait parameters from single-camera video recordings. Our results, based on a publicly available dataset of patients with cerebral palsy, show that our proposed framework outperforms existing state-of-the-art methods, achieving significant improvements in predicting crucial gait parameters such as walking speed, gait deviation index, and knee flexion angle at maximum extension, while requiring fewer parameters and eliminating manual feature extraction."
        ],
        [
            "Individuals living with musculoskeletal disorders and cognitive impairments frequently face challenges related to restricted mobility and emotional suffering. Healthcare professionals typically perform a thorough evaluation of patients' walking patterns to diagnose and manage these conditions, often relying on advanced and expensive motion capture systems. However, recent advancements in computer vision and deep learning have made it possible to develop more cost-effective and widely accessible alternatives. This study introduces a novel spatio-temporal transformer network capable of precisely estimating essential gait parameters from single-camera video recordings. Our findings, based on a publicly available dataset of patients with cerebral palsy, demonstrate that our proposed framework surpasses existing state-of-the-art methods, achieving notable improvements in predicting critical gait parameters such as walking speed, gait deviation index, and knee flexion angle at maximum extension, while requiring fewer parameters and eliminating manual feature extraction."
        ],
        [
            "People with musculoskeletal disorders and cognitive impairments often struggle with limited mobility and emotional distress. To diagnose and treat these conditions, healthcare providers typically conduct a detailed assessment of patients' walking patterns, usually relying on sophisticated and costly motion capture systems. However, breakthroughs in computer vision and deep learning have enabled the development of more affordable and accessible alternatives. This study presents a novel spatio-temporal transformer network that accurately estimates key gait parameters from single-camera video recordings. Our results, based on a publicly available dataset of patients with cerebral palsy, show that our proposed framework outperforms existing state-of-the-art methods, achieving significant improvements in predicting crucial gait parameters such as walking speed, gait deviation index, and knee flexion angle at maximum extension, all while requiring fewer parameters and eliminating manual feature extraction."
        ],
        [
            "Individuals with musculoskeletal disorders and cognitive impairments frequently face challenges related to restricted mobility and emotional suffering. Healthcare professionals typically perform a thorough evaluation of patients' walking patterns to diagnose and treat these conditions, often relying on advanced and expensive motion capture technology. Nevertheless, recent advancements in computer vision and deep learning have paved the way for the development of more affordable and accessible solutions. This research introduces a novel spatio-temporal transformer network that can accurately estimate key gait parameters from video recordings taken with a single camera. Our findings, based on a publicly available dataset of patients with cerebral palsy, demonstrate that our proposed framework surpasses existing state-of-the-art methods, achieving significant enhancements in predicting essential gait parameters such as walking speed, gait deviation index, and knee flexion angle at maximum extension, while requiring fewer parameters and eliminating the need for manual feature extraction."
        ],
        [
            "People with musculoskeletal disorders and cognitive impairments often struggle with limited mobility and emotional distress. To diagnose and treat these conditions, healthcare providers usually conduct a detailed analysis of patients' walking patterns, often relying on sophisticated and costly motion capture systems. However, recent breakthroughs in computer vision and deep learning have made it possible to develop more affordable and accessible alternatives. This study presents a new spatio-temporal transformer network that can precisely estimate key gait parameters from video recordings taken with a single camera. Our results, based on a publicly available dataset of patients with cerebral palsy, show that our proposed framework outperforms existing state-of-the-art methods, achieving significant improvements in predicting crucial gait parameters such as walking speed, gait deviation index, and knee flexion angle at maximum extension, while requiring fewer parameters and eliminating the need for manual feature extraction."
        ],
        [
            "Individuals affected by musculoskeletal disorders and cognitive impairments frequently face challenges related to restricted mobility and emotional suffering. In order to identify and address these conditions, healthcare professionals typically undertake a thorough examination of patients' walking patterns, often relying on advanced and expensive motion capture systems. Nevertheless, recent advancements in computer vision and deep learning have enabled the development of more cost-effective and accessible solutions. This research introduces a novel spatio-temporal transformer network capable of accurately estimating key gait parameters from video recordings captured using a single camera. Our findings, based on a publicly accessible dataset of patients with cerebral palsy, demonstrate that our proposed framework surpasses existing state-of-the-art methods, achieving substantial improvements in predicting critical gait parameters such as walking speed, gait deviation index, and knee flexion angle at maximum extension, while requiring fewer parameters and eliminating the need for manual feature extraction."
        ],
        [
            "People with musculoskeletal disorders and cognitive impairments often struggle with limited mobility and emotional distress. To diagnose and treat these conditions, healthcare providers usually conduct a comprehensive analysis of patients' walking patterns, typically relying on sophisticated and costly motion capture systems. However, recent breakthroughs in computer vision and deep learning have made it possible to develop more affordable and accessible alternatives. This study presents a new spatio-temporal transformer network that can accurately estimate key gait parameters from video recordings taken with a single camera. Our results, based on a publicly available dataset of patients with cerebral palsy, show that our proposed framework outperforms existing state-of-the-art methods, achieving significant improvements in predicting critical gait parameters such as walking speed, gait deviation index, and knee flexion angle at maximum extension, while requiring fewer parameters and eliminating the need for manual feature extraction."
        ],
        [
            "Individuals affected by musculoskeletal disorders and cognitive impairments frequently face challenges related to restricted mobility and emotional suffering. Healthcare professionals typically perform an in-depth examination of patients' walking patterns to diagnose and treat these conditions, often relying on advanced and expensive motion capture systems. However, recent advancements in computer vision and deep learning have enabled the development of more cost-effective and accessible solutions. This study introduces a novel spatio-temporal transformer network capable of precisely estimating crucial gait parameters from video recordings captured by a single camera. Our findings, based on a publicly available dataset of patients with cerebral palsy, demonstrate that our proposed framework surpasses existing state-of-the-art methods, achieving substantial improvements in predicting vital gait parameters such as walking speed, gait deviation index, and knee flexion angle at maximum extension, while requiring fewer parameters and eliminating the need for manual feature extraction."
        ],
        [
            "People with musculoskeletal disorders and cognitive impairments often struggle with limited mobility and emotional distress. To diagnose and treat these conditions, healthcare professionals usually conduct a thorough analysis of patients' walking patterns, often relying on sophisticated and costly motion capture systems. However, recent breakthroughs in computer vision and deep learning have made it possible to develop more affordable and accessible alternatives. This study presents a new spatio-temporal transformer network that can accurately estimate key gait parameters from video recordings taken by a single camera. Our results, based on a publicly available dataset of patients with cerebral palsy, show that our proposed framework outperforms existing state-of-the-art methods, achieving significant improvements in predicting crucial gait parameters such as walking speed, gait deviation index, and knee flexion angle at maximum extension, while requiring fewer parameters and eliminating the need for manual feature extraction."
        ],
        [
            "Individuals suffering from musculoskeletal disorders and cognitive impairments frequently face challenges related to restricted mobility and emotional distress. Healthcare professionals typically undertake a comprehensive examination of patients' gait patterns to diagnose and treat these conditions, often relying on advanced and expensive motion capture systems. However, recent advancements in computer vision and deep learning have enabled the development of more cost-effective and accessible alternatives. This study introduces a novel spatio-temporal transformer network capable of accurately estimating crucial gait parameters from video recordings captured by a single camera. Our findings, based on a publicly available dataset of patients with cerebral palsy, demonstrate that our proposed framework surpasses existing state-of-the-art methods, achieving notable improvements in predicting key gait parameters such as walking speed, gait deviation index, and knee flexion angle at maximum extension, while requiring fewer parameters and eliminating manual feature extraction."
        ],
        [
            "People with musculoskeletal disorders and cognitive impairments often struggle with limited mobility and emotional distress. To diagnose and treat these conditions, healthcare professionals usually conduct a thorough analysis of patients' gait patterns, often relying on sophisticated and costly motion capture systems. However, breakthroughs in computer vision and deep learning have made it possible to develop more affordable and accessible solutions. This study presents a novel spatio-temporal transformer network that can accurately estimate key gait parameters from video footage captured by a single camera. Our results, based on a publicly available dataset of patients with cerebral palsy, show that our proposed framework outperforms existing state-of-the-art methods, achieving significant improvements in predicting crucial gait parameters such as walking speed, gait deviation index, and knee flexion angle at maximum extension, while requiring fewer parameters and eliminating manual feature extraction."
        ],
        [
            "Individuals affected by musculoskeletal disorders and cognitive impairments frequently face challenges related to restricted mobility and emotional suffering. Healthcare professionals typically conduct an in-depth examination of patients' walking patterns to diagnose and treat these conditions, often relying on advanced and expensive motion capture technology. Nevertheless, recent advances in computer vision and deep learning have enabled the development of more cost-effective and accessible alternatives. This research introduces a novel spatio-temporal transformer network capable of precisely estimating key gait parameters from video recordings captured by a single camera. Our findings, based on a publicly available dataset of patients with cerebral palsy, demonstrate that our proposed framework surpasses existing state-of-the-art methods, achieving substantial improvements in predicting vital gait parameters such as walking speed, gait deviation index, and knee flexion angle at maximum extension, while requiring fewer parameters and eliminating manual feature extraction."
        ],
        [
            "People with musculoskeletal disorders and cognitive impairments often struggle with limited mobility and emotional distress. To diagnose and treat these conditions, healthcare providers usually conduct a thorough analysis of patients' walking patterns, often relying on sophisticated and costly motion capture technology. However, breakthroughs in computer vision and deep learning have made it possible to develop more affordable and accessible solutions. This study presents a new spatio-temporal transformer network that can accurately estimate crucial gait parameters from video recordings taken by a single camera. Our results, based on a publicly available dataset of patients with cerebral palsy, show that our proposed framework outperforms existing state-of-the-art methods, achieving significant improvements in predicting essential gait parameters such as walking speed, gait deviation index, and knee flexion angle at maximum extension, while requiring fewer parameters and eliminating manual feature extraction."
        ],
        [
            "Individuals suffering from musculoskeletal disorders and cognitive impairments frequently face challenges related to restricted movement and emotional suffering. Healthcare professionals typically conduct an in-depth examination of patients' walking patterns to diagnose and treat these conditions, often relying on advanced and expensive motion capture technology. Nevertheless, recent advancements in computer vision and deep learning have enabled the development of more cost-effective and accessible alternatives. This study introduces a novel spatio-temporal transformer network capable of precisely estimating vital gait parameters from single-camera video recordings. Our findings, based on a publicly available dataset of patients with cerebral palsy, demonstrate that our proposed framework surpasses existing state-of-the-art methods, achieving notable improvements in predicting essential gait parameters such as walking speed, gait deviation index, and knee flexion angle at maximum extension, while requiring fewer parameters and eliminating manual feature extraction."
        ],
        [
            "People with musculoskeletal disorders and cognitive impairments often struggle with limited mobility and emotional distress. To diagnose and treat these conditions, healthcare providers typically conduct a thorough analysis of patients' walking patterns, often relying on advanced and costly motion capture technology. However, recent breakthroughs in computer vision and deep learning have made it possible to develop more affordable and accessible solutions. This study presents a new spatio-temporal transformer network that can accurately estimate crucial gait parameters from single-camera video recordings. Our results, based on a publicly available dataset of patients with cerebral palsy, show that our proposed framework outperforms existing state-of-the-art methods, achieving significant improvements in predicting key gait parameters such as walking speed, gait deviation index, and knee flexion angle at maximum extension, while requiring fewer parameters and eliminating manual feature extraction."
        ],
        [
            "Individuals suffering from musculoskeletal disorders and cognitive impairments frequently face restricted movement and emotional suffering. Healthcare professionals usually conduct an in-depth examination of patients' walking patterns to diagnose and treat these conditions, often relying on sophisticated and expensive motion capture technology. However, recent advancements in computer vision and deep learning have enabled the development of more affordable and accessible alternatives. This study introduces a novel spatio-temporal transformer network capable of accurately estimating vital gait parameters from single-camera video recordings. Our findings, based on a publicly available dataset of patients with cerebral palsy, demonstrate that our proposed framework surpasses existing state-of-the-art methods, achieving notable improvements in predicting crucial gait parameters such as walking speed, gait deviation index, and knee flexion angle at maximum extension, while requiring fewer parameters and eliminating manual feature extraction."
        ],
        [
            "People with musculoskeletal disorders and cognitive impairments often experience limited mobility and emotional distress. To diagnose and treat these conditions, healthcare providers typically conduct a thorough analysis of patients' walking patterns, which often involves the use of advanced and costly motion capture systems. However, breakthroughs in computer vision and deep learning have led to the creation of more affordable and accessible solutions. This study presents a new spatio-temporal transformer network that can accurately estimate key gait parameters from single-camera video recordings. Our results, based on a publicly available dataset of patients with cerebral palsy, show that our proposed framework outperforms existing state-of-the-art methods, achieving significant improvements in predicting essential gait parameters such as walking speed, gait deviation index, and knee flexion angle at maximum extension, while requiring fewer parameters and eliminating the need for manual feature extraction."
        ],
        [
            "Individuals suffering from musculoskeletal disorders and cognitive impairments frequently face restricted mobility and emotional suffering. Healthcare professionals typically conduct an in-depth examination of patients' gait patterns to diagnose and treat these conditions, often relying on sophisticated and expensive motion capture systems. Nevertheless, advances in computer vision and deep learning have paved the way for more affordable and accessible alternatives. This research introduces a novel spatio-temporal transformer network capable of accurately estimating crucial gait parameters from single-camera video recordings. Our findings, based on a publicly available dataset of patients with cerebral palsy, demonstrate that our proposed framework surpasses existing state-of-the-art methods, achieving notable improvements in predicting vital gait parameters such as walking speed, gait deviation index, and knee flexion angle at maximum extension, while requiring fewer parameters and eliminating manual feature extraction."
        ],
        [
            "People with musculoskeletal disorders and cognitive impairments often experience limited mobility and emotional distress. To diagnose and treat these conditions, healthcare providers typically conduct a thorough analysis of patients' walking patterns, which often involves the use of complex and costly motion capture systems. However, recent breakthroughs in computer vision and deep learning have made it possible to develop more affordable and accessible solutions. This study presents a new spatio-temporal transformer network that can accurately estimate key gait parameters from single-camera video recordings. Our results, based on a publicly available dataset of patients with cerebral palsy, show that our proposed framework outperforms existing state-of-the-art methods, achieving significant improvements in predicting essential gait parameters such as walking speed, gait deviation index, and knee flexion angle at maximum extension, all while requiring fewer parameters and eliminating the need for manual feature extraction."
        ],
        [
            "Individuals suffering from musculoskeletal disorders and cognitive impairments often face restricted mobility and emotional suffering. Healthcare professionals typically undertake a comprehensive examination of patients' walking patterns to diagnose and treat these conditions, which frequently involves the use of intricate and expensive motion capture systems. Nevertheless, recent advancements in computer vision and deep learning have enabled the development of more cost-effective and accessible alternatives. This study introduces a novel spatio-temporal transformer network capable of precisely estimating crucial gait parameters from single-camera video recordings. Our findings, based on a publicly available dataset of patients with cerebral palsy, demonstrate that our proposed framework surpasses existing state-of-the-art methods, achieving notable improvements in predicting vital gait parameters such as walking speed, gait deviation index, and knee flexion angle at maximum extension, while requiring fewer parameters and eliminating the need for manual feature extraction."
        ],
        [
            "People with musculoskeletal disorders and cognitive impairments often experience limited mobility and emotional distress. To diagnose and treat these conditions, healthcare professionals usually conduct a thorough analysis of patients' walking patterns, often relying on complex and costly motion capture systems. However, recent breakthroughs in computer vision and deep learning have led to the development of more affordable and accessible solutions. This study presents a new spatio-temporal transformer network that accurately estimates key gait parameters from single-camera video recordings. Our results, based on a publicly available dataset of patients with cerebral palsy, show that our proposed framework outperforms existing state-of-the-art methods, achieving significant improvements in predicting essential gait parameters such as walking speed, gait deviation index, and knee flexion angle at maximum extension, all while requiring fewer parameters and eliminating manual feature extraction."
        ],
        [
            "Individuals suffering from musculoskeletal disorders and cognitive impairments frequently face restricted mobility and emotional suffering. Healthcare professionals typically conduct an in-depth examination of patients' walking patterns to diagnose and treat these conditions, often relying on sophisticated and expensive motion capture technology. Nevertheless, recent advancements in computer vision and deep learning have paved the way for more economical and accessible alternatives. This research introduces a novel spatio-temporal transformer network capable of accurately estimating crucial gait parameters from single-camera video recordings. Our findings, based on a publicly available dataset of patients with cerebral palsy, demonstrate that our proposed framework surpasses existing state-of-the-art methods, achieving notable improvements in predicting vital gait parameters such as walking speed, gait deviation index, and knee flexion angle at maximum extension, while requiring fewer parameters and eliminating the need for manual feature extraction."
        ],
        [
            "People with musculoskeletal disorders and cognitive impairments often experience limited mobility and emotional distress. To diagnose and treat these conditions, healthcare professionals usually conduct a thorough analysis of patients' walking patterns, often relying on advanced and costly motion capture systems. However, recent breakthroughs in computer vision and deep learning have led to the development of more affordable and accessible solutions. This study presents a novel spatio-temporal transformer network that can accurately estimate key gait parameters from single-camera video recordings. Our results, based on a publicly available dataset of patients with cerebral palsy, show that our proposed framework outperforms existing state-of-the-art methods, achieving significant improvements in predicting crucial gait parameters such as walking speed, gait deviation index, and knee flexion angle at maximum extension, while requiring fewer parameters and eliminating the need for manual feature extraction."
        ],
        [
            "Individuals suffering from musculoskeletal disorders and cognitive impairments frequently face restrictions in their mobility and emotional well-being. Healthcare providers typically perform a comprehensive examination of patients' gait patterns to diagnose and treat these conditions, often relying on sophisticated and expensive motion capture systems. Nevertheless, recent advancements in computer vision and deep learning have paved the way for the creation of more affordable and accessible alternatives. This research introduces a novel spatio-temporal transformer network capable of accurately estimating crucial gait parameters from single-camera video recordings. Our findings, based on a publicly available dataset of patients with cerebral palsy, demonstrate that our proposed framework surpasses existing state-of-the-art methods, achieving notable improvements in predicting vital gait parameters such as walking speed, gait deviation index, and knee flexion angle at maximum extension, while requiring fewer parameters and eliminating the need for manual feature extraction."
        ],
        [
            "People with musculoskeletal disorders and cognitive impairments often experience limitations in their mobility and emotional health. To diagnose and treat these conditions, healthcare professionals typically conduct a thorough analysis of patients' walking patterns, relying on advanced and costly motion capture systems. However, breakthroughs in computer vision and deep learning have enabled the development of more affordable and accessible solutions. This study presents a new spatio-temporal transformer network that can precisely estimate key gait parameters from video recordings taken with a single camera. Our results, based on a publicly available dataset of patients with cerebral palsy, show that our proposed framework outperforms existing state-of-the-art methods, achieving significant improvements in predicting essential gait parameters such as walking speed, gait deviation index, and knee flexion angle at maximum extension, all while requiring fewer parameters and eliminating the need for manual feature extraction."
        ],
        [
            "Individuals suffering from musculoskeletal disorders and cognitive impairments often face restrictions in their physical mobility and emotional wellbeing. Healthcare professionals usually conduct a comprehensive examination of patients' gait patterns to diagnose and treat these conditions, typically relying on sophisticated and expensive motion capture systems. However, recent advancements in computer vision and deep learning have paved the way for more affordable and accessible alternatives. This research introduces a novel spatio-temporal transformer network capable of accurately estimating crucial gait parameters from video recordings captured by a single camera. Our findings, based on a publicly available dataset of patients with cerebral palsy, demonstrate that our proposed framework surpasses existing state-of-the-art methods, achieving substantial improvements in predicting vital gait parameters such as walking speed, gait deviation index, and knee flexion angle at maximum extension, while requiring fewer parameters and eliminating the need for manual feature extraction."
        ],
        [
            "People with musculoskeletal disorders and cognitive impairments often experience limitations in their physical movement and emotional wellbeing. To diagnose and treat these conditions, healthcare professionals typically conduct a thorough analysis of patients' walking patterns, relying on complex and costly motion capture systems. However, breakthroughs in computer vision and deep learning have made it possible to develop more affordable and accessible solutions. This study presents a novel spatio-temporal transformer network that can accurately estimate key gait parameters from video recordings taken by a single camera. Our results, based on a publicly available dataset of patients with cerebral palsy, show that our proposed framework outperforms existing state-of-the-art methods, achieving significant improvements in predicting essential gait parameters such as walking speed, gait deviation index, and knee flexion angle at maximum extension, while requiring fewer parameters and eliminating the need for manual feature extraction."
        ],
        [
            "Individuals with musculoskeletal disorders and cognitive impairments frequently face restrictions in their physical mobility and emotional well-being. Healthcare professionals typically conduct an in-depth examination of patients' walking patterns to diagnose and treat these conditions, often relying on intricate and expensive motion capture systems. However, advances in computer vision and deep learning have enabled the development of more cost-effective and accessible alternatives. This study introduces a novel spatio-temporal transformer network capable of accurately estimating key gait parameters from single-camera video recordings. Our findings, based on a publicly available dataset of patients with cerebral palsy, demonstrate that our proposed framework surpasses existing state-of-the-art methods, achieving notable improvements in predicting crucial gait parameters such as walking speed, gait deviation index, and knee flexion angle at maximum extension, all while requiring fewer parameters and eliminating manual feature extraction."
        ]
    ]
}