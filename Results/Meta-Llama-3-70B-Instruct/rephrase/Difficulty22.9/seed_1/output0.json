{
    "adjacency_matrix": [
        [
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0
        ]
    ],
    "prompt_init": [
        "You will receive a text. Your task is to rephrase this text without modifying its meaning. Just output your new text, nothing else. Here is the text:"
    ],
    "prompt_update": [
        "You will receive a text. Your task is to rephrase this text without modifying its meaning. Just output your new text, nothing else. Here is the text:"
    ],
    "personality_list": [
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        ""
    ],
    "format_prompt": [
        ""
    ],
    "initial_story": "musculoskeletal diseases and cognitive impairments in patients lead todifficulties in movement as well as negative effects on their psychologicalhealth. clinical gait analysis, a vital tool for early diagnosis and treatment,traditionally relies on expensive optical motion capture systems. recentadvances in computer vision and deep learning have opened the door to moreaccessible and cost-effective alternatives. this paper introduces a novelspatio-temporal transformer network to estimate critical gait parameters fromrgb videos captured by a single-view camera. empirical evaluations on a publicdataset of cerebral palsy patients indicate that the proposed frameworksurpasses current state-of-the-art approaches and show significant improvementsin predicting general gait parameters (including walking speed, gait deviationindex - gdi, and knee flexion angle at maximum extension), while utilizingfewer parameters and alleviating the need for manual feature extraction. ",
    "stories": [
        [
            "Patients suffering from musculoskeletal diseases and cognitive impairments often experience mobility issues and adverse psychological effects. Clinical gait analysis is essential for early diagnosis and treatment, but conventional methods rely on costly optical motion capture systems. Fortunately, breakthroughs in computer vision and deep learning have paved the way for more affordable and accessible alternatives. This paper presents a novel spatio-temporal transformer network that accurately estimates key gait parameters from RGB videos captured by a single-view camera. Experimental results using a public dataset of cerebral palsy patients demonstrate that our proposed framework outperforms current state-of-the-art approaches, achieving significant improvements in predicting general gait parameters (including walking speed, gait deviation index, and knee flexion angle at maximum extension) while reducing the number of parameters required and eliminating the need for manual feature extraction."
        ],
        [
            "Individuals with musculoskeletal disorders and cognitive impairments often face difficulties with mobility and experience negative psychological consequences. Accurate diagnosis and treatment rely heavily on clinical gait analysis, but traditional methods are hindered by the high cost of optical motion capture systems. However, recent advancements in computer vision and deep learning have made it possible to develop more affordable and accessible solutions. This study introduces a novel spatio-temporal transformer network capable of accurately estimating key gait parameters from single-view camera RGB videos. Our experimental results, based on a publicly available dataset of patients with cerebral palsy, show that our proposed framework surpasses current state-of-the-art methods, achieving notable improvements in predicting general gait parameters (including walking speed, gait deviation index, and knee flexion angle at maximum extension) while reducing the number of required parameters and eliminating manual feature extraction."
        ],
        [
            "People with musculoskeletal and cognitive disorders often struggle with mobility and suffer psychologically as a result. To diagnose and treat these issues effectively, healthcare professionals rely on clinical gait analysis, but this is typically hindered by the high expense of optical motion capture systems. Fortunately, breakthroughs in computer vision and deep learning have enabled the development of more affordable alternatives. This research presents a new spatio-temporal transformer network that can accurately estimate key gait parameters from single-view camera videos. Our experiments, based on a publicly available dataset of patients with cerebral palsy, demonstrate that our proposed framework outperforms current state-of-the-art methods, achieving significant improvements in predicting general gait parameters (such as walking speed, gait deviation index, and knee flexion angle at maximum extension) while requiring fewer parameters and eliminating manual feature extraction."
        ],
        [
            "Individuals affected by musculoskeletal and cognitive impairments frequently experience mobility difficulties, which can lead to psychological distress. Healthcare providers depend on clinical gait analysis to accurately diagnose and treat these conditions, but the high cost of optical motion capture systems often hinders this process. However, advances in computer vision and deep learning have made it possible to create more affordable solutions. This study introduces a novel spatio-temporal transformer network capable of accurately estimating crucial gait parameters from single-view camera videos. Our experiments, utilizing a publicly available dataset of patients with cerebral palsy, show that our proposed framework surpasses current state-of-the-art methods, achieving notable improvements in predicting general gait parameters (including walking speed, gait deviation index, and knee flexion angle at maximum extension) while requiring fewer parameters and eliminating the need for manual feature extraction."
        ],
        [
            "People with musculoskeletal and cognitive impairments often struggle with mobility, which can cause emotional distress. To diagnose and treat these conditions, healthcare professionals rely on clinical gait analysis, but the expense of optical motion capture systems can be a significant obstacle. Fortunately, recent breakthroughs in computer vision and deep learning have paved the way for more affordable alternatives. This study presents a new spatio-temporal transformer network that can accurately estimate key gait parameters from single-view camera videos. Our experiments, using a publicly available dataset of patients with cerebral palsy, demonstrate that our proposed framework outperforms current state-of-the-art methods, achieving significant improvements in predicting general gait parameters (including walking speed, gait deviation index, and knee flexion angle at maximum extension) while requiring fewer parameters and eliminating manual feature extraction."
        ],
        [
            "Individuals with musculoskeletal and cognitive disabilities frequently face mobility challenges, leading to emotional suffering. Healthcare providers diagnose and treat these conditions using clinical gait analysis, but the high cost of optical motion capture systems can be a major hurdle. However, advancements in computer vision and deep learning have made it possible to develop more affordable options. This research introduces a novel spatio-temporal transformer network capable of precisely estimating essential gait parameters from single-camera video footage. Our experiments, utilizing a publicly available dataset of patients with cerebral palsy, show that our proposed framework surpasses current state-of-the-art methods, achieving substantial improvements in predicting general gait parameters (including walking speed, gait deviation index, and knee flexion angle at maximum extension) while requiring fewer parameters and eliminating the need for manual feature extraction."
        ],
        [
            "People with musculoskeletal and cognitive impairments often struggle with mobility, which can lead to emotional distress. Healthcare professionals use clinical gait analysis to diagnose and treat these conditions, but the high expense of optical motion capture systems can be a significant obstacle. Fortunately, breakthroughs in computer vision and deep learning have enabled the development of more budget-friendly alternatives. This study presents a novel spatio-temporal transformer network that can accurately estimate critical gait parameters from single-camera video recordings. Our experiments, using a publicly available dataset of patients with cerebral palsy, demonstrate that our proposed framework outperforms current state-of-the-art methods, achieving significant improvements in predicting general gait parameters (including walking speed, gait deviation index, and knee flexion angle at maximum extension) while requiring fewer parameters and eliminating manual feature extraction."
        ],
        [
            "Individuals with musculoskeletal and cognitive disabilities frequently face mobility challenges, which can result in emotional suffering. Healthcare providers utilize clinical gait analysis to identify and address these conditions, but the high cost of optical motion capture systems can be a substantial barrier. However, advancements in computer vision and deep learning have paved the way for more affordable solutions. This study introduces a novel spatio-temporal transformer network capable of accurately estimating key gait parameters from single-camera video recordings. Our experiments, utilizing a publicly available dataset of patients with cerebral palsy, show that our proposed framework surpasses current state-of-the-art methods, achieving notable improvements in predicting general gait parameters (including walking speed, gait deviation index, and knee flexion angle at maximum extension) while requiring fewer parameters and eliminating manual feature extraction."
        ],
        [
            "People with musculoskeletal and cognitive impairments often struggle with mobility, leading to emotional distress. To diagnose and treat these conditions, healthcare professionals rely on clinical gait analysis, but the expense of optical motion capture systems can be a significant obstacle. Fortunately, breakthroughs in computer vision and deep learning have made more affordable options possible. This research presents a new spatio-temporal transformer network that can precisely estimate key gait metrics from single-camera video recordings. Our experiments, using a publicly available dataset of patients with cerebral palsy, demonstrate that our proposed framework outperforms current state-of-the-art methods, achieving significant improvements in predicting general gait metrics (including walking speed, gait deviation index, and knee flexion angle at maximum extension) while requiring fewer parameters and eliminating manual feature extraction."
        ],
        [
            "Individuals with musculoskeletal and cognitive disabilities often face mobility challenges, which can lead to emotional suffering. Healthcare providers diagnose and treat these conditions using clinical gait analysis, but the high cost of optical motion capture systems can be a major hurdle. However, advancements in computer vision and deep learning have made more cost-effective solutions possible. This study introduces a novel spatio-temporal transformer network capable of accurately estimating key gait metrics from video recordings captured by a single camera. Our experiments, using a publicly available dataset of patients with cerebral palsy, show that our proposed framework surpasses current state-of-the-art methods, achieving significant improvements in predicting general gait metrics (including walking speed, gait deviation index, and knee flexion angle at maximum extension) while requiring fewer parameters and eliminating the need for manual feature extraction."
        ],
        [
            "People with musculoskeletal and cognitive impairments frequently encounter difficulties with mobility, which can result in emotional distress. Healthcare professionals assess and manage these conditions through clinical gait analysis, but the high expense of optical motion capture systems can pose a significant obstacle. Fortunately, breakthroughs in computer vision and deep learning have enabled the development of more affordable alternatives. This research presents a novel spatio-temporal transformer network that can precisely estimate crucial gait metrics from video footage recorded by a single camera. Our experiments, utilizing a publicly accessible dataset of patients with cerebral palsy, demonstrate that our proposed framework outperforms current state-of-the-art methods, achieving notable enhancements in predicting general gait metrics (including walking speed, gait deviation index, and knee flexion angle at maximum extension) while requiring fewer parameters and eliminating the need for manual feature extraction."
        ],
        [
            "Individuals with musculoskeletal and cognitive disabilities often face mobility challenges, leading to emotional suffering. To address these issues, healthcare providers rely on clinical gait analysis, but the high cost of traditional motion capture systems can be a major hurdle. However, advances in computer vision and deep learning have made it possible to develop more cost-effective solutions. This study introduces a novel spatio-temporal transformer network that can accurately estimate key gait metrics from video recordings captured by a single camera. Our experiments, using a publicly available dataset of patients with cerebral palsy, show that our proposed framework surpasses existing state-of-the-art methods, achieving significant improvements in predicting general gait metrics (including walking speed, gait deviation index, and knee flexion angle at maximum extension) while requiring fewer parameters and eliminating the need for manual feature extraction."
        ],
        [
            "People with musculoskeletal and cognitive impairments often struggle with mobility, leading to emotional distress. Healthcare professionals use clinical gait analysis to tackle these issues, but the expense of traditional motion capture technology can be a significant obstacle. Nevertheless, breakthroughs in computer vision and deep learning have enabled the development of more affordable alternatives. This research presents a novel spatio-temporal transformer network that can precisely estimate crucial gait metrics from video recordings taken by a single camera. Our experiments, using a publicly available dataset of patients with cerebral palsy, demonstrate that our proposed framework outperforms existing state-of-the-art methods, achieving substantial improvements in predicting general gait metrics (including walking speed, gait deviation index, and knee flexion angle at maximum extension) while requiring fewer parameters and eliminating manual feature extraction."
        ],
        [
            "Individuals with musculoskeletal and cognitive disabilities frequently experience mobility limitations, resulting in emotional suffering. To address this, healthcare providers employ clinical gait analysis, but the high cost of traditional motion capture systems can be a major hindrance. However, recent advances in computer vision and deep learning have made it possible to develop more cost-effective solutions. This study introduces a novel spatio-temporal transformer network capable of accurately estimating essential gait metrics from single-camera video recordings. Our experiments, utilizing a publicly available dataset of patients with cerebral palsy, show that our proposed framework surpasses existing state-of-the-art methods, achieving significant improvements in predicting general gait metrics (including walking speed, gait deviation index, and knee flexion angle at maximum extension) while requiring fewer parameters and eliminating manual feature extraction."
        ],
        [
            "People with musculoskeletal and cognitive disabilities often face mobility restrictions, leading to emotional distress. Healthcare professionals use clinical gait analysis to tackle this issue, but traditional motion capture systems are often prohibitively expensive. Fortunately, breakthroughs in computer vision and deep learning have enabled the development of more affordable alternatives. This research presents a novel spatio-temporal transformer network that can accurately estimate crucial gait metrics from single-camera video recordings. Our experiments, using a publicly available dataset of patients with cerebral palsy, demonstrate that our proposed framework outperforms existing state-of-the-art methods, achieving significant improvements in predicting general gait metrics (including walking speed, gait deviation index, and knee flexion angle at maximum extension) while requiring fewer parameters and eliminating manual feature extraction."
        ],
        [
            "Individuals with musculoskeletal and cognitive impairments frequently encounter limitations in their mobility, resulting in emotional suffering. To address this challenge, healthcare providers employ clinical gait analysis, but conventional motion capture systems are often too costly. However, advancements in computer vision and deep learning have made it possible to develop more affordable solutions. This study introduces a novel spatio-temporal transformer network capable of accurately estimating key gait metrics from video recordings captured by a single camera. Our experiments, utilizing a publicly available dataset of patients with cerebral palsy, show that our proposed framework surpasses existing state-of-the-art methods, achieving notable improvements in predicting general gait metrics (including walking speed, gait deviation index, and knee flexion angle at maximum extension) while requiring fewer parameters and eliminating the need for manual feature extraction."
        ],
        [
            "People with musculoskeletal and cognitive disabilities often face mobility restrictions, leading to emotional distress. Healthcare professionals use clinical gait analysis to tackle this issue, but traditional motion capture systems are typically unaffordable. Fortunately, breakthroughs in computer vision and deep learning have enabled the development of more budget-friendly alternatives. This study presents a novel spatio-temporal transformer network that can accurately estimate crucial gait metrics from video footage recorded by a single camera. Our experiments, using a publicly available dataset of patients with cerebral palsy, demonstrate that our proposed framework outperforms existing state-of-the-art methods, achieving significant improvements in predicting general gait metrics (including walking speed, gait deviation index, and knee flexion angle at maximum extension) while requiring fewer parameters and eliminating manual feature extraction."
        ],
        [
            "Individuals with musculoskeletal and cognitive impairments often experience limited mobility, which can lead to emotional suffering. To address this, healthcare providers employ clinical gait analysis, but conventional motion capture systems are usually too costly. However, advancements in computer vision and deep learning have made it possible to create more affordable options. This research introduces a novel spatio-temporal transformer network capable of accurately estimating key gait metrics from video recordings captured by a single camera. Our experiments, utilizing a publicly available dataset of patients with cerebral palsy, show that our proposed framework surpasses existing state-of-the-art methods, achieving substantial improvements in predicting general gait metrics (including walking speed, gait deviation index, and knee flexion angle at maximum extension) while requiring fewer parameters and eliminating the need for manual feature extraction."
        ],
        [
            "People with musculoskeletal and cognitive disabilities often have reduced mobility, leading to emotional distress. Healthcare professionals use clinical gait analysis to address this, but traditional motion capture systems are typically too expensive. Fortunately, breakthroughs in computer vision and deep learning have enabled the development of more affordable alternatives. This study presents a new spatio-temporal transformer network that can precisely estimate crucial gait metrics from video recordings taken by a single camera. Our experiments, using a publicly available dataset of patients with cerebral palsy, demonstrate that our proposed framework outperforms existing state-of-the-art methods, achieving significant improvements in predicting general gait metrics (including walking speed, gait deviation index, and knee flexion angle at maximum extension) while requiring fewer parameters and eliminating manual feature extraction."
        ],
        [
            "Individuals with musculoskeletal and cognitive impairments often experience limited mobility, which can lead to emotional suffering. To combat this, healthcare professionals utilize clinical gait analysis, but conventional motion capture systems are often prohibitively costly. However, recent advancements in computer vision and deep learning have paved the way for more budget-friendly solutions. This research introduces a novel spatio-temporal transformer network capable of accurately estimating key gait metrics from single-camera video recordings. Our experiments, utilizing a publicly available dataset of patients with cerebral palsy, show that our proposed framework surpasses existing state-of-the-art methods, achieving notable improvements in predicting general gait metrics (including stride speed, gait deviation index, and maximum knee flexion angle) while requiring fewer parameters and eliminating manual feature extraction."
        ],
        [
            "People with musculoskeletal and cognitive disabilities often have restricted movement, leading to emotional distress. To address this, healthcare providers use clinical gait analysis, but traditional motion capture systems are often too expensive. Fortunately, breakthroughs in computer vision and deep learning have made more affordable options possible. This study presents a new spatio-temporal transformer network that can accurately estimate important gait metrics from video recordings using a single camera. Our experiments, using a publicly available dataset of patients with cerebral palsy, demonstrate that our proposed framework outperforms existing state-of-the-art methods, achieving significant improvements in predicting general gait metrics (including stride speed, gait deviation index, and maximum knee flexion angle) while requiring fewer parameters and eliminating the need for manual feature extraction."
        ],
        [
            "Individuals with musculoskeletal and cognitive impairments frequently experience limited mobility, which can lead to emotional suffering. To combat this, healthcare professionals employ clinical gait analysis, but conventional motion capture systems are often prohibitively costly. However, recent advancements in computer vision and deep learning have made more budget-friendly alternatives feasible. This research introduces a novel spatio-temporal transformer network capable of accurately predicting crucial gait metrics from video footage using a single camera. Our experiments, utilizing a publicly accessible dataset of patients with cerebral palsy, show that our proposed framework surpasses existing state-of-the-art methods, achieving notable enhancements in forecasting general gait metrics (including stride speed, gait deviation index, and maximum knee flexion angle) while requiring fewer parameters and eliminating manual feature extraction."
        ],
        [
            "People with musculoskeletal and cognitive disabilities often face mobility restrictions, which can cause emotional distress. To address this, healthcare providers use gait analysis, but traditional motion capture systems are often too expensive. However, breakthroughs in computer vision and deep learning have made more affordable options possible. This study presents a new spatio-temporal transformer network that can accurately predict key gait metrics from video recordings using a single camera. Our tests, using a publicly available dataset of patients with cerebral palsy, demonstrate that our proposed framework outperforms existing state-of-the-art methods, achieving significant improvements in predicting general gait metrics (including stride speed, gait deviation index, and maximum knee flexion angle) while requiring fewer parameters and eliminating manual feature extraction."
        ],
        [
            "Individuals with musculoskeletal and cognitive impairments frequently experience limitations in mobility, leading to emotional suffering. Healthcare professionals employ gait analysis to tackle this issue, but conventional motion capture systems are often prohibitively costly. Fortunately, advances in computer vision and deep learning have enabled the development of more budget-friendly alternatives. This research introduces a novel spatio-temporal transformer network capable of accurately forecasting essential gait metrics from video recordings using a single camera. Our experiments, utilizing a publicly accessible dataset of patients with cerebral palsy, reveal that our proposed framework surpasses existing state-of-the-art methods, achieving substantial enhancements in predicting general gait metrics (including stride speed, gait deviation index, and maximum knee flexion angle) while necessitating fewer parameters and obviating manual feature extraction."
        ],
        [
            "People with musculoskeletal and cognitive disabilities often face mobility restrictions, which can lead to emotional distress. To address this, healthcare professionals use gait analysis, but traditional motion capture systems are usually too expensive. However, breakthroughs in computer vision and deep learning have made it possible to create more affordable options. This study presents a new spatio-temporal transformer network that can accurately predict key gait metrics from video recordings using just one camera. Our tests, which used a public dataset of patients with cerebral palsy, show that our proposed approach outperforms existing state-of-the-art methods, significantly improving the prediction of general gait metrics (including stride speed, gait deviation index, and maximum knee flexion angle) while requiring fewer parameters and eliminating the need for manual feature extraction."
        ],
        [
            "Individuals with musculoskeletal and cognitive impairments often experience limitations in their mobility, which can result in emotional suffering. To tackle this issue, healthcare experts employ gait analysis, but conventional motion capture systems are typically unaffordable. Nevertheless, advancements in computer vision and deep learning have enabled the development of more cost-effective alternatives. This research introduces a novel spatio-temporal transformer network capable of precisely predicting crucial gait metrics from video recordings using a single camera. Our experiments, utilizing a publicly available dataset of patients with cerebral palsy, demonstrate that our proposed approach surpasses existing state-of-the-art methods, substantially enhancing the prediction of general gait metrics (including stride speed, gait deviation index, and maximum knee flexion angle) while necessitating fewer parameters and omitting the need for manual feature extraction."
        ],
        [
            "People with musculoskeletal and cognitive disabilities often face mobility restrictions, leading to emotional distress. To address this, healthcare professionals use gait analysis, but traditional motion capture systems are often too expensive. However, breakthroughs in computer vision and deep learning have made it possible to create more affordable options. This study presents a new spatio-temporal transformer network that can accurately predict key gait metrics from video recordings using a single camera. Our experiments, using a publicly available dataset of patients with cerebral palsy, show that our approach outperforms existing state-of-the-art methods, significantly improving the prediction of general gait metrics (such as stride speed, gait deviation index, and maximum knee flexion angle) while requiring fewer parameters and eliminating the need for manual feature extraction."
        ],
        [
            "Individuals with musculoskeletal and cognitive impairments frequently encounter limitations in their mobility, which can lead to emotional suffering. To combat this, healthcare providers employ gait analysis, but conventional motion capture technologies are often prohibitively costly. Nevertheless, advancements in computer vision and deep learning have enabled the development of more budget-friendly alternatives. This research introduces a novel spatio-temporal transformer network capable of accurately forecasting crucial gait metrics from video recordings using a single camera. Our experiments, utilizing a publicly accessible dataset of patients with cerebral palsy, demonstrate that our approach surpasses existing cutting-edge methods, significantly enhancing the prediction of general gait metrics (such as stride speed, gait deviation index, and maximum knee flexion angle) while requiring fewer parameters and eliminating the need for manual feature extraction."
        ],
        [
            "People with musculoskeletal and cognitive disabilities often face mobility restrictions, which can cause emotional distress. To address this, healthcare professionals use gait analysis, but traditional motion capture systems are often too expensive. However, recent breakthroughs in computer vision and deep learning have made it possible to create more affordable alternatives. This study presents a new spatio-temporal transformer network that can accurately predict key gait metrics from video footage using a single camera. Our experiments, using a publicly available dataset of patients with cerebral palsy, show that our approach outperforms current state-of-the-art methods, significantly improving the prediction of general gait metrics (such as stride speed, gait deviation index, and maximum knee flexion angle) while requiring fewer parameters and eliminating the need for manual feature extraction."
        ],
        [
            "Individuals with musculoskeletal and cognitive impairments frequently encounter limitations in their mobility, leading to emotional suffering. Healthcare providers have turned to gait analysis to tackle this issue, but traditional motion capture systems are often prohibitively costly. Fortunately, advancements in computer vision and deep learning have paved the way for more budget-friendly options. This research introduces a novel spatio-temporal transformer network capable of accurately forecasting key gait metrics from video recordings using a single camera. Our experiments, leveraging a publicly accessible dataset of patients with cerebral palsy, demonstrate that our approach surpasses current state-of-the-art methods, significantly enhancing the prediction of general gait metrics (including stride speed, gait deviation index, and maximum knee flexion angle) while reducing the number of parameters required and eliminating manual feature extraction."
        ],
        [
            "People with musculoskeletal and cognitive disabilities often face mobility restrictions, which can lead to emotional distress. To address this, healthcare professionals have been using gait analysis, but conventional motion capture systems are frequently too expensive. However, recent breakthroughs in computer vision and deep learning have made it possible to develop more affordable alternatives. This study presents a new spatio-temporal transformer network that can precisely predict key gait metrics from video recordings using a single camera. Our experiments, which utilized a publicly available dataset of patients with cerebral palsy, show that our approach outperforms current state-of-the-art methods, significantly improving the prediction of general gait metrics (such as stride speed, gait deviation index, and maximum knee flexion angle) while reducing the number of required parameters and eliminating the need for manual feature extraction."
        ],
        [
            "Individuals with musculoskeletal and cognitive impairments often experience limitations in mobility, resulting in emotional suffering. Healthcare providers have turned to gait analysis to address this issue, but traditional motion capture systems are often prohibitively costly. Fortunately, advancements in computer vision and deep learning have enabled the development of more cost-effective solutions. This research introduces a novel spatio-temporal transformer network capable of accurately predicting crucial gait metrics from video recordings using a single camera. Our experiments, utilizing a publicly available dataset of patients with cerebral palsy, demonstrate that our approach surpasses current state-of-the-art methods, significantly enhancing the prediction of general gait metrics (including stride speed, gait deviation index, and maximum knee flexion angle) while reducing the number of required parameters and eliminating manual feature extraction."
        ],
        [
            "People with musculoskeletal and cognitive disabilities frequently face mobility restrictions, leading to emotional distress. To tackle this, healthcare professionals have turned to gait analysis, but conventional motion capture systems are often too expensive. However, breakthroughs in computer vision and deep learning have made it possible to develop more affordable alternatives. This study presents a new spatio-temporal transformer network that can accurately forecast key gait metrics from single-camera video recordings. Our tests, using a publicly available dataset of patients with cerebral palsy, show that our approach outperforms current state-of-the-art methods, significantly improving the prediction of general gait metrics (including stride speed, gait deviation index, and maximum knee flexion angle) while reducing the number of required parameters and eliminating manual feature extraction."
        ],
        [
            "Individuals with musculoskeletal and cognitive impairments often experience limited mobility, resulting in emotional suffering. To address this, healthcare experts have utilized gait analysis, but traditional motion capture systems are frequently cost-prohibitive. Nevertheless, advances in computer vision and deep learning have enabled the development of more budget-friendly options. This research introduces a novel spatio-temporal transformer network capable of accurately predicting key gait metrics from single-camera video footage. Our evaluation, utilizing a publicly available dataset of cerebral palsy patients, demonstrates that our approach surpasses existing state-of-the-art methods, substantially enhancing the prediction of general gait metrics (including stride speed, gait deviation index, and maximum knee flexion angle) while minimizing required parameters and eliminating manual feature extraction."
        ],
        [
            "People with musculoskeletal and cognitive disabilities often face mobility limitations, leading to emotional distress. Healthcare professionals have turned to gait analysis to address this, but traditional motion capture systems are often too expensive. However, recent breakthroughs in computer vision and deep learning have made it possible to develop more affordable alternatives. This study presents a new spatio-temporal transformer network that can accurately estimate key gait metrics from video footage captured by a single camera. Our assessment, using a publicly available dataset of patients with cerebral palsy, shows that our approach outperforms existing state-of-the-art methods, significantly improving the prediction of general gait metrics (such as stride speed, gait deviation index, and maximum knee flexion angle) while reducing the number of required parameters and eliminating the need for manual feature extraction."
        ],
        [
            "Individuals with musculoskeletal and cognitive impairments frequently encounter restrictions in their mobility, resulting in emotional strain. To combat this, healthcare providers have turned to gait analysis, but traditional motion capture systems are often prohibitively costly. However, advancements in computer vision and deep learning have enabled the development of more affordable solutions. This research introduces a novel spatio-temporal transformer network capable of accurately estimating crucial gait metrics from video recordings captured by a single camera. Our evaluation, utilizing a publicly accessible dataset of patients with cerebral palsy, demonstrates that our approach surpasses existing state-of-the-art methods, significantly enhancing the prediction of general gait metrics (including stride speed, gait deviation index, and maximum knee flexion angle) while reducing the number of required parameters and eliminating the need for manual feature extraction."
        ],
        [
            "People with musculoskeletal and cognitive disabilities often face limitations in their mobility, leading to emotional distress. Healthcare professionals have sought to address this issue through gait analysis, but traditional motion capture systems are typically too expensive. Fortunately, breakthroughs in computer vision and deep learning have made it possible to develop more affordable alternatives. This study presents a new spatio-temporal transformer network that can accurately estimate key gait metrics from video recordings taken by a single camera. Our assessment, which uses a publicly available dataset of patients with cerebral palsy, shows that our approach outperforms existing state-of-the-art methods, significantly improving the prediction of general gait metrics (such as stride speed, gait deviation index, and maximum knee flexion angle) while reducing the number of required parameters and eliminating the need for manual feature extraction."
        ],
        [
            "Individuals with musculoskeletal and cognitive impairments frequently experience restricted mobility, resulting in emotional suffering. To tackle this issue, healthcare providers have turned to gait analysis, but conventional motion capture systems have been prohibitively costly. However, advancements in computer vision and deep learning have enabled the creation of more budget-friendly options. This research introduces a novel spatio-temporal transformer network capable of precisely estimating crucial gait metrics from single-camera video recordings. Our evaluation, utilizing a publicly accessible dataset of cerebral palsy patients, demonstrates that our approach surpasses current state-of-the-art methods, significantly enhancing the prediction of general gait metrics (including stride speed, gait deviation index, and maximum knee flexion angle) while decreasing the required parameters and eliminating manual feature extraction."
        ],
        [
            "People with musculoskeletal and cognitive disabilities often struggle with limited mobility, leading to emotional distress. To address this problem, healthcare professionals have employed gait analysis, but traditional motion capture systems have been too expensive. However, breakthroughs in computer vision and deep learning have made more affordable alternatives possible. This study presents a new spatio-temporal transformer network that can accurately estimate key gait metrics from single-camera video footage. Our assessment, using a publicly available dataset of patients with cerebral palsy, shows that our approach outperforms current state-of-the-art methods, significantly improving the prediction of general gait metrics (such as stride speed, gait deviation index, and maximum knee flexion angle) while reducing the required parameters and eliminating the need for manual feature extraction."
        ],
        [
            "Individuals with musculoskeletal and cognitive impairments frequently face restricted mobility, resulting in emotional suffering. To tackle this issue, healthcare providers have utilized gait analysis, but traditional motion capture systems have been prohibitively costly. Nevertheless, advances in computer vision and deep learning have enabled the development of more affordable options. This research introduces a novel spatio-temporal transformer network capable of precisely estimating crucial gait metrics from single-camera video recordings. Our evaluation, based on a publicly available dataset of patients with cerebral palsy, reveals that our approach surpasses current state-of-the-art methods, substantially enhancing the prediction of general gait metrics (including stride speed, gait deviation index, and maximum knee flexion angle) while decreasing the required parameters and eliminating manual feature extraction."
        ],
        [
            "People with musculoskeletal and cognitive disabilities often experience limited mobility, leading to emotional distress. To address this, healthcare professionals have employed gait analysis, but traditional motion capture systems have been too expensive. However, breakthroughs in computer vision and deep learning have made more affordable solutions possible. This study presents a new spatio-temporal transformer network that can accurately estimate key gait metrics from single-camera video recordings. Our assessment, using a publicly available dataset of patients with cerebral palsy, shows that our approach outperforms current state-of-the-art methods, significantly improving the prediction of general gait metrics (such as stride speed, gait deviation index, and maximum knee flexion angle) while reducing the required parameters and eliminating manual feature extraction."
        ],
        [
            "Individuals with musculoskeletal and cognitive impairments frequently face mobility limitations, resulting in emotional suffering. To tackle this issue, healthcare providers have utilized gait analysis, but conventional motion capture technologies have been cost-prohibitive. Nevertheless, advancements in computer vision and deep learning have paved the way for more budget-friendly solutions. This research introduces a novel spatio-temporal transformer network capable of precisely estimating crucial gait metrics from single-camera video footage. Our evaluation, based on a publicly accessible dataset of patients with cerebral palsy, demonstrates that our approach surpasses existing state-of-the-art methods, substantially enhancing the prediction of general gait metrics (including stride speed, gait deviation index, and maximum knee flexion angle) while minimizing required parameters and eliminating manual feature extraction."
        ],
        [
            "People with physical and mental disabilities often struggle with mobility issues, leading to emotional distress. To address this, healthcare professionals have turned to gait analysis, but traditional motion capture systems have been too expensive. However, breakthroughs in computer vision and deep learning have made more affordable solutions possible. This study presents a new spatio-temporal transformer network that can accurately estimate key gait metrics from a single camera video. Our results, based on a publicly available dataset of patients with cerebral palsy, show that our approach outperforms existing methods, significantly improving the prediction of overall gait metrics (such as stride speed, gait deviation index, and maximum knee flexion angle) while reducing the need for parameters and manual feature extraction."
        ],
        [
            "Individuals with physical and mental disabilities frequently experience difficulties with mobility, resulting in emotional suffering. To combat this, healthcare providers have turned to gait analysis, but conventional motion capture systems have been cost-prohibitive. Nevertheless, advancements in computer vision and deep learning have enabled the development of more affordable alternatives. This research introduces a novel spatio-temporal transformer network capable of precisely estimating crucial gait metrics from a single-camera video recording. Our findings, based on a publicly accessible dataset of patients with cerebral palsy, demonstrate that our approach surpasses existing methods, substantially enhancing the prediction of overall gait metrics (including stride speed, gait deviation index, and maximum knee flexion angle) while minimizing the need for parameters and manual feature extraction."
        ],
        [
            "People with physical and mental disabilities often struggle with mobility, leading to emotional distress. To address this, healthcare professionals have utilized gait analysis, but traditional motion capture systems have been too expensive. However, breakthroughs in computer vision and deep learning have made it possible to create more affordable options. This study presents a new spatio-temporal transformer network that can accurately estimate key gait metrics from a single-camera video recording. Our results, based on a publicly available dataset of patients with cerebral palsy, show that our approach outperforms existing methods, significantly improving the prediction of overall gait metrics (including stride speed, gait deviation index, and maximum knee flexion angle) while reducing the need for manual input and feature extraction."
        ],
        [
            "Individuals with physical and mental disabilities frequently face mobility challenges, resulting in emotional suffering. To combat this, healthcare experts have turned to gait analysis, but conventional motion capture systems have been cost-prohibitive. Nevertheless, advances in computer vision and deep learning have enabled the development of more affordable alternatives. This research introduces a novel spatio-temporal transformer network capable of precisely estimating crucial gait metrics from a single-camera video recording. Our findings, derived from a publicly accessible dataset of cerebral palsy patients, demonstrate that our approach surpasses existing methods, substantially enhancing the prediction of overall gait metrics (including stride speed, gait deviation index, and maximum knee flexion angle) while minimizing the need for manual input and feature extraction."
        ],
        [
            "People with physical and mental disabilities often struggle with mobility, leading to emotional distress. To address this, healthcare professionals have utilized gait analysis, but traditional motion capture systems have been too expensive. However, breakthroughs in computer vision and deep learning have made it possible to create more affordable options. This study presents a new spatio-temporal transformer network that can accurately estimate key gait metrics from a single-camera video recording. Our results, based on a publicly available dataset of patients with cerebral palsy, show that our approach outperforms existing methods, significantly improving the prediction of overall gait metrics (including stride speed, gait deviation index, and maximum knee flexion angle) while reducing the need for manual input and feature extraction."
        ],
        [
            "Individuals with physical and mental disabilities frequently face mobility challenges, which can lead to emotional suffering. Healthcare providers have turned to gait analysis to tackle this issue, but conventional motion capture systems have been prohibitively costly. Fortunately, advances in computer vision and deep learning have paved the way for more budget-friendly alternatives. This research introduces a novel spatio-temporal transformer network capable of precisely estimating essential gait metrics from a single-camera video recording. Our findings, based on a publicly available dataset of patients with cerebral palsy, demonstrate that our approach surpasses existing methods, significantly enhancing the prediction of overall gait metrics (including stride speed, gait deviation index, and maximum knee flexion angle) while minimizing the need for manual input and feature extraction."
        ],
        [
            "People with physical and mental disabilities often struggle with mobility, leading to emotional distress. To address this, healthcare professionals have utilized gait analysis, but traditional motion capture systems have been too expensive. However, recent breakthroughs in computer vision and deep learning have made more affordable options possible. This study presents a new spatio-temporal transformer network that accurately estimates crucial gait metrics from a single video recording. Our results, based on a publicly available dataset of patients with cerebral palsy, show that our approach outperforms existing methods, significantly improving the prediction of key gait metrics (such as stride speed, gait deviation index, and maximum knee flexion angle) while reducing the need for manual input and feature extraction."
        ],
        [
            "Individuals with physical and mental disabilities frequently face mobility issues, which can lead to emotional suffering. Healthcare providers have turned to gait analysis to tackle this problem, but traditional motion capture systems have been prohibitively costly. Nevertheless, advances in computer vision and deep learning have paved the way for more budget-friendly alternatives. This study introduces a novel spatio-temporal transformer network capable of accurately estimating vital gait metrics from a single video recording. Our findings, based on a publicly available dataset of patients with cerebral palsy, demonstrate that our approach surpasses existing methods, significantly enhancing the prediction of crucial gait metrics (including stride speed, gait deviation index, and maximum knee flexion angle) while minimizing the need for manual data input and feature extraction."
        ]
    ]
}