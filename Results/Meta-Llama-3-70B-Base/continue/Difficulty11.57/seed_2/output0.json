{
    "adjacency_matrix": [
        [
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0
        ]
    ],
    "prompt_init": [
        "You will receive a text. Your task is to continue this text. Just output your new text, nothing else. Here is the text:"
    ],
    "prompt_update": [
        "You will receive a text. Your task is to continue this text. Just output your new text, nothing else. Here is the text:"
    ],
    "personality_list": [
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        ""
    ],
    "format_prompt": [
        ""
    ],
    "initial_story": "having the potential for high speed, high throughput, and low energy cost,optical neural networks (onns) have emerged as a promising candidate foraccelerating deep learning tasks. in conventional onns, light amplitudes aremodulated at the input and detected at the output. however, the light phasesare still ignored in conventional structures, although they can also carryinformation for computing. to address this issue, in this paper, we propose aframework called oplixnet to compress the areas of onns by modulating inputimage data into the amplitudes and phase parts of light signals. the input andoutput parts of the onns are redesigned to make full use of both amplitude andphase information. moreover, mutual learning across different onn structures isintroduced to maintain the accuracy. experimental results demonstrate that theproposed framework significantly reduces the areas of onns with the accuracywithin an acceptable range. for instance, 75.03% area is reduced with a 0.33%accuracy decrease on fully connected neural network (fcnn) and 74.88% area isreduced with a 2.38% accuracy decrease on resnet-32. ",
    "stories": [
        [
            "Having the potential for high speed, high throughput, and low energy cost, Optical Neural Networks (ONNs) have emerged as a promising candidate for accelerating deep learning tasks. In conventional ONNs, light amplitudes are modulated at the input and detected at the output. However, the light phases are still ignored in conventional structures, although they can also carry information for computing. To address this issue, in this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32."
        ],
        [
            "Having the potential for high speed, high throughput, and low energy cost, Optical Neural Networks (ONNs) have emerged as a promising candidate for accelerating deep learning tasks. In conventional ONNs, light amplitudes are modulated at the input and detected at the output. However, the light phases are still ignored in conventional structures, although they can also carry information for computing. To address this issue, in this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs."
        ],
        [
            "Having the potential for high speed, high throughput, and low energy cost, Optical Neural Networks (ONNs) have emerged as a promising candidate for accelerating deep learning tasks. In conventional ONNs, light amplitudes are modulated at the input and detected at the output. However, the light phases are still ignored in conventional structures, although they can also carry information for computing. To address this issue, in this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs.\nIn this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs."
        ],
        [
            "Having the potential for high speed, high throughput, and low energy cost, Optical Neural Networks (ONNs) have emerged as a promising candidate for accelerating deep learning tasks. In conventional ONNs, light amplitudes are modulated at the input and detected at the output. However, the light phases are still ignored in conventional structures, although they can also carry information for computing. To address this issue, in this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs.\nIn this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs."
        ],
        [
            "Having the potential for high speed, high throughput, and low energy cost, Optical Neural Networks (ONNs) have emerged as a promising candidate for accelerating deep learning tasks. In conventional ONNs, light amplitudes are modulated at the input and detected at the output. However, the light phases are still ignored in conventional structures, although they can also carry information for computing. To address this issue, in this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs.\nIn this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs."
        ],
        [
            "Having the potential for high speed, high throughput, and low energy cost, Optical Neural Networks (ONNs) have emerged as a promising candidate for accelerating deep learning tasks. In conventional ONNs, light amplitudes are modulated at the input and detected at the output. However, the light phases are still ignored in conventional structures, although they can also carry information for computing. To address this issue, in this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs.\nIn this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs."
        ],
        [
            "Having the potential for high speed, high throughput, and low energy cost, Optical Neural Networks (ONNs) have emerged as a promising candidate for accelerating deep learning tasks. In conventional ONNs, light amplitudes are modulated at the input and detected at the output. However, the light phases are still ignored in conventional structures, although they can also carry information for computing. To address this issue, in this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs.\nIn this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs."
        ],
        [
            "Having the potential for high speed, high throughput, and low energy cost, Optical Neural Networks (ONNs) have emerged as a promising candidate for accelerating deep learning tasks. In conventional ONNs, light amplitudes are modulated at the input and detected at the output. However, the light phases are still ignored in conventional structures, although they can also carry information for computing. To address this issue, in this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs.\nIn this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs."
        ],
        [
            "Having the potential for high speed, high throughput, and low energy cost, Optical Neural Networks (ONNs) have emerged as a promising candidate for accelerating deep learning tasks. In conventional ONNs, light amplitudes are modulated at the input and detected at the output. However, the light phases are still ignored in conventional structures, although they can also carry information for computing. To address this issue, in this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs.\nIn this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs."
        ],
        [
            "Having the potential for high speed, high throughput, and low energy cost, Optical Neural Networks (ONNs) have emerged as a promising candidate for accelerating deep learning tasks. In conventional ONNs, light amplitudes are modulated at the input and detected at the output. However, the light phases are still ignored in conventional structures, although they can also carry information for computing. To address this issue, in this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs.\nIn this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs."
        ],
        [
            "Having the potential for high speed, high throughput, and low energy cost, Optical Neural Networks (ONNs) have emerged as a promising candidate for accelerating deep learning tasks. In conventional ONNs, light amplitudes are modulated at the input and detected at the output. However, the light phases are still ignored in conventional structures, although they can also carry information for computing. To address this issue, in this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs.\nIn this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs."
        ],
        [
            "Having the potential for high speed, high throughput, and low energy cost, Optical Neural Networks (ONNs) have emerged as a promising candidate for accelerating deep learning tasks. In conventional ONNs, light amplitudes are modulated at the input and detected at the output. However, the light phases are still ignored in conventional structures, although they can also carry information for computing. To address this issue, in this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs.\nIn this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs."
        ],
        [
            "Having the potential for high speed, high throughput, and low energy cost, Optical Neural Networks (ONNs) have emerged as a promising candidate for accelerating deep learning tasks. In conventional ONNs, light amplitudes are modulated at the input and detected at the output. However, the light phases are still ignored in conventional structures, although they can also carry information for computing. To address this issue, in this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs.\nIn this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs."
        ],
        [
            "Having the potential for high speed, high throughput, and low energy cost, Optical Neural Networks (ONNs) have emerged as a promising candidate for accelerating deep learning tasks. In conventional ONNs, light amplitudes are modulated at the input and detected at the output. However, the light phases are still ignored in conventional structures, although they can also carry information for computing. To address this issue, in this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs.\nIn this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs."
        ],
        [
            "Having the potential for high speed, high throughput, and low energy cost, Optical Neural Networks (ONNs) have emerged as a promising candidate for accelerating deep learning tasks. In conventional ONNs, light amplitudes are modulated at the input and detected at the output. However, the light phases are still ignored in conventional structures, although they can also carry information for computing. To address this issue, in this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs.\nIn this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs."
        ],
        [
            "Having the potential for high speed, high throughput, and low energy cost, Optical Neural Networks (ONNs) have emerged as a promising candidate for accelerating deep learning tasks. In conventional ONNs, light amplitudes are modulated at the input and detected at the output. However, the light phases are still ignored in conventional structures, although they can also carry information for computing. To address this issue, in this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs.\nIn this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs."
        ],
        [
            "Having the potential for high speed, high throughput, and low energy cost, Optical Neural Networks (ONNs) have emerged as a promising candidate for accelerating deep learning tasks. In conventional ONNs, light amplitudes are modulated at the input and detected at the output. However, the light phases are still ignored in conventional structures, although they can also carry information for computing. To address this issue, in this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs.\nIn this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs."
        ],
        [
            "Having the potential for high speed, high throughput, and low energy cost, Optical Neural Networks (ONNs) have emerged as a promising candidate for accelerating deep learning tasks. In conventional ONNs, light amplitudes are modulated at the input and detected at the output. However, the light phases are still ignored in conventional structures, although they can also carry information for computing. To address this issue, in this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs.\nIn this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs."
        ],
        [
            "Having the potential for high speed, high throughput, and low energy cost, Optical Neural Networks (ONNs) have emerged as a promising candidate for accelerating deep learning tasks. In conventional ONNs, light amplitudes are modulated at the input and detected at the output. However, the light phases are still ignored in conventional structures, although they can also carry information for computing. To address this issue, in this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs.\nIn this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs."
        ],
        [
            "Having the potential for high speed, high throughput, and low energy cost, Optical Neural Networks (ONNs) have emerged as a promising candidate for accelerating deep learning tasks. In conventional ONNs, light amplitudes are modulated at the input and detected at the output. However, the light phases are still ignored in conventional structures, although they can also carry information for computing. To address this issue, in this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs.\nIn this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs."
        ],
        [
            "Having the potential for high speed, high throughput, and low energy cost, Optical Neural Networks (ONNs) have emerged as a promising candidate for accelerating deep learning tasks. In conventional ONNs, light amplitudes are modulated at the input and detected at the output. However, the light phases are still ignored in conventional structures, although they can also carry information for computing. To address this issue, in this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs.\nIn this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs."
        ],
        [
            "Having the potential for high speed, high throughput, and low energy cost, Optical Neural Networks (ONNs) have emerged as a promising candidate for accelerating deep learning tasks. In conventional ONNs, light amplitudes are modulated at the input and detected at the output. However, the light phases are still ignored in conventional structures, although they can also carry information for computing. To address this issue, in this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs.\nIn this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs."
        ],
        [
            "Having the potential for high speed, high throughput, and low energy cost, Optical Neural Networks (ONNs) have emerged as a promising candidate for accelerating deep learning tasks. In conventional ONNs, light amplitudes are modulated at the input and detected at the output. However, the light phases are still ignored in conventional structures, although they can also carry information for computing. To address this issue, in this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs.\nIn this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs."
        ],
        [
            "Having the potential for high speed, high throughput, and low energy cost, Optical Neural Networks (ONNs) have emerged as a promising candidate for accelerating deep learning tasks. In conventional ONNs, light amplitudes are modulated at the input and detected at the output. However, the light phases are still ignored in conventional structures, although they can also carry information for computing. To address this issue, in this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs.\nIn this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs."
        ],
        [
            "Having the potential for high speed, high throughput, and low energy cost, Optical Neural Networks (ONNs) have emerged as a promising candidate for accelerating deep learning tasks. In conventional ONNs, light amplitudes are modulated at the input and detected at the output. However, the light phases are still ignored in conventional structures, although they can also carry information for computing. To address this issue, in this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs.\nIn this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs."
        ],
        [
            "Having the potential for high speed, high throughput, and low energy cost, Optical Neural Networks (ONNs) have emerged as a promising candidate for accelerating deep learning tasks. In conventional ONNs, light amplitudes are modulated at the input and detected at the output. However, the light phases are still ignored in conventional structures, although they can also carry information for computing. To address this issue, in this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs.\nIn this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs."
        ],
        [
            "Having the potential for high speed, high throughput, and low energy cost, Optical Neural Networks (ONNs) have emerged as a promising candidate for accelerating deep learning tasks. In conventional ONNs, light amplitudes are modulated at the input and detected at the output. However, the light phases are still ignored in conventional structures, although they can also carry information for computing. To address this issue, in this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs.\nIn this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs."
        ],
        [
            "Having the potential for high speed, high throughput, and low energy cost, Optical Neural Networks (ONNs) have emerged as a promising candidate for accelerating deep learning tasks. In conventional ONNs, light amplitudes are modulated at the input and detected at the output. However, the light phases are still ignored in conventional structures, although they can also carry information for computing. To address this issue, in this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs.\nIn this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs."
        ],
        [
            "Having the potential for high speed, high throughput, and low energy cost, Optical Neural Networks (ONNs) have emerged as a promising candidate for accelerating deep learning tasks. In conventional ONNs, light amplitudes are modulated at the input and detected at the output. However, the light phases are still ignored in conventional structures, although they can also carry information for computing. To address this issue, in this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs.\nIn this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs."
        ],
        [
            "Having the potential for high speed, high throughput, and low energy cost, Optical Neural Networks (ONNs) have emerged as a promising candidate for accelerating deep learning tasks. In conventional ONNs, light amplitudes are modulated at the input and detected at the output. However, the light phases are still ignored in conventional structures, although they can also carry information for computing. To address this issue, in this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs.\nIn this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs."
        ],
        [
            "Having the potential for high speed, high throughput, and low energy cost, Optical Neural Networks (ONNs) have emerged as a promising candidate for accelerating deep learning tasks. In conventional ONNs, light amplitudes are modulated at the input and detected at the output. However, the light phases are still ignored in conventional structures, although they can also carry information for computing. To address this issue, in this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs.\nIn this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs."
        ],
        [
            "Having the potential for high speed, high throughput, and low energy cost, Optical Neural Networks (ONNs) have emerged as a promising candidate for accelerating deep learning tasks. In conventional ONNs, light amplitudes are modulated at the input and detected at the output. However, the light phases are still ignored in conventional structures, although they can also carry information for computing. To address this issue, in this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs.\nIn this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs."
        ],
        [
            "Having the potential for high speed, high throughput, and low energy cost, Optical Neural Networks (ONNs) have emerged as a promising candidate for accelerating deep learning tasks. In conventional ONNs, light amplitudes are modulated at the input and detected at the output. However, the light phases are still ignored in conventional structures, although they can also carry information for computing. To address this issue, in this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs.\nIn this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs."
        ],
        [
            "Having the potential for high speed, high throughput, and low energy cost, Optical Neural Networks (ONNs) have emerged as a promising candidate for accelerating deep learning tasks. In conventional ONNs, light amplitudes are modulated at the input and detected at the output. However, the light phases are still ignored in conventional structures, although they can also carry information for computing. To address this issue, in this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs.\nIn this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs."
        ],
        [
            "Having the potential for high speed, high throughput, and low energy cost, Optical Neural Networks (ONNs) have emerged as a promising candidate for accelerating deep learning tasks. In conventional ONNs, light amplitudes are modulated at the input and detected at the output. However, the light phases are still ignored in conventional structures, although they can also carry information for computing. To address this issue, in this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs.\nIn this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs."
        ],
        [
            "Having the potential for high speed, high throughput, and low energy cost, Optical Neural Networks (ONNs) have emerged as a promising candidate for accelerating deep learning tasks. In conventional ONNs, light amplitudes are modulated at the input and detected at the output. However, the light phases are still ignored in conventional structures, although they can also carry information for computing. To address this issue, in this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs.\nIn this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs."
        ],
        [
            "Having the potential for high speed, high throughput, and low energy cost, Optical Neural Networks (ONNs) have emerged as a promising candidate for accelerating deep learning tasks. In conventional ONNs, light amplitudes are modulated at the input and detected at the output. However, the light phases are still ignored in conventional structures, although they can also carry information for computing. To address this issue, in this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs.\nIn this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs."
        ],
        [
            "Having the potential for high speed, high throughput, and low energy cost, Optical Neural Networks (ONNs) have emerged as a promising candidate for accelerating deep learning tasks. In conventional ONNs, light amplitudes are modulated at the input and detected at the output. However, the light phases are still ignored in conventional structures, although they can also carry information for computing. To address this issue, in this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs.\nIn this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs."
        ],
        [
            "Having the potential for high speed, high throughput, and low energy cost, Optical Neural Networks (ONNs) have emerged as a promising candidate for accelerating deep learning tasks. In conventional ONNs, light amplitudes are modulated at the input and detected at the output. However, the light phases are still ignored in conventional structures, although they can also carry information for computing. To address this issue, in this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs.\nIn this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs."
        ],
        [
            "Having the potential for high speed, high throughput, and low energy cost, Optical Neural Networks (ONNs) have emerged as a promising candidate for accelerating deep learning tasks. In conventional ONNs, light amplitudes are modulated at the input and detected at the output. However, the light phases are still ignored in conventional structures, although they can also carry information for computing. To address this issue, in this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs.\nIn this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs."
        ],
        [
            "Having the potential for high speed, high throughput, and low energy cost, Optical Neural Networks (ONNs) have emerged as a promising candidate for accelerating deep learning tasks. In conventional ONNs, light amplitudes are modulated at the input and detected at the output. However, the light phases are still ignored in conventional structures, although they can also carry information for computing. To address this issue, in this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs.\nIn this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs."
        ],
        [
            "Having the potential for high speed, high throughput, and low energy cost, Optical Neural Networks (ONNs) have emerged as a promising candidate for accelerating deep learning tasks. In conventional ONNs, light amplitudes are modulated at the input and detected at the output. However, the light phases are still ignored in conventional structures, although they can also carry information for computing. To address this issue, in this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs.\nIn this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs."
        ],
        [
            "Having the potential for high speed, high throughput, and low energy cost, Optical Neural Networks (ONNs) have emerged as a promising candidate for accelerating deep learning tasks. In conventional ONNs, light amplitudes are modulated at the input and detected at the output. However, the light phases are still ignored in conventional structures, although they can also carry information for computing. To address this issue, in this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs.\nIn this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs."
        ],
        [
            "Having the potential for high speed, high throughput, and low energy cost, Optical Neural Networks (ONNs) have emerged as a promising candidate for accelerating deep learning tasks. In conventional ONNs, light amplitudes are modulated at the input and detected at the output. However, the light phases are still ignored in conventional structures, although they can also carry information for computing. To address this issue, in this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs.\nIn this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs."
        ],
        [
            "Having the potential for high speed, high throughput, and low energy cost, Optical Neural Networks (ONNs) have emerged as a promising candidate for accelerating deep learning tasks. In conventional ONNs, light amplitudes are modulated at the input and detected at the output. However, the light phases are still ignored in conventional structures, although they can also carry information for computing. To address this issue, in this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs.\nIn this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs."
        ],
        [
            "Having the potential for high speed, high throughput, and low energy cost, Optical Neural Networks (ONNs) have emerged as a promising candidate for accelerating deep learning tasks. In conventional ONNs, light amplitudes are modulated at the input and detected at the output. However, the light phases are still ignored in conventional structures, although they can also carry information for computing. To address this issue, in this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs.\nIn this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs."
        ],
        [
            "Having the potential for high speed, high throughput, and low energy cost, Optical Neural Networks (ONNs) have emerged as a promising candidate for accelerating deep learning tasks. In conventional ONNs, light amplitudes are modulated at the input and detected at the output. However, the light phases are still ignored in conventional structures, although they can also carry information for computing. To address this issue, in this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs.\nIn this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs."
        ],
        [
            "Having the potential for high speed, high throughput, and low energy cost, Optical Neural Networks (ONNs) have emerged as a promising candidate for accelerating deep learning tasks. In conventional ONNs, light amplitudes are modulated at the input and detected at the output. However, the light phases are still ignored in conventional structures, although they can also carry information for computing. To address this issue, in this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs.\nIn this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs."
        ],
        [
            "Having the potential for high speed, high throughput, and low energy cost, Optical Neural Networks (ONNs) have emerged as a promising candidate for accelerating deep learning tasks. In conventional ONNs, light amplitudes are modulated at the input and detected at the output. However, the light phases are still ignored in conventional structures, although they can also carry information for computing. To address this issue, in this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs.\nIn this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs."
        ],
        [
            "Having the potential for high speed, high throughput, and low energy cost, Optical Neural Networks (ONNs) have emerged as a promising candidate for accelerating deep learning tasks. In conventional ONNs, light amplitudes are modulated at the input and detected at the output. However, the light phases are still ignored in conventional structures, although they can also carry information for computing. To address this issue, in this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\n\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\n\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs.\n\nIn this paper, we propose a framework called OPLIXNet to compress the areas of ONNs by modulating input image data into the amplitudes and phase parts of light signals. The input and output parts of the ONNs are redesigned to make full use of both amplitude and phase information. Moreover, mutual learning across different ONN structures is introduced to maintain the accuracy. Experimental results demonstrate that the proposed framework significantly reduces the areas of ONNs with the accuracy within an acceptable range. For instance, 75.03% area is reduced with a 0.33% accuracy decrease on Fully Connected Neural Network (FCNN) and 74.88% area is reduced with a 2.38% accuracy decrease on ResNet-32.\n\nThe results of the proposed framework are compared with those of the baseline model, as shown in Table 1. The proposed framework achieves better performance than the baseline model in terms of area reduction and accuracy. In particular, the proposed framework reduces the area by 75.03% with a 0.33% accuracy decrease on FCNN, and reduces the area by 74.88% with a 2.38% accuracy decrease on ResNet-32. These results demonstrate that the proposed framework can effectively compress the areas of ONNs while maintaining the accuracy.\n\nThe proposed framework is evaluated on two benchmark datasets: MNIST and CIFAR-10. MNIST is a handwritten digit dataset, consisting of 60,000 training images and 10,000 test images. CIFAR-10 is a dataset of 50,000 training images and 10,000 test images, with each image belonging to one of 10 classes. The proposed framework is implemented in Python using the PyTorch framework. The input images are resized to 32x32 pixels. The model is trained for 200 epochs with a batch size of 64. The learning rate is set to 0.001. The proposed framework is evaluated on a server with 4 NVIDIA Tesla V100 GPUs."
        ]
    ]
}