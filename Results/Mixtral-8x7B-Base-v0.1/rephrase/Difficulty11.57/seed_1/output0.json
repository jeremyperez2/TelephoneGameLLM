{
    "adjacency_matrix": [
        [
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0
        ]
    ],
    "prompt_init": [
        "You will receive a text. Your task is to rephrase this text without modifying its meaning. Just output your new text, nothing else. Here is the text:"
    ],
    "prompt_update": [
        "You will receive a text. Your task is to rephrase this text without modifying its meaning. Just output your new text, nothing else. Here is the text:"
    ],
    "personality_list": [
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        ""
    ],
    "format_prompt": [
        ""
    ],
    "initial_story": "having the potential for high speed, high throughput, and low energy cost,optical neural networks (onns) have emerged as a promising candidate foraccelerating deep learning tasks. in conventional onns, light amplitudes aremodulated at the input and detected at the output. however, the light phasesare still ignored in conventional structures, although they can also carryinformation for computing. to address this issue, in this paper, we propose aframework called oplixnet to compress the areas of onns by modulating inputimage data into the amplitudes and phase parts of light signals. the input andoutput parts of the onns are redesigned to make full use of both amplitude andphase information. moreover, mutual learning across different onn structures isintroduced to maintain the accuracy. experimental results demonstrate that theproposed framework significantly reduces the areas of onns with the accuracywithin an acceptable range. for instance, 75.03% area is reduced with a 0.33%accuracy decrease on fully connected neural network (fcnn) and 74.88% area isreduced with a 2.38% accuracy decrease on resnet-32. ",
    "stories": [
        [
            "to optimize the area of optical neural networks (onns), a framework called oplixnet is proposed to modulate input image data into both the amplitudes and phase parts of light signals. the input and output parts of the onns are redesigned to make full use of both amplitude and phase information. mutual learning across different onn structures is introduced to maintain the accuracy. experimental results demonstrate that the proposed framework significantly reduces the areas of onns with the accuracy within an acceptable range. for instance, 75.03% area is reduced with a 0.33% accuracy decrease on fully connected neural network (fcnn) and 74.88% area is reduced with a 2.38% accuracy decrease on resnet-32."
        ],
        [
            "To reduce the area of optical neural networks (ONNs), a framework called Oplixnet is proposed. It optimizes the input image data by using both amplitude and phase parts of light signals. The framework redesigns the input and output parts of the ONNs to make full use of both amplitude and phase information. Mutual learning across different ONN structures is introduced to maintain accuracy. Experimental results show that the proposed framework significantly reduces the areas of ONNs while keeping the accuracy within an acceptable range. For example, the area is reduced by 75.03% with a 0.33% accuracy decrease on fully connected neural network (FCNN), and by 74.88% with a 2.38% accuracy decrease on ResNet-32."
        ],
        [
            "To optimize the input image data in optical neural networks (ONNs), the Oplixnet framework is proposed. The framework redesigns the input and output parts of the ONNs to make full use of both amplitude and phase information. Mutual learning across different ONN structures is introduced to maintain accuracy. Experimental results show that the proposed framework significantly reduces the areas of ONNs while keeping the accuracy within an acceptable range. For example, the area is reduced by 75.03% with a 0.33% accuracy decrease on fully connected neural network (FCNN), and by 74.88% with a 2.38% accuracy decrease on ResNet-32."
        ],
        [
            "Optical neural networks (ONNs) have the potential to optimize the input image data, but the challenge is to fully utilize both amplitude and phase information without sacrificing accuracy. To address this challenge, the Oplixnet framework is proposed, which redesigns the input and output parts of the ONNs to make use of the amplitude and phase information. Additionally, mutual learning across different ONN structures is introduced to maintain accuracy. Experimental results show that the proposed framework significantly reduces the areas of ONNs while keeping the accuracy within an acceptable range. For example, the area is reduced by 75.03% with a 0.33% accuracy decrease on fully connected neural network (FCNN), and by 74.88% with a 2.38% accuracy decrease on ResNet-32."
        ],
        [
            "The Optical neural network (ONN) framework is designed to reduce the area of ONNs while maintaining accuracy. The framework redesigns the input and output parts of ONNs to make use of the amplitude and phase information. Additionally, mutual learning across different ONN structures is introduced to maintain accuracy. Experimental results show that the proposed framework significantly reduces the areas of ONNs while keeping the accuracy within an acceptable range. For example, the area is reduced by 75.03% with a 0.33% accuracy decrease on fully connected neural network (FCNN), and by 74.88% with a 2.38% accuracy decrease on ResNet-32."
        ],
        [
            "The Optical neural network (ONN) framework is designed to significantly reduce the area of ONNs while maintaining accuracy. The framework redesigns the input and output parts of ONNs to make use of the amplitude and phase information. Additionally, mutual learning across different ONN structures is introduced to maintain accuracy. Experimental results show that the proposed framework significantly reduces the areas of ONNs while keeping the accuracy within an acceptable range. For example, the area is reduced by 75.03% with a 0.33% accuracy decrease on fully connected neural network (FCNN), and by 74.88% with a 2.38% accuracy decrease on ResNet-32."
        ],
        [
            "The Optical neural network (ONN) framework is designed to significantly reduce the area of ONNs while maintaining accuracy. This is achieved by redesigning the input and output parts of ONNs to make use of the amplitude and phase information, and by introducing mutual learning across different ONN structures to maintain accuracy. The experimental results show that the proposed framework significantly reduces the areas of ONNs while keeping the accuracy within an acceptable range. For example, the area is reduced by 75.03% with a 0.33% accuracy decrease on fully connected neural network (FCNN), and by 74.88% with a 2.38% accuracy decrease on ResNet-32."
        ],
        [
            "The Optical neural network (ONN) framework is designed to greatly reduce the area of ONNs while maintaining their accuracy. This is achieved by redesigning the input and output parts of ONNs to make use of the amplitude and phase information, and by introducing mutual learning across different ONN structures to maintain accuracy. The experimental results show that the proposed framework significantly reduces the areas of ONNs while keeping the accuracy within an acceptable range. For example, the area is reduced by 75.03% with a 0.33% accuracy decrease on fully connected neural network (FCNN), and by 74.88% with a 2.38% accuracy decrease on ResNet-32."
        ],
        [
            "The Optical neural network (ONN) framework is designed to greatly reduce the area of ONNs while maintaining their accuracy. This is achieved by redesigning the input and output parts of ONNs to make use of the amplitude and phase information, and by introducing mutual learning across different ONN structures to maintain accuracy. The experimental results show that the proposed framework significantly reduces the areas of ONNs while keeping the accuracy within an acceptable range. For instance, the area is reduced by 75.03% with a 0.33% accuracy decrease on fully connected neural network (FCNN), and by 74.88% with a 2.38% accuracy decrease on ResNet-32."
        ],
        [
            "The Optical neural network (ONN) framework is designed to significantly reduce the area of ONNs while preserving their accuracy. This is achieved by redesigning the input and output parts of ONNs to leverage their amplitude and phase information, and by implementing mutual learning across different ONN architectures to maintain accuracy. The experimental results demonstrate that the proposed framework greatly reduces the areas of ONNs while keeping the accuracy within an acceptable range. For example, the area is reduced by 75.03% with a 0.33% accuracy drop on fully connected neural network (FCNN), and by 74.88% with a 2.38% accuracy drop on ResNet-32."
        ],
        [
            "The Optical Neural Network (ONN) framework has been redesigned to reduce its area while maintaining accuracy. This was achieved by redesigning the input and output parts of the ONN to take advantage of its amplitude and phase information, as well as by implementing mutual learning across different ONN architectures to maintain accuracy. The experimental results demonstrate that the proposed framework greatly reduces the areas of ONNs while keeping the accuracy within an acceptable range. For example, the area is reduced by 75.03% with a 0.33% accuracy drop on fully connected neural network (FCNN), and by 74.88% with a 2.38% accuracy drop on ResNet-32."
        ],
        [
            "The Optical Neural Network (ONN) framework has been redesigned to shrink its area while preserving accuracy. This was accomplished by redesigning the input and output portions of the ONN to utilize its amplitude and phase data, as well as by carrying out mutual learning across various ONN architectures to preserve accuracy. The experimental results show that the proposed framework considerably reduces the areas of ONNs while maintaining accuracy within an acceptable range. For instance, the area is reduced by 75.03% with a 0.33% accuracy drop on fully connected neural network (FCNN), and by 74.88% with a 2.38% accuracy drop on ResNet-32."
        ],
        [
            "The Optical Neural Network (ONN) framework has been redesigned to minimize its area while retaining accuracy. This was achieved by redesigning the input and output sections of the ONN to use its amplitude and phase data and by performing mutual learning across different ONN architectures to preserve accuracy. The experimental results show that the proposed framework significantly reduces the areas of ONNs while maintaining accuracy within an acceptable range. For instance, the area is reduced by 75.03% with a 0.33% accuracy drop on fully connected neural network (FCNN), and by 74.88% with a 2.38% accuracy drop on ResNet-32."
        ],
        [
            "The Optical Neural Network (ONN) framework has been redesigned with a focus on reducing its area while maintaining accuracy. This was achieved by implementing two strategies: (1) redesigning the input and output sections to make use of amplitude and phase data, and (2) performing mutual learning across different ONN architectures to preserve accuracy. Experimental results demonstrate that the proposed framework significantly reduces the area of ONNs while maintaining accuracy within an acceptable range. For example, the area is reduced by 75.03% with a 0.33% accuracy drop on fully connected neural network (FCNN), and by 74.88% with a 2.38% accuracy drop on ResNet-32."
        ],
        [
            "The Optical Neural Network (ONN) framework has undergone redesign to decrease its area while sustaining accuracy. This objective was attained by implementing two techniques: (1) refashioning the input and output segments to capitalize on amplitude and phase data, and (2) executing mutual training across various ONN architectures to conserve accuracy. Experimental outcomes reveal that the proposed framework substantially lessens the area of ONNs while preserving accuracy within an acceptable range. For instance, the area is reduced by 75.03% with a 0.33% accuracy drop on fully connected neural network (FCNN), and by 74.88% with a 2.38% accuracy drop on ResNet-32."
        ],
        [
            "The Optical Neural Network (ONN) framework has been redesigned to minimize its area while maintaining accuracy. This was achieved by implementing two strategies: (1) modifying the input and output segments to make the most of amplitude and phase data, and (2) performing concurrent training across different ONN architectures to preserve accuracy. Experimental results demonstrate that the proposed framework significantly reduces the area of ONNs while maintaining accuracy within an acceptable range. For instance, the area is reduced by 75.03% with a 0.33% accuracy drop on fully connected neural network (FCNN), and by 74.88% with a 2.38% accuracy drop on ResNet-32."
        ],
        [
            "A newly developed Optical Neural Network (ONN) framework has been redesigned to minimize its area while maintaining its accuracy. The reduction in area was achieved by implementing two strategies: first, modifying the input and output segments to optimize the use of amplitude and phase data; and second, performing concurrent training across different ONN architectures to maintain accuracy. Experimental results show that the proposed framework significantly reduces the area of ONNs while keeping accuracy within an acceptable range. For example, the area is reduced by 75.03% with only a 0.33% accuracy drop in a fully connected neural network (FCNN), and by 74.88% with a 2.38% accuracy drop in a ResNet-32 network."
        ],
        [
            "Optical Neural Networks (ONNs) have been redesigned to minimize their area while maintaining their accuracy. The area reduction was achieved by optimizing the use of amplitude and phase data in the input and output segments, and by performing concurrent training across different ONN architectures to maintain accuracy. Experimental results show that the proposed framework significantly reduces the area of ONNs while keeping accuracy within an acceptable range. For example, the area is reduced by 75.03% with only a 0.33% accuracy drop in a fully connected neural network (FCNN), and by 74.88% with a 2.38% accuracy drop in a ResNet-32 network."
        ],
        [
            "Researchers have developed a new method for creating optical neural networks (ONNs) that are smaller in area while maintaining their accuracy. The area reduction was achieved by optimizing the use of amplitude and phase data in the input and output segments, and by performing concurrent training across different ONN architectures to maintain accuracy. Experimental results show that the proposed framework significantly reduces the area of ONNs while keeping accuracy within an acceptable range. For example, the area is reduced by 75.03% with only a 0.33% accuracy drop in a fully connected neural network (FCNN), and by 74.88% with a 2.38% accuracy drop in a ResNet-32 network."
        ],
        [
            "Researchers have developed a new method for reducing the area of optical neural networks (ONNs) while maintaining their accuracy. The reduction was achieved by optimizing the use of amplitude and phase data, and by concurrent training across different ONN architectures to maintain accuracy. Experimental results show that the proposed framework significantly reduces the area of ONNs while keeping accuracy within an acceptable range. For example, the area is reduced by 75.03% with only a 0.33% accuracy drop in a fully connected neural network (FCNN), and by 74.88% with a 2.38% accuracy drop in a ResNet-32 network."
        ],
        [
            "In order to reduce the area of optical neural networks (ONNs) while maintaining their accuracy, researchers have developed a novel method that involves optimizing the use of amplitude and phase data. This method also involves concurrent training across different ONN architectures to maintain accuracy. Experimental results show that this framework significantly reduces the area of ONNs while keeping accuracy within an acceptable range. For instance, in a fully connected neural network (FCNN), the area is reduced by 75.03% with only a 0.33% accuracy drop, and in a ResNet-32 network, the area is reduced by 74.88% with a 2.38% accuracy drop."
        ],
        [
            "Researchers have developed a new method that combines the optimization of amplitude and phase data with concurrent training across various ONN architectures to reduce the area of ONNs while maintaining accuracy. Experimental results show that this framework significantly reduces the area of ONNs, with an acceptable accuracy drop. For example, in an FCNN, the area is reduced by 75.03% with only a 0.33% accuracy drop, and in a ResNet-32 network, the area is reduced by 74.88% with a 2.38% accuracy drop."
        ],
        [
            "Researchers have introduced a novel approach that integrates the optimization of amplitude and phase information with simultaneous training of multiple ONN architectures to decrease the ONN area while preserving accuracy. Experimental results indicate that this framework substantially reduces the ONN area, with an acceptable accuracy drop. For instance, in an FCNN, the area is reduced by 75.03% with a mere 0.33% accuracy drop, and in a ResNet-32 network, the area is reduced by 74.88% with a 2.38% accuracy drop."
        ],
        [
            "Researchers have introduced a novel approach to optimize amplitude and phase information while training multiple ONN architectures simultaneously, resulting in reduced ONN area while maintaining accuracy. Experimental results show that this framework significantly decreases the ONN area, with a minimal accuracy drop. For example, in an FCNN, the area is reduced by 75.03% with a 0.33% accuracy drop, and in a ResNet-32 network, the area is reduced by 74.88% with a 2.38% accuracy drop."
        ],
        [
            "In this paper, researchers have presented a new technique for optimizing amplitude and phase information while training multiple ONN architectures simultaneously. This approach leads to a reduction in the ONN area while maintaining accuracy. The experimental results demonstrate that this framework significantly decreases the ONN area, with only a slight accuracy drop. For instance, in an FCNN, the area is reduced by 75.03%, with a 0.33% accuracy drop. Similarly, in a ResNet-32 network, the area is reduced by 74.88%, with a 2.38% accuracy drop."
        ],
        [
            "A new technique for optimizing amplitude and phase information during the training of multiple ONN architectures simultaneously has been presented in this paper. The researchers claim that this approach reduces the ONN area while maintaining accuracy. The experimental results show that the framework significantly decreases the ONN area, with only a slight accuracy drop. For instance, in an FCNN, the area is reduced by 75.03%, with a 0.33% accuracy drop. In a ResNet-32 network, the area is reduced by 74.88%, with a 2.38% accuracy drop."
        ],
        [
            "This paper introduces a new technique for optimizing amplitude and phase information during the training of multiple ONN architectures simultaneously. The researchers claim that this approach can reduce the ONN area while maintaining accuracy. The experimental results show that the framework significantly decreases the ONN area, with only a slight accuracy drop. For example, in an FCNN, the area is reduced by 75.03%, with a 0.33% accuracy drop. In a ResNet-32 network, the area is reduced by 74.88%, with a 2.38% accuracy drop."
        ],
        [
            "A group of researchers have developed a new technique that optimizes the amplitude and phase information of multiple ONN architectures during the training process. They claim that this approach can reduce the ONN area while maintaining accuracy. The experimental results showed that the framework significantly decreased the ONN area, with only a slight accuracy drop. For example, in an FCNN, the area was reduced by 75.03%, with a 0.33% accuracy drop, and in a ResNet-32 network, the area was reduced by 74.88%, with a 2.38% accuracy drop."
        ],
        [
            "A team of researchers has developed a method that optimizes the training process for optical neural networks (ONNs) by adjusting the amplitude and phase information. They claim that this approach can reduce the size of the ONN while maintaining accuracy. The experimental results showed that their framework significantly reduced the size of the ONN with only a minor decrease in accuracy. For example, in a fully connected neural network (FCNN), the size was reduced by 75.03%, with a 0.33% accuracy drop, and in a ResNet-32 network, the size was reduced by 74.88%, with a 2.38% accuracy drop."
        ],
        [
            "A team of researchers has developed a method to optimize the training process for optical neural networks (ONNs) by adjusting amplitude and phase information. They claim that this approach can reduce the size of the ONN while maintaining accuracy. Their experimental results showed that their framework significantly reduced the size of the ONN with only a minor accuracy drop. For example, in an FCNN, the size was reduced by 75.03%, with a 0.33% accuracy drop, and in a ResNet-32 network, the size was reduced by 74.88%, with a 2.38% accuracy drop."
        ],
        [
            "Scientists have developed a method to optimize the training process for optical neural networks (ONNs) by adjusting amplitude and phase information. This approach can reduce the size of the ONN while maintaining accuracy. The experimental results showed that their framework significantly reduced the size of the ONN with only a minor accuracy drop. For example, in an FCNN, the size was reduced by 75.03%, with a 0.33% accuracy drop, and in a ResNet-32 network, the size was reduced by 74.88%, with a 2.38% accuracy drop."
        ],
        [
            "Researchers have created a method to optimize the training process for optical neural networks (ONNs) by adjusting amplitude and phase information. The aim is to minimize the size of the ONN without compromising accuracy. Experiments showed that their framework significantly reduced the size of the ONN with only a minor accuracy drop. For example, in an FCNN, the size was reduced by 75.03%, with a 0.33% accuracy drop, and in a ResNet-32 network, the size was reduced by 74.88%, with a 2.38% accuracy drop."
        ],
        [
            "Researchers have developed a technique to optimize the training process of optical neural networks (ONNs) by adjusting the amplitude and phase information to decrease the size of the ONN without affecting accuracy. Experiments showed that their framework reduced the size of the ONN with a slight accuracy drop. For instance, in an FCNN, the size was reduced by 75.03%, with a 0.33% accuracy drop, and in a ResNet-32 network, the size was reduced by 74.88%, with a 2.38% accuracy drop."
        ],
        [
            "Researchers have developed a technique to optimize the training process of optical neural networks (ONNs) by adjusting the amplitude and phase information to decrease the size of the ONN without affecting accuracy. The experiments showed that their framework reduced the size of the ONN with a slight accuracy drop. For instance, in an FCNN, the size was reduced by 75.03%, with a 0.33% accuracy drop, and in a ResNet-32 network, the size was reduced by 74.88%, with a 2.38% accuracy drop."
        ],
        [
            "Scientists have created a method for optimizing the training procedure of optical neural networks (ONNs) by modifying the amplitude and phase information to lower the size of the ONN with no change in accuracy. The experiments showed that their technique reduced the size of the ONN with only a slight accuracy drop. For example, in an FCNN, the size was reduced by 75.03%, with a 0.33% accuracy drop, and in a ResNet-32 network, the size was reduced by 74.88%, with a 2.38% accuracy drop."
        ],
        [
            "Scientists have developed a method to optimize the training process of optical neural networks (ONNs) by altering the amplitude and phase data to decrease the size of the ONN while keeping the accuracy constant. The experiments demonstrated that their approach reduced the size of the ONN with a slight decrease in accuracy. For instance, in an FCNN, the size was reduced by 75.03%, with a 0.33% accuracy drop, and in a ResNet-32 network, the size was reduced by 74.88%, with a 2.38% accuracy drop."
        ],
        [
            "Researchers have found a way to optimize the training process of optical neural networks (ONNs) by changing the amplitude and phase data. This method leads to a smaller ONN size while maintaining accuracy. The results show that in an FCNN, the size decreased by 75.03% with a 0.33% accuracy drop. In a ResNet-32 network, the size reduced by 74.88% with a 2.38% accuracy drop."
        ],
        [
            "Researchers have discovered a technique to improve the training process of optical neural networks (ONNs) by modifying the amplitude and phase data. This method leads to a smaller ONN size while maintaining accuracy. According to the results, an FCNN's size shrank by 75.03% with a 0.33% accuracy drop. In a ResNet-32 network, the size decreased by 74.88% with a 2.38% accuracy drop."
        ],
        [
            "Scientists have discovered a method to optimize optical neural networks (ONNs) by adjusting amplitude and phase data. This technique reduces the network's size while preserving accuracy. The results show that a fully connected neural network's (FCNN) size shrunk by 75.03%, with an accuracy drop of only 0.33%. For a ResNet-32 network, the size decreased by 74.88%, with an accuracy drop of 2.38%."
        ],
        [
            "A groundbreaking discovery by scientists has revolutionized the way we understand and manipulate optical neural networks (ONNs). By meticulously adjusting the amplitude and phase data within these networks, researchers have devised an ingenious method to compress and optimize them. This innovative technique has demonstrated remarkable results, significantly reducing the size of fully connected neural networks (FCNNs) by a staggering 75.03%, with only a minimal accuracy drop of 0.33%. Similarly, in the case of ResNet-32 networks, the size has been condensed by an astounding 74.88%, while maintaining an impressive accuracy level with a marginal drop of 2.38%."
        ],
        [
            "Researchers have made a groundbreaking discovery in the field of optical neural networks (ONNs), revealing a way to compress and optimize them by adjusting their amplitude and phase data. This innovative technique has resulted in a significant reduction in the size of fully connected neural networks (FCNNs) by 75.03%, with only a minor accuracy drop of 0.33%. Similarly, in the case of ResNet-32 networks, the size has been condensed by an impressive 74.88%, with an accuracy drop of just 2.38%."
        ],
        [
            "Scientists have made a remarkable breakthrough in the study of optical neural networks (ONNs), demonstrating a novel technique to reduce and optimize them by manipulating their amplitude and phase data. This cutting-edge method has led to a substantial reduction in the size of fully connected neural networks (FCNNs) by a striking 75.03%, with only a slight decrease in accuracy of 0.33%. Likewise, in the case of ResNet-32 networks, the size has been condensed by a remarkable 74.88%, with an accuracy drop of only 2.38%."
        ],
        [
            "Researchers have made a breakthrough in the study of optical neural networks (ONNs) by introducing a novel method for reducing and optimizing their size. Using this technique, they have successfully reduced the size of fully connected neural networks (FCNNs) by 75.03% while only decreasing accuracy by 0.33%. They have also applied this method to ResNet-32 networks, shrinking their size by 74.88% with a minimal accuracy drop of 2.38%."
        ],
        [
            "Researchers have made a significant advancement in the field of optical neural networks (ONNs) by introducing a novel technique for reducing and optimizing their size. This method has successfully minimized the size of fully connected neural networks (FCNNs) by 75.03%, resulting in a minimal accuracy reduction of 0.33%. They have also applied this technique to ResNet-32 networks, shrinking their size by 74.88% with only a small accuracy drop of 2.38%."
        ],
        [
            "Scientists have achieved a notable breakthrough in the realm of optical neural networks (ONNs) by developing a novel approach to downsize and optimize them. This innovative technique has reduced the size of fully connected neural networks (FCNNs) by 75.03% while only causing a minimal accuracy decrease of 0.33%. Furthermore, they have applied this method to ResNet-32 networks, trimming their size by 74.88% while experiencing only a small accuracy drop of 2.38%."
        ],
        [
            "Researchers have made significant progress in the field of optical neural networks (ONNs) by introducing a novel technique to reduce the size of fully connected neural networks (FCNNs) by 75.03% and ResNet-32 networks by 74.88%. Despite some accuracy loss of 0.33% for FCNNs and 2.38% for ResNet-32, the method has proven to be a promising approach for optimizing ONNs."
        ],
        [
            "Recent research in the realm of optical neural networks (ONNs) has led to a significant reduction in the size of fully connected neural networks (FCNNs) by 75.03% and ResNet-32 networks by 74.88%. While some accuracy loss is observed, with FCNNs losing 0.33% and ResNet-32 losing 2.38%, this innovative method demonstrates promising potential for optimizing ONNs."
        ],
        [
            "Researchers have found a way to make optical neural networks (ONNs) smaller by 75.03% for fully connected neural networks (FCNNs) and 74.88% for ResNet-32 networks. While there is some loss of accuracy, with FCNNs losing 0.33% and ResNet-32 losing 2.38%, this innovative method shows potential for improving ONNs."
        ],
        [
            "Scientists have discovered a technique to make optical neural networks (ONNs) 75.03% smaller for fully connected neural networks (FCNNs) and 74.88% smaller for ResNet-32 networks. Although there is some decrease in accuracy, with FCNNs losing 0.33% and ResNet-32 losing 2.38%, this new approach offers promise for enhancing ONNs."
        ],
        [
            "Researchers have created a new method to decrease the size of optical neural networks (ONNs) by 75.03% for fully connected neural networks (FCNNs) and 74.88% for ResNet-32 networks. While there is a slight decrease in accuracy, with FCNNs losing 0.33% and ResNet-32 losing 2.38%, this new technique provides a promising way to enhance ONNs."
        ]
    ]
}