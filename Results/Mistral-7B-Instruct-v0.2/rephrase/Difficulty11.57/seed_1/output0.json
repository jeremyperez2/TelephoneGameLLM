{
    "adjacency_matrix": [
        [
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0
        ]
    ],
    "prompt_init": [
        "You will receive a text. Your task is to rephrase this text without modifying its meaning. Just output your new text, nothing else. Here is the text:"
    ],
    "prompt_update": [
        "You will receive a text. Your task is to rephrase this text without modifying its meaning. Just output your new text, nothing else. Here is the text:"
    ],
    "personality_list": [
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        ""
    ],
    "format_prompt": [
        ""
    ],
    "initial_story": "having the potential for high speed, high throughput, and low energy cost,optical neural networks (onns) have emerged as a promising candidate foraccelerating deep learning tasks. in conventional onns, light amplitudes aremodulated at the input and detected at the output. however, the light phasesare still ignored in conventional structures, although they can also carryinformation for computing. to address this issue, in this paper, we propose aframework called oplixnet to compress the areas of onns by modulating inputimage data into the amplitudes and phase parts of light signals. the input andoutput parts of the onns are redesigned to make full use of both amplitude andphase information. moreover, mutual learning across different onn structures isintroduced to maintain the accuracy. experimental results demonstrate that theproposed framework significantly reduces the areas of onns with the accuracywithin an acceptable range. for instance, 75.03% area is reduced with a 0.33%accuracy decrease on fully connected neural network (fcnn) and 74.88% area isreduced with a 2.38% accuracy decrease on resnet-32. ",
    "stories": [
        [
            "Optical neural networks (ONNs), known for their capacity for fast speeds, large data transfer rates, and low energy consumption, have gained significant attention as a potential solution for accelerating deep learning tasks. In traditional ONNs, light amplitude modifications occur at the input, while detections are made at the output. However, the significance of light phases, which also hold valuable information for computation, is overlooked in conventional designs. To rectify this, this study introduces a new framework called OplixNet. This framework compresses ONN areas by encoding input data into both amplitude and phase parts of light signals. By redesigning the ONN input and output components to utilize both amplitude and phase information, as well as implementing mutual learning across various ONN structures, OplixNet maintains accuracy while significantly reducing ONN area requirements. For instance, a 0.33% accuracy decrease corresponds to a 75.03% reduction in area for fully connected neural networks (FCNNs), and a 2.38% accuracy decrease results in a 74.88% area reduction for ResNet-32."
        ],
        [
            "Text rephrased: Optical neural networks (ONNs), recognized for their quick processing speeds, substantial data transfer capabilities, and minimal energy consumption, have garnered substantial interest as a viable option for expediting deep learning processes. Conventional ONNs operate by inducing light amplitude alterations at the input and identifying outputs at the termination. Nevertheless, the importance of light phases, which contain crucial information for computations, is disregarded in standard designs. To address this issue, this research proposes a novel framework called OplixNet. This framework condenses ONN regions by encoding input data into both the amplitude and phase components of light pulses. By modifying the ONN input and output elements to exploit amplitude and phase data, as well as fostering mutual learning among various ONN architectures, OplixNet preserves precision while substantially decreasing ONN size requirements. For example, a 0.33% accuracy decline leads to a 75.03% reduction in area for fully connected neural networks (FCNNs), and a 2.38% accuracy decrease translates to a 74.88% reduction in area for ResNet-32."
        ],
        [
            "Optical neural networks (ONNs), known for their fast processing speeds, large data transfer capacities, and low energy consumption, have attracted significant attention as a potential solution for accelerating deep learning processes. Traditional ONNs function by generating light amplitude changes at the input and detecting outputs at the end. However, the significance of light phases, which hold essential data for computations, is overlooked in traditional architectures. To tackle this challenge, this study introduces a new framework named OplixNet. This framework compresses ONN regions by encoding input information into both the amplitude and phase aspects of light pulses. By transforming ONN input and output components to utilize amplitude and phase data and encouraging mutual learning among distinct ONN designs, OplixNet maintains accuracy while drastically cutting ONN size specifications. For instance, a 0.33% decrease in accuracy results in a 75.03% reduction in area for fully connected neural networks (FCNNs), while a 2.38% accuracy decrease corresponds to a 74.88% reduction in area for ResNet-32."
        ],
        [
            "Text rephrased: Optical neural networks (ONNs), recognized for their swift processing speeds, extensive data transfer capabilities, and minimal energy consumption, have garnered considerable interest as a probable method for expediting deep learning operations. Conventionally, ONNs operate by inducing light intensity adjustments at the entrance and identifying outputs towards the end. Nevertheless, the importance of light phases, which carry crucial information for calculations, is disregarded in conventional layouts. To address this issue, this research proposes a novel framework called OplixNet. This framework condenses ONN regions by encoding input details into both the amplitude and phase dimensions of light signals. By altering ONN input and output elements to harness amplitude and phase data and promoting cooperative learning among distinct ONN designs, OplixNet preserves precision while markedly diminishing ONN size requirements. For illustration, a 0.33% decline in precision brings about a 75.03% decrease in area for fully connected neural networks (FCNNs), while a 2.38% precision reduction corresponds to a 74.88% reduction in area for ResNet-32."
        ],
        [
            "Optical neural networks (ONNs), known for their fast processing speeds, large data transfer capacities, and low energy consumption, have gained significant attention as a potential solution for accelerating deep learning processes. Traditionally, ONNs function by causing light intensity alterations at the input and detecting outputs at the end. However, the significance of light phases, which contain vital information for calculations, is overlooked in conventional architectures. To tackle this problem, this study proposes a new framework named OplixNet. This framework compresses ONN regions by encoding input information into both the amplitude and phase dimensions of light pulses. By modifying ONN input and output components to utilize amplitude and phase data and fostering collaborative learning among distinct ONN designs, OplixNet maintains accuracy while substantially reducing ONN size demands. For instance, a 0.33% drop in accuracy results in a 75.03% decrease in area for fully connected neural networks (FCNNs), while a 2.38% accuracy reduction corresponds to a 74.88% reduction in area for ResNet-32."
        ],
        [
            "Fast and energy-efficient optical neural networks (ONNs) have attracted considerable interest due to their ability to expedite deep learning processes with high processing speeds, large data transfer capacities, and low energy consumption. However, the importance of light phases, which carry essential data for computations, is often disregarded in conventional ONN architectures. To address this issue, this research introduces a novel framework called OplixNet. OplixNet condenses ONN regions by encoding input data into both the amplitude and phase dimensions of light pulses. By adjusting ONN input and output components to process amplitude and phase information and promoting cooperative learning among different ONN designs, OplixNet preserves accuracy while drastically reducing ONN size requirements. For example, a 0.33% decrease in accuracy for fully connected neural networks (FCNNs) results in a 75.03% reduction in area, while a 2.38% accuracy reduction for ResNet-32 corresponds to a 74.88% reduction in area."
        ],
        [
            "Optical neural networks (ONNs), known for their fast processing speeds, large data transfer capacities, and low energy consumption, have garnered significant attention due to their potential to accelerate deep learning processes. Yet, the significance of light phases, which convey vital data for computations, is frequently overlooked in standard ONN designs. To tackle this challenge, this study proposes a new framework named OplixNet. OplixNet compresses ONN regions by encoding input data into both the intensity and phase attributes of light pulses. By tailoring ONN input and output elements to process amplitude and phase details and fostering collaborative learning among distinct ONN architectures, OplixNet maintains precision while markedly decreasing ONN size specifications. For instance, a 0.33% decline in accuracy for fully connected neural networks (FCNNs) brings about a 75.03% decrease in area, while a 2.38% reduction in accuracy for ResNet-32 leads to a 74.88% reduction in area."
        ],
        [
            "Fast processing and low energy consumption in optical neural networks (ONNs) have made them a popular choice for deep learning applications. However, the importance of light phases, which carry crucial data for computations, is often neglected in conventional ONN architectures. To address this issue, this research introduces a novel framework called OplixNet. OplixNet condenses ONN regions by encoding input data into both the intensity and phase components of light pulses. By customizing ONN input and output components to handle amplitude and phase information, and encouraging collaborative learning among distinct ONN designs, OplixNet preserves accuracy while substantially reducing ONN size requirements. For illustration, a 0.33% drop in accuracy for fully connected neural networks (FCNNs) results in a 75.03% decrease in area, while a 2.38% decrease in accuracy for ResNet-32 leads to a 74.88% reduction in area."
        ],
        [
            "Optical neural networks (ONNs) are widely used for deep learning due to their rapid processing speeds and energy efficiency. Yet, the significance of the light phases, which convey vital data for computations, is often disregarded in standard ONN designs. To rectify this problem, this study presents a new framework named OplixNet. OplixNet compresses ONN areas by encoding input data into both the intensity and phase aspects of light pulses. By tailoring ONN input and output elements to manage amplitude and phase data, and fostering cooperative learning between separate ONN configurations, OplixNet retains precision while significantly lowering ONN space demands. For instance, a 0.33% decline in FCNN performance engenders a 75.03% decrease in area, while a 2.38% drop in ResNet-32 accuracy results in a 74.88% reduction in area."
        ],
        [
            "Text rephrased:\n\nOptical neural networks (ONNs) are popular for deep learning due to their fast processing speeds and energy savings. However, the importance of the light phases, which hold crucial data for computations, is frequently overlooked in traditional ONN architectures. To address this issue, this research proposes a novel framework called OplixNet. OplixNet reduces ONN size by encoding input data into both the intensity and phase components of light pulses. By customizing ONN input and output components to handle amplitude and phase information, and promoting collaborative learning among distinct ONN configurations, OplixNet maintains accuracy while substantially decreasing ONN area requirements. For example, a 0.33% decrease in FCNN performance leads to a 75.03% reduction in area, while a 2.38% decrease in ResNet-32 accuracy results in a 74.88% reduction in area."
        ],
        [
            "In the realm of optical neural networks (ONNs), which are widely used for deep learning on account of their swift processing speeds and energy efficiency, the significance of the light phases, carrying essential data for computations, is often disregarded in standard ONN designs. To rectify this, this study puts forth a new framework, named OplixNet. OplixNet lessens ONN size by encoding input data into both the intensity and phase dimensions of light impulses. By tailoring ONN input and output components to manage amplitude and phase details, and inciting cooperative learning among separate ONN configurations, OplixNet upholds precision while significantly diminishing ONN space requirements. For instance, a 0.33% decline in FCNN performance triggers a 75.03% reduction in area, whereas a 2.38% decrease in ResNet-32 precision brings about a 74.88% reduction in area."
        ],
        [
            "In the field of optical neural networks (ONNs), known for their fast processing speeds and energy efficiency in deep learning applications, the role of light phases, carrying vital data for computations, is frequently overlooked in conventional ONN architectures. To address this, this research introduces a novel framework called OplixNet. OplixNet reduces ONN size by encoding input data into both the intensity and phase components of light pulses. By customizing ONN input and output elements to handle amplitude and phase information, and promoting collaborative learning among distinct ONN configurations, OplixNet preserves accuracy while markedly decreasing ONN space consumption. For example, a 0.33% drop in FCNN performance results in a 75.03% decrease in area, whereas a 2.38% decline in ResNet-32 precision leads to a 74.88% reduction in area."
        ],
        [
            "In the realm of optical neural networks (ONNs), recognized for their swift processing speeds and energy savings in deep learning, the significance of light phases, which convey crucial information for computations, is often disregarded in standard ONN designs. To rectify this, this study proposes a new approach named OplixNet. OplixNet shrinks ONN size by encoding input data into both the intensity and phase aspects of light pulses. By tailoring ONN input and output components to manage amplitude and phase details, and fostering cooperative learning among diverse ONN setups, OplixNet maintains accuracy while substantially cutting down ONN resource usage. For instance, a 0.33% decrease in FCNN performance causes a 75.03% reduction in area, while a 2.38% slip in ResNet-32 precision results in a 74.88% decrease in area."
        ],
        [
            "In the field of optical neural networks (ONNs), known for their fast processing speeds and energy efficiency in deep learning, the role of light phases in transmitting vital data for computations is frequently overlooked in conventional ONN architectures. To address this issue, this research introduces a novel method referred to as OplixNet. OplixNet reduces ONN size by encoding input data into both the intensity and phase dimensions of light pulses. By customizing ONN input and output components to handle amplitude and phase particulars, and encouraging collaborative learning between various ONN configurations, OplixNet preserves accuracy while markedly decreasing ONN resource consumption. For illustration, a 0.33% decline in FCNN performance leads to a 75.03% reduction in area, while a 2.38% drop in ResNet-32 precision results in a 74.88% reduction in area."
        ],
        [
            "In the realm of optical neural networks (ONNs), which are recognized for their swift processing speeds and energy efficiency in deep learning, the significance of light phases in transmitting crucial data for computations is often disregarded in traditional ONN designs. To tackle this problem, this study presents a new approach called OplixNet. OplixNet minimizes ONN size by encoding input data into both the intensity and phase dimensions of light pulses. By tailoring ONN input and output elements to manage amplitude and phase specifics, and promoting cooperative learning among diverse ONN configurations, OplixNet maintains precision while significantly decreasing ONN resource usage. For instance, a 0.33% decrease in FCNN performance brings about a 75.03% reduction in area, and a 2.38% dip in ResNet-32 accuracy results in a 74.88% reduction in area."
        ],
        [
            "In the domain of optical neural networks (ONNs), known for their fast processing speeds and energy efficiency in deep learning, the role of light phases in transmitting vital data for computations is frequently overlooked in conventional ONN architectures. To address this issue, this research proposes a novel method referred to as OplixNet. OplixNet reduces ONN size by encoding input data into both the intensity and phase components of light impulses. By customizing ONN input and output components to handle amplitude and phase details, and fostering collaborative learning among various ONN configurations, OplixNet preserves accuracy while markedly lowering ONN resource consumption. For example, a 0.33% decline in FCNN performance leads to a 75.03% decrease in area, and a 2.38% drop in ResNet-32 accuracy results in a 74.88% reduction in area."
        ],
        [
            "In the field of optical neural networks (ONNs), which are recognized for their swift processing speeds and energy savings in deep learning, the significance of light phases in conveying crucial data for computations is often disregarded in standard ONN structures. To rectify this, this study introduces a new approach called OplixNet. OplixNet minimizes ONN dimensions by encoding input data into both the light intensity and phase. By tailoring ONN input and output elements to accommodate amplitude and phase information, and promoting cooperative learning among diverse ONN configurations, OplixNet maintains precision while substantially reducing ONN resource usage. An instance of this is a 0.33% drop in FCNN efficiency causing a 75.03% decrease in area, and a 2.38% decrease in ResNet-32 accuracy resulting in a 74.88% reduction in area."
        ],
        [
            "In the domain of optical neural networks (ONNs), which are renowned for their fast processing speeds and energy efficiency in deep learning, the role of light phases in transmitting essential data for computations is frequently overlooked in conventional ONN designs. To address this issue, this research proposes a novel method called OplixNet. OplixNet reduces ONN dimensions by encoding input data into both light intensity and phase. By customizing ONN input and output components to handle amplitude and phase details, and fostering collaborative learning among assorted ONN architectures, OplixNet preserves accuracy while markedly decreasing ONN resource consumption. A case in point is a 0.33% decrease in FCNN performance leading to a 75.03% diminution in size, and a 2.38% reduction in ResNet-32 accuracy resulting in a 74.88% reduction in area."
        ],
        [
            "In the field of optical neural networks (ONNs), known for their rapid processing speeds and energy savings in deep learning, the significance of light phases in transferring crucial data for computations is often disregarded in standard ONN architectures. To rectify this, this study presents a new approach referred to as OplixNet. OplixNet cuts down ONN dimensions by encoding input data into both light intensity and phase. By tailoring ONN input and output elements to manage amplitude and phase information, and encouraging cooperative learning among various ONN structures, OplixNet maintains precision while significantly lowering ONN resource utilization. An illustration is a 0.33% drop in FCNN performance leading to a 75.03% shrinkage in size, and a 2.38% decline in ResNet-32 accuracy causing a 74.88% reduction in area."
        ],
        [
            "In the realm of optical neural networks (ONNs), which are renowned for their swift processing speeds and energy efficiency in deep learning, the role of light phases in conveying vital data for computations is frequently overlooked in conventional ONN designs. To address this issue, this research introduces a novel method called OplixNet. OplixNet reduces ONN dimensions by encoding input data into both light intensity and phase. By customizing ONN input and output components to handle amplitude and phase information, and promoting collaborative learning among diverse ONN configurations, OplixNet preserves accuracy while markedly decreasing ONN resource consumption. An example demonstrates a 0.33% decrease in FCNN performance resulting in a 75.03% decrease in size, and a 2.38% decrease in ResNet-32 accuracy leading to a 74.88% reduction in area."
        ],
        [
            "In the field of optical neural networks (ONNs), recognized for their rapid processing speeds and energy savings in deep learning, the significance of light phases in transmitting essential data for calculations is often disregarded in standard ONN architectures. To rectify this, this study proposes a new approach named OplixNet. OplixNet minimizes ONN dimensions by encoding input information into both light intensity and phase. By tailoring ONN input and output elements to manage amplitude and phase details, and fostering cooperative learning among varied ONN configurations, OplixNet maintains precision while substantially lowering ONN resource usage. An illustration reveals a 0.33% decline in FCNN performance accompanied by a 75.03% reduction in size, and a 2.38% reduction in ResNet-32 accuracy resulting in a 74.88% cut in area."
        ],
        [
            "In the realm of optical neural networks (ONNs), which are renowned for their swift processing speeds and energy efficiency in deep learning, the role of light phases in transmitting vital data for computations is frequently overlooked in conventional ONN designs. To address this oversight, this research introduces a novel method called OplixNet. OplixNet reduces ONN dimensions by encoding input data into both light intensity and phase. By customizing ONN input and output components to handle amplitude and phase specifics, and promoting collaborative learning among diverse ONN configurations, OplixNet preserves accuracy while markedly decreasing ONN resource requirements. A diagram demonstrates a 0.33% decrease in the performance of a fully connected neural network (FCNN) coupled with a 75.03% size reduction, and a 2.38% reduction in ResNet-32 accuracy leading to a 74.88% reduction in area."
        ],
        [
            "In the field of optical neural networks (ONNs), known for their rapid processing speeds and energy savings in deep learning, the significance of light phases in transmitting essential data for computations is often disregarded in standard ONN architectures. To rectify this neglect, this study proposes a new approach called OplixNet. OplixNet diminishes ONN dimensions by encoding input data into both light intensity and phase. By tailoring ONN input and output elements to accommodate amplitude and phase characteristics, and fostering cooperative learning between various ONN configurations, OplixNet maintains precision while dramatically cutting ONN resource consumption. An illustration depicts a 0.33% drop in the performance of a fully connected neural network (FCNN) combined with a 75.03% reduction in size, and a 2.38% decrease in ResNet-32 accuracy resulting in a 74.88% reduction in area."
        ],
        [
            "In the domain of optical neural networks (ONNs), recognized for their fast processing speeds and energy efficiency in deep learning, the role of light phases in conveying crucial data for computations is frequently overlooked in conventional ONN designs. To address this oversight, this research introduces a novel method referred to as OplixNet. OplixNet reduces ONN complexity by encoding input data into both light intensity and phase. By customizing ONN input and output components to address amplitude and phase attributes, and encouraging collaborative learning among different ONN configurations, OplixNet preserves accuracy while markedly decreasing ONN resource usage. An image displays a decrease of 0.33% in the performance of a fully connected neural network (FCNN) coupled with a reduction of 75.03% in size, and a decline of 2.38% in ResNet-32 accuracy leading to a 74.88% reduction in area."
        ],
        [
            "In the realm of optical neural networks (ONNs), where they are distinguished for their swift processing speeds and energy savings in deep learning, the significance of light phases in transmitting vital data for computations is often disregarded in standard ONN architectures. This study introduces a new approach, termed OplixNet, to rectify this neglect. OplixNet streamlines ONN intricacy by encoding input data into both light intensity and phase. By tailoring ONN input and output components to accommodate amplitude and phase characteristics, and fostering cooperative learning among various ONN configurations, OplixNet maintains precision while substantially cutting down ONN resource consumption. A depiction illustrates a 0.33% performance dip in a fully connected neural network (FCNN) in conjunction with a 75.03% size reduction, and a 2.38% decline in ResNet-32 accuracy translating to a 74.88% reduction in area."
        ],
        [
            "In the field of optical neural networks (ONNs), which are recognized for their fast processing speeds and energy efficiency in deep learning, the role of light phases in transmitting crucial data for computations is frequently overlooked in conventional ONN designs. This research proposes a novel method, referred to as OplixNet, to address this oversight. OplixNet simplifies ONN complexity by encoding input data into both light intensity and phase. By customizing ONN input and output components to accommodate amplitude and phase attributes, and encouraging collaborative learning among multiple ONN configurations, OplixNet preserves accuracy while significantly reducing ONN resource usage. The results show a decrease of 0.33% in the performance of a fully connected neural network (FCNN), coupled with a 75.03% reduction in size. Additionally, there is a decrease of 2.38% in the accuracy of ResNet-32, which translates to a 74.88% reduction in area."
        ],
        [
            "In the realm of optical neural networks (ONNs), known for their swift processing speeds and energy savings in deep learning, the contribution of light phases in conveying vital data for computations is often disregarded in standard ONN architectures. This study introduces a new approach, named OplixNet, to rectify this neglect. OplixNet streamlines ONN intricacy by encoding input data into both light intensity and phase. By tailoring ONN input and output elements to accommodate amplitude and phase characteristics, and fostering cooperative learning among several ONN configurations, OplixNet maintains precision while markedly diminishing ONN resource consumption. The findings reveal a performance drop of 0.33% for a fully connected neural network (FCNN), accompanied by a 75.03% size reduction. Moreover, there is a 2.38% accuracy decrease in ResNet-32, equivalent to a 74.88% area reduction."
        ],
        [
            "In the field of optical neural networks (ONNs), which are recognized for their rapid processing speeds and energy efficiency in deep learning, the role of light phases in transmitting crucial data for computations is generally overlooked in conventional ONN designs. This research proposes a novel method, referred to as OplixNet, to address this oversight. OplixNet simplifies ONN complexity by encoding input data into both light intensity and phase. By customizing ONN input and output components to align with amplitude and phase features, and encouraging collaborative learning among several ONN configurations, OplixNet preserves accuracy while significantly reducing ONN resource usage. The results demonstrate a decline of 0.33% in performance for a fully connected neural network (FCNN), coupled with a reduction of 75.03% in size. Additionally, there is a 2.38% drop in accuracy for ResNet-32, equivalent to a 74.88% decrease in area."
        ],
        [
            "In the realm of optical neural networks (ONNs), which are celebrated for their quick processing speeds and energy savings in deep learning, the significance of light phases in transmitting vital data for computations is often disregarded in traditional ONN architectures. This study presents an innovative approach, named OplixNet, to rectify this neglect. OplixNet streamlines ONN intricacy by encoding input information into both light intensity and phase. By tailoring ONN input and output elements to correspond with amplitude and phase characteristics, and fostering cooperative learning among assorted ONN arrangements, OplixNet maintains precision while substantially cutting down ONN resource consumption. The findings reveal a drop of 0.33% in performance for a fully connected neural network (FCNN), combined with a reduction of 75.03% in dimensions. Furthermore, there is a 2.38% decrease in accuracy for ResNet-32, which correlates to a 74.88% shrinkage in area."
        ],
        [
            "In the field of optical neural networks (ONNs), known for their fast processing speeds and energy efficiency in deep learning, the role of light phases in transmitting crucial data for computations is frequently overlooked in conventional ONN designs. This research introduces an advanced solution, called OplixNet, to address this oversight. OplixNet simplifies ONN complexity by encoding input data into both light intensity and phase. By aligning ONN input and output components with amplitude and phase traits, and encouraging collaborative learning among different ONN configurations, OplixNet preserves accuracy while significantly reducing ONN resource usage. The results show a 0.33% decline in FCNN performance, along with a 75.03% decrease in size. Moreover, there is a 2.38% dip in ResNet-32 accuracy, equivalent to a 74.88% reduction in area."
        ],
        [
            "In the realm of optical neural networks (ONNs), where their quick processing speeds and energy savings in deep learning are well-known, the significance of light phases in conveying essential data for computations is often disregarded in traditional ONN architectures. This study proposes an innovative response, named OplixNet, to rectify this neglected aspect. OplixNet streamlines ONN intricacy by encoding input data into both light intensity and phase. By synchronizing ONN input and output components with amplitude and phase characteristics, and promoting cooperative learning among distinct ONN configurations, OplixNet maintains precision while markedly diminishing ONN resource consumption. The findings indicate a 0.33% decrease in FCNN performance, coupled with a 75.03% reduction in dimensions. Furthermore, there is a 2.38% decrease in ResNet-32 accuracy, which corresponds to a 74.88% shrinkage in area."
        ],
        [
            "In the field of optical neural networks (ONNs), which are recognized for their fast processing speeds and energy efficiency in deep learning, the role of light phases in transmitting vital data for calculations is frequently overlooked in conventional ONN designs. This research introduces a novel solution, referred to as OplixNet, to address this overlooked aspect. OplixNet simplifies ONN complexity by encoding input data into both light intensity and phase. By aligning ONN input and output elements with amplitude and phase properties, and fostering collaborative learning among various ONN configurations, OplixNet preserves accuracy while significantly cutting down ONN resource usage. The results suggest a 0.33% decrease in performance for fully connected neural networks (FCNNs), along with a 75.03% reduction in size. Additionally, there is a 2.38% reduction in accuracy for ResNet-32, which corresponds to a 74.88% size reduction."
        ],
        [
            "In the realm of optical neural networks (ONNs), which are acclaimed for their swift processing speeds and energy savings in deep learning, the significance of light phases in conveying essential data for computations is often disregarded in standard ONN architectures. This study proposes an innovative approach, named OplixNet, to tackle this neglected element. OplixNet streamlines ONN intricacy by encoding input data into both light intensity and phase. By synchronizing ONN input and output components with amplitude and phase characteristics, and promoting cooperative learning among assorted ONN configurations, OplixNet maintains precision while drastically reducing ONN resource requirements. The findings indicate a 0.33% decline in FCNN performance and a 75.03% decrease in size. Moreover, there is a 2.38% reduction in ResNet-32 accuracy, which equates to a 74.88% reduction in size."
        ],
        [
            "In the field of optical neural networks (ONNs), known for their fast processing speeds and energy efficiency in deep learning, the role of light phases in transmitting vital data for computations is frequently overlooked in traditional ONN designs. This research introduces a novel method, referred to as OplixNet, to address this overlooked aspect. OplixNet simplifies ONN complexity by encoding input data into both light intensity and phase. By aligning ONN input and output elements with amplitude and phase attributes, and fostering collaborative learning among diverse ONN configurations, OplixNet preserves accuracy while considerably lowering ONN resource utilization. The results suggest a 0.33% drop in FCNN performance and a 75.03% decrease in size. Additionally, there is a 2.38% decrease in ResNet-32 accuracy, corresponding to a 74.88% reduction in size."
        ],
        [
            "In the realm of optical neural networks (ONNs), recognized for their swift processing speeds and energy economy in deep learning, the significance of light phases in conveying crucial data for computations is often disregarded in conventional ONN architectures. This study proposes a groundbreaking approach, named OplixNet, to tackle this neglected element. OplixNet streamlines ONN intricacy by encoding input information into both light intensity and phase. By synchronizing ONN input and output components with amplitude and phase properties, and promoting cooperative learning among assorted ONN configurations, OplixNet maintains precision while significantly reducing ONN resource consumption. The findings indicate a 0.33% decline in FCNN performance and a 75.03% reduction in size. Furthermore, there is a 2.38% decrease in ResNet-32 accuracy, which corresponds to a 74.88% size reduction."
        ],
        [
            "In the field of optical neural networks (ONNs), where ONNs are acknowledged for their fast processing speeds and energy efficiency in deep learning, the role of light phases in transmitting vital data for computations is frequently overlooked in standard ONN designs. This research introduces a pioneering method, referred to as OplixNet, to address this disregarded aspect. OplixNet simplifies ONN complexity by encoding input data into both light intensity and phase. By aligning ONN input and output elements with amplitude and phase characteristics, and fostering collaborative learning among various ONN structures, OplixNet preserves accuracy while markedly decreasing ONN resource utilization. The results reveal a 0.33% drop in FCNN performance and a 75.03% reduction in size. Additionally, there is a 2.38% decrease in ResNet-32 accuracy, which corresponds to a 74.88% size reduction."
        ],
        [
            "In the realm of optical neural networks (ONNs), which are recognized for their swift processing speeds and energy savings in deep learning, the significance of light phases in transferring crucial data for computations is often neglected in conventional ONN architectures. This study presents an innovative approach, named OplixNet, to tackle this overlooked factor. OplixNet streamlines ONN intricacy by encoding input data into both light intensity and phase. By synchronizing ONN input and output components with amplitude and phase attributes, and encouraging cooperative learning among various ONN architectures, OplixNet maintains precision while significantly reducing ONN resource consumption. The findings indicate a 0.33% decline in FCNN performance and a 75.03% decrease in size. Furthermore, there is a 2.38% decrease in ResNet-32 accuracy, translating to a 74.88% size reduction."
        ],
        [
            "In the field of optical neural networks (ONNs), known for their fast processing speeds and energy efficiency in deep learning, the role of light phases in transmitting vital data for computations is frequently disregarded in standard ONN designs. This research introduces an advanced method, referred to as OplixNet, to address this underestimated aspect. OplixNet simplifies ONN complexity by encoding input data into both light intensity and phase. By aligning ONN input and output elements with amplitude and phase characteristics, and fostering collaborative learning among multiple ONN configurations, OplixNet preserves accuracy while markedly reducing ONN resource utilization. The results suggest a 0.33% decrease in the performance of a fully connected neural network (FCNN) and a 75.03% reduction in size. Additionally, there is a 2.38% decrease in the accuracy of ResNet-32, which corresponds to a 74.88% size reduction."
        ],
        [
            "In the realm of optical neural networks (ONNs), recognized for their swift processing speeds and energy conservation in deep learning, the significance of light phases in transmitting essential data for computations is often overlooked in conventional ONN architectures. This study proposes a sophisticated approach, named OplixNet, to tackle this overlooked aspect. OplixNet streamlines ONN intricacy by encoding input data into both light intensity and phase. By synchronizing ONN input and output components with amplitude and phase features, and encouraging cooperative learning among various ONN configurations, OplixNet maintains precision while significantly decreasing ONN resource consumption. The findings indicate a 0.33% decline in the performance of a fully connected neural network (FCNN) and a 75.03% reduction in its size. Furthermore, there is a 2.38% decrease in ResNet-32's accuracy, equivalent to a 74.88% size reduction."
        ],
        [
            "In the field of optical neural networks (ONNs), which are renowned for their fast processing speeds and energy efficiency in deep learning, the role of light phases in transmitting crucial data for computations is frequently disregarded in traditional ONN designs. This research introduces a sophisticated method, called OplixNet, to address this neglected aspect. OplixNet simplifies ONN complexity by encoding input data into both light intensity and phase. By aligning ONN input and output elements with amplitude and phase characteristics, and promoting collaborative learning among multiple ONN configurations, OplixNet preserves accuracy while substantially reducing ONN resource utilization. The results reveal a 0.33% drop in the performance of a fully connected neural network (FCNN) and a 75.03% decrease in its size. Additionally, there is a 2.38% decline in ResNet-32's accuracy, which correlates to a 74.88% reduction in its size."
        ],
        [
            "In the realm of optical neural networks (ONNs), recognized for their swift processing speeds and energy savings in deep learning, the contribution of light phases in transferring vital data for computations is often overlooked in conventional ONN architectures. This study proposes an innovative approach, named OplixNet, to tackle this disregarded element. OplixNet streamlines ONN intricacy by encoding input data into both light intensity and phase. By synchronizing ONN input and output components with amplitude and phase traits, and fostering cooperative learning among various ONN configurations, OplixNet upholds precision while significantly decreasing ONN resource consumption. The findings indicate a 0.33% reduction in the performance of a fully connected neural network (FCNN) and a 75.03% shrinkage in its dimensions. Furthermore, there is a 2.38% drop in ResNet-32's accuracy, which corresponds to a 74.88% reduction in its size."
        ],
        [
            "In the field of optical neural networks (ONNs), known for their fast processing speeds and energy efficiency in deep learning, the role of light phases in transmitting crucial data for computations is frequently disregarded in traditional ONN designs. This research introduces a novel method, called OplixNet, to address this neglected aspect. OplixNet simplifies ONN complexity by encoding input data into both light intensity and phase. By aligning ONN input and output elements with amplitude and phase features, and promoting collaborative learning among diverse ONN configurations, OplixNet maintains accuracy while substantially reducing ONN resource utilization. The results demonstrate a 0.33% decrease in the performance of a fully connected neural network (FCNN) and a 75.03% reduction in its size. Additionally, there is a 2.38% decline in ResNet-32's accuracy, which translates to a 74.88% reduction in its dimensions."
        ],
        [
            "In the realm of optical neural networks (ONNs), which are recognized for their swift processing speeds and energy savings in deep learning, the significance of light phases in transmitting vital data for computations is often overlooked in conventional ONN architectures. This study proposes a new approach, named OplixNet, to tackle this disregarded element. OplixNet streamlines ONN complexity by encoding input data into both light intensity and phase. By synchronizing ONN input and output components with amplitude and phase characteristics, and fostering cooperative learning among various ONN configurations, OplixNet preserves accuracy while markedly diminishing ONN resource consumption. The findings reveal a 0.33% drop in the performance of a fully connected neural network (FCNN) and a 75.03% decrease in its dimensions. Moreover, there is a 2.38% decrease in ResNet-32's accuracy, corresponding to a 74.88% reduction in its spatial requirements."
        ],
        [
            "In the field of optical neural networks (ONNs), known for their fast processing speeds and energy efficiency in deep learning, the role of light phases in transmitting crucial data for computations is frequently disregarded in traditional ONN designs. This research introduces a novel method, referred to as OplixNet, to address this neglected aspect. OplixNet simplifies ONN complexity by encoding input data into both light intensity and phase. By aligning ONN input and output elements with amplitude and phase characteristics, and encouraging collaborative learning among various ONN configurations, OplixNet maintains precision while significantly reducing ONN resource usage. The results indicate a 0.33% decline in the performance of a fully connected neural network (FCNN) and a 75.03% reduction in its size. Additionally, there is a 2.38% decrease in ResNet-32's accuracy, equating to a 74.88% reduction in its spatial requirements."
        ],
        [
            "In the realm of optical neural networks (ONNs), recognized for their swift processing speeds and energy savings in deep learning, the significance of light phases in conveying essential data for computations is often overlooked in conventional ONN architectures. This study proposes a new technique, named OplixNet, to tackle this overlooked dimension. OplixNet streamlines ONN intricacy by encoding input data into both light intensity and phase. By synchronizing ONN input and output components with amplitude and phase properties, and stimulating cooperative learning among distinct ONN layouts, OplixNet preserves accuracy while markedly diminishing ONN resource utilization. The findings reveal a 0.33% decrease in a fully connected neural network's (FCNN) efficiency and a 75.03% decrease in its dimensionality. Furthermore, there is a 2.38% decrease in ResNet-32's performance, corresponding to a 74.88% reduction in its spatial demands."
        ],
        [
            "In the domain of optical neural networks (ONNs), known for their fast processing speeds and energy efficiency in deep learning, the role of light phases in transmitting crucial data for computations is frequently disregarded in traditional ONN designs. This research introduces a novel method, referred to as OplixNet, to address this neglected aspect. OplixNet simplifies ONN complexity by encoding input data into both light intensity and phase. By aligning ONN input and output elements with amplitude and phase features, and fostering collaborative learning among distinct ONN configurations, OplixNet maintains precision while substantially reducing ONN resource consumption. The results indicate a 0.33% drop in the effectiveness of a fully connected neural network (FCNN), as well as a 75.03% reduction in its size. Additionally, there is a 2.38% decline in ResNet-32's performance, equivalent to a 74.88% decrease in its spatial requirements."
        ],
        [
            "In the field of optical neural networks (ONNs), where they are renowned for their quick processing speeds and energy savings in deep learning, the significance of light phases in transmitting vital data for computations is often overlooked in standard ONN architectures. This study proposes a new approach, labeled OplixNet, to tackle this disregarded facet. OplixNet streamlines ONN complexity by encoding input data into both light intensity and phase. By synchronizing ONN input and output elements with amplitude and phase characteristics, and encouraging cooperative learning among distinct ONN configurations, OplixNet preserves accuracy while significantly diminishing ONN resource usage. The findings suggest a 0.33% decrease in the functionality of a fully connected neural network (FCNN), accompanied by a 75.03% reduction in its dimensions. Furthermore, there is a 2.38% decrease in ResNet-32's efficiency, which is equivalent to a 74.88% reduction in its space requirements."
        ],
        [
            "In the realm of optical neural networks (ONNs), known for their fast processing speeds and energy efficiency in deep learning, the role of light phases in transmitting essential data for computations is frequently disregarded in conventional ONN designs. This research introduces a novel method, named OplixNet, to address this neglected aspect. OplixNet simplifies ONN complexity by encoding input data into both light intensity and phase. By aligning ONN input and output components with amplitude and phase properties, and fostering collaborative learning among various ONN configurations, OplixNet maintains precision while substantially cutting down ONN resource consumption. The results indicate a 0.33% drop in the performance of a fully connected neural network (FCNN), coupled with a 75.03% decrease in its size. Additionally, there is a 2.38% reduction in ResNet-32's performance, which is equivalent to a 74.88% reduction in its space demands."
        ],
        [
            "In the domain of optical neural networks (ONNs), which are recognized for their rapid processing speeds and energy savings in deep learning, the contribution of light phases in conveying vital data for computations is often overlooked in standard ONN architectures. This study proposes a new approach, referred to as OplixNet, to tackle this overlooked aspect. OplixNet streamlines ONN intricacy by encoding input data into both light intensity and phase. By synchronizing ONN input and output components with amplitude and phase characteristics, and encouraging cooperative learning among diverse ONN configurations, OplixNet upholds accuracy while significantly reducing ONN resource utilization. The findings suggest a 0.33% decrease in the performance of a fully connected neural network (FCNN), accompanied by a 75.03% reduction in its dimensions. Furthermore, there is a 2.38% decline in ResNet-32's performance, which is akin to a 74.88% reduction in its storage requirements."
        ],
        [
            "In the field of optical neural networks (ONNs), known for their fast processing speeds and energy efficiency in deep learning, the role of light phases in transmitting crucial data for computations is frequently disregarded in traditional ONN designs. This research introduces a novel method, named OplixNet, to address this neglected element. OplixNet simplifies ONN complexity by encoding input data into both light intensity and phase. By aligning ONN input and output components with amplitude and phase properties, and promoting collaborative learning across various ONN configurations, OplixNet maintains precision while significantly cutting down ONN resource usage. The results indicate a 0.33% decrease in the performance of a fully connected neural network (FCNN), coupled with a 75.03% reduction in its size. Additionally, there is a 2.38% decrease in ResNet-32's performance, equivalent to a 74.88% reduction in its storage space."
        ]
    ]
}