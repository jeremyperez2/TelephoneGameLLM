{
    "adjacency_matrix": [
        [
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0,
            0.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0,
            1.0
        ],
        [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0
        ]
    ],
    "prompt_init": [
        "You will receive a text. Your task is to rephrase this text without modifying its meaning. Just output your new text, nothing else. Here is the text:"
    ],
    "prompt_update": [
        "You will receive a text. Your task is to rephrase this text without modifying its meaning. Just output your new text, nothing else. Here is the text:"
    ],
    "personality_list": [
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        ""
    ],
    "format_prompt": [
        ""
    ],
    "initial_story": "having the potential for high speed, high throughput, and low energy cost,optical neural networks (onns) have emerged as a promising candidate foraccelerating deep learning tasks. in conventional onns, light amplitudes aremodulated at the input and detected at the output. however, the light phasesare still ignored in conventional structures, although they can also carryinformation for computing. to address this issue, in this paper, we propose aframework called oplixnet to compress the areas of onns by modulating inputimage data into the amplitudes and phase parts of light signals. the input andoutput parts of the onns are redesigned to make full use of both amplitude andphase information. moreover, mutual learning across different onn structures isintroduced to maintain the accuracy. experimental results demonstrate that theproposed framework significantly reduces the areas of onns with the accuracywithin an acceptable range. for instance, 75.03% area is reduced with a 0.33%accuracy decrease on fully connected neural network (fcnn) and 74.88% area isreduced with a 2.38% accuracy decrease on resnet-32. ",
    "stories": [
        [
            "Optical neural networks (ONNs), characterized by their capacity for fast speeds, large data transfer rates, and low energy consumption, have gained significant attention as a potential solution for accelerating deep learning tasks. In traditional ONNs, the input light amplitudes are modified and the output light amplitudes are measured. However, the significance of the light phases, which can also convey computational information, is overlooked in conventional designs. In this study, we propose a new framework named Oplixnet to optimize ONNs by encoding input data into both amplitude and phase components of light signals. The input and output regions of ONNs are reengineered to maximally utilize amplitude and phase information. Additionally, we implement mutual learning between various ONN architectures to preserve accuracy. Our findings indicate that Oplixnet effectively minimizes ONN areas while maintaining acceptable accuracy levels. For example, a reduction of 75.03% in area results in a 0.33% decrease in accuracy for fully connected neural networks (FCNNs), and a reduction of 74.88% in area corresponds to a 2.38% decrease in accuracy for ResNet-32."
        ],
        [
            "Text rephrased:\n\nOptical neural networks (ONNs), known for their rapid speeds, expansive data transfer capabilities, and minimal energy consumption, have garnered substantial interest as a viable option for expediting deep learning processes. In contrast to standard ONNs, which focus solely on modifying input light amplitudes and measuring output light amplitudes, the significance of the light phases, which can transmit computational data as well, is disregarded in traditional designs. In this research, we introduce a novel approach called Oplixnet to enhance ONNs by encoding input data into both the amplitude and phase elements of light signals. The input and output zones of ONNs are redesigned to capitalize on amplitude and phase details. Furthermore, we integrate mutual learning among distinct ONN architectures to ensure precision is preserved. Our results reveal that Oplixnet successfully minimizes ONN size while maintaining satisfactory accuracy levels. For instance, a 75.03% decrease in size leads to a mere 0.33% decline in accuracy for fully connected neural networks (FCNNs), and a 74.88% reduction in size corresponds to a 2.38% dip in accuracy for ResNet-32."
        ],
        [
            "Optical neural networks (ONNs), renowned for their swift speeds, extensive data transfer capacities, and meager energy consumption, have attracted considerable attention as an efficient method for accelerating deep learning tasks. In contrast to conventional ONNs, which solely concentrate on altering input light amplitude and measuring output light amplitude, the role of light phases, which can transmit computational data as well, is overlooked in conventional designs. In this study, we propose a new strategy named Oplixnet to improve ONNs by encoding input data into both the amplitude and phase components of light pulses. The input and output regions of ONNs are reengineered to leverage amplitude and phase subtleties. Additionally, we incorporate collaborative learning among distinct ONN architectures to safeguard accuracy. Our findings demonstrate that Oplixnet effectively diminishes ONN scale while preserving satisfactory precision. For example, a 75.03% reduction in scale results in a minimal 0.33% loss in accuracy for fully connected neural networks (FCNNs), and a 74.88% shrinkage in scale corresponds to a moderate 2.38% reduction in accuracy for ResNet-32."
        ],
        [
            "Swift and energy-efficient optical neural networks (ONNs), recognized for their rapid processing speeds, extensive data transfer capabilities, and minimal energy consumption, have garnered significant interest as an efficient solution for expediting deep learning tasks. Unlike traditional ONNs that solely focus on adjusting input light amplitude and detecting output light amplitude, the role of light phases, which can also transmit computational information, is often disregarded in conventional designs. In this research, we suggest a novel approach called Oplixnet to enhance ONNs by encoding input data into both the amplitude and phase aspects of light pulses. The design of ONNs is revamped to capitalize on amplitude and phase nuances in input and output regions. Furthermore, we integrate collaborative learning among distinct ONN architectures to maintain accuracy. Our results indicate that Oplixnet successfully reduces ONN scale without compromising acceptable precision. For instance, a 75.03% reduction in scale leads to a negligible 0.33% drop in accuracy for fully connected neural networks (FCNNs), and a 74.88% reduction in scale corresponds to a moderate 2.38% decrease in accuracy for ResNet-32."
        ],
        [
            "Optical neural networks (ONNs) utilizing Swift technology and known for their rapid processing, extensive data transfer capabilities, and minimal energy consumption, have generated considerable attention due to their efficiency in accelerating deep learning tasks. However, most ONN designs primarily focus on manipulating input light amplitude and detecting output light amplitude, overlooking the potential role of light phases in transmitting computational information. In this study, we propose an innovative method named Oplixnet to boost ONNs by encoding input data into both the amplitude and phase elements of light pulses. The ONN architecture is reconceptualized to leverage amplitude and phase differences in input and output zones. Additionally, we incorporate collaborative learning among various ONN architectures to preserve accuracy. Our findings suggest that Oplixnet effectively diminishes ONN size without sacrificing satisfactory precision. For example, a 75.03% reduction in size causes a negligible 0.33% decline in accuracy for fully connected neural networks (FCNNs), while a 74.88% reduction in size results in a moderate 2.38% decrease in accuracy for ResNet-32."
        ],
        [
            "Text rephrased:\n\nOptical neural networks (ONNs) powered by Swift technology, renowned for their quick processing, vast data transfer abilities, and low energy consumption, have garnered significant interest due to their ability to expedite deep learning tasks. Nevertheless, most ONN designs predominantly concentrate on managing the amplitude of input light and identifying the amplitude of output light, neglecting the possibility of light phases in conveying computational data. In this research, we present a novel approach called Oplixnet to enhance ONNs by encoding input data into both the amplitude and phase aspects of light pulses. The ONN architecture is refashioned to capitalize on amplitude and phase disparities in input and output regions. Moreover, we integrate cooperative learning among distinct ONN architectures to maintain accuracy. Our results indicate that Oplixnet effectively reduces ONN dimensions without compromising satisfactory accuracy. For instance, a reduction of 75.03% in size leads to an insignificant drop of 0.33% in accuracy for fully connected neural networks (FCNNs), while a 74.88% reduction in size causes a moderate decrease of 2.38% in accuracy for ResNet-32."
        ],
        [
            "Text rephrased:\n\nOptical neural networks (ONNs) utilizing Swift technology, recognized for their fast processing, extensive data transfer capabilities, and minimal energy consumption, have generated considerable attention due to their potential for accelerating deep learning processes. However, the majority of ONN designs primarily focus on handling the amplitude of input light and determining the amplitude of output light, disregarding the role of light phases in transmitting computational information. In this study, we propose a new method called Oplixnet to improve ONNs by encoding input data into both the amplitude and phase dimensions of light pulses. The ONN structure is redesigned to leverage amplitude and phase differences in input and output areas. Additionally, we introduce collaborative learning among distinct ONN architectures to preserve precision. Our findings suggest that Oplixnet efficiently decreases ONN dimensions without sacrificing acceptable accuracy. For illustration, a decrease of 75.03% in dimensions results in a negligible decline of 0.33% in accuracy for fully connected neural networks (FCNNs), while a reduction of 74.88% in dimensions causes a moderate reduction of 2.38% in accuracy for ResNet-32."
        ],
        [
            "Optical neural networks (ONNs) based on Swift technology, celebrated for their rapid processing, expansive data transfer abilities, and meager energy consumption, have garnered significant interest due to their capability to expedite deep learning operations. Nevertheless, most ONN designs predominantly concentrate on managing the amplitude of input light and figuring out the amplitude of output light, overlooked is the significance of light phases in conveying computational data. In this research, we suggest an innovative strategy named Oplixnet to augment ONNs by encoding input data in the amplitude and phase dimensions of light impulses. The ONN framework is refashioned to capitalize on amplitude and phase disparities in input and output zones. Furthermore, we incorporate cooperative learning among disparate ONN architectures to maintain precision. Our results indicate that Oplixnet effectively shrinks ONN dimensions without compromising satisfactory accuracy. For instance, a reduction of 75.03% in dimensions yields a nearly imperceptible decrease of 0.33% in accuracy for fully connected neural networks (FCNNs), whereas a diminution of 74.88% in dimensions elicits a moderate reduction of 2.38% in accuracy for ResNet-32."
        ],
        [
            "Text rephrased: Optical neural networks (ONNs), powered by Swift technology, are renowned for their swift processing, extensive data transfer capabilities, and minimal energy consumption. They have attracted considerable attention due to their potential to accelerate deep learning processes. However, most ONN designs primarily focus on manipulating the intensity of input and output light, while the importance of light phases as carriers of computational information is often disregarded. In this study, we propose a novel approach named Oplixnet to enhance ONNs by encoding input data in both the amplitude and phase dimensions of light pulses. The ONN architecture is adapted to leverage amplitude and phase differences in input and output regions. Additionally, we introduce collaborative learning among distinct ONN architectures to preserve precision. Our findings demonstrate that Oplixnet successfully compresses ONN dimensions without sacrificing satisfactory performance. For example, a 75.03% reduction in dimensions results in a negligible decline of 0.33% in accuracy for fully connected neural networks (FCNNs), whereas a 74.88% reduction in dimensions brings about a moderate decrease of 2.38% in accuracy for ResNet-32."
        ],
        [
            "Text rephrased: Optical neural networks (ONNs), utilizing Swift technology, are recognized for their rapid processing, extensive data transfer abilities, and low energy consumption. ONNs have gained significant interest due to their capacity to expedite deep learning procedures. Yet, most ONN designs predominantly concentrate on altering the intensity of incoming and outgoing light, while the significance of light phases as vectors of computational information is frequently overlooked. In this research, we present a new strategy called Oplixnet to augment ONNs by encoding input data in both the amplitude and phase aspects of light impulses. The ONN infrastructure is modified to utilize amplitude and phase discrepancies within input and output areas. Furthermore, we implement cooperative learning among separate ONN architectures to maintain precision. Our results indicate that Oplixnet effectively compresses ONN dimensions without compromising satisfactory results. For instance, a reduction of 75.03% in dimensions results in an insignificant decrease of 0.33% in accuracy for fully connected neural networks (FCNNs), whereas a reduction of 74.88% in dimensions leads to a moderate decrease of 2.38% in accuracy for ResNet-32."
        ],
        [
            "Optical neural networks (ONNs), leveraging Swift technology, are renowned for their quick processing speeds, extensive data transfer capabilities, and minimal energy consumption. The appeal of ONNs lies in their potential to accelerate deep learning processes. However, the majority of ONN designs focus on manipulating the intensity of incoming and outgoing light, often neglecting the importance of light phases as carriers of computational information. In this study, we propose a novel approach named Oplixnet to expand ONNs by encoding input data in the amplitude and phase dimensions of light pulses. The ONN architecture is adapted to capitalize on amplitude and phase differences within input and output regions. Additionally, we incorporate cooperative learning among distinct ONN architectures to preserve precision. Our findings suggest that Oplixnet successfully condenses ONN dimensions without sacrificing satisfactory outcomes. For instance, a 75.03% reduction in dimensions causes a negligible 0.33% decrease in accuracy for fully connected neural networks (FCNNs), whereas a 74.88% reduction in dimensions results in a moderate 2.38% decrease in accuracy for ResNet-32."
        ],
        [
            "Text rephrased:\n\nOptical neural networks (ONNs), which utilize Swift technology, are recognized for their fast processing times, vast data transfer capacities, and low power consumption. ONNs' allure comes from their ability to expedite deep learning computations. However, most ONN designs primarily concentrate on managing the intensity of incoming and outgoing light, frequently disregarding the significance of light phases as conveyors of computational information. In this research, we propose a new method called Oplixnet to enhance ONNs by encoding input data in the amplitude and phase dimensions of light impulses. The ONN design is modified to exploit amplitude and phase disparities within input and output areas. Furthermore, we integrate cooperative learning among separate ONN architectures to maintain accuracy. Our results indicate that Oplixnet effectively compresses ONN dimensions without compromising satisfactory results. For example, a 75.03% reduction in dimensions results in a barely perceptible 0.33% drop in accuracy for fully connected neural networks (FCNNs), while a 74.88% reduction in dimensions brings about a moderate 2.38% decrease in accuracy for ResNet-32."
        ],
        [
            "Optical neural networks (ONNs), leveraging Swift technology, are celebrated for their swift processing speeds, extensive data transfer capabilities, and minimal power consumption. The appeal of ONNs lies in their capacity to accelerate deep learning calculations. However, the majority of existing ONN designs predominantly focus on regulating the intensity of incoming and outgoing light, often neglecting the importance of light phases as carriers of computational information. In this study, we put forth a novel approach called Oplixnet to augment ONNs by encoding input data in the amplitude and phase dimensions of light pulses. The ONN architecture is adapted to exploit amplitude and phase differences within input and output regions. Additionally, we incorporate collaborative learning among distinct ONN architectures to preserve accuracy. Our findings suggest that Oplixnet proficiently condenses ONN dimensions without sacrificing satisfactory outcomes. For instance, a reduction of 75.03% in dimensions leads to a negligible 0.33% decline in accuracy for fully connected neural networks (FCNNs), while a reduction of 74.88% in dimensions results in a moderate 2.38% decrease in accuracy for ResNet-32."
        ],
        [
            "Optical neural networks (ONNs), utilizing Swift technology, are renowned for their quick processing speeds, extensive data transfer capabilities, and low power consumption. ONNs are highly regarded due to their ability to expedite deep learning computations. However, most current ONN designs mainly concentrate on controlling the intensity of incoming and outgoing light, frequently disregarding the significance of light phases as conveyors of computational information. In this research, we propose a new method named Oplixnet to enhance ONNs by encoding input data in the amplitude and phase dimensions of light pulses. The ONN design is adjusted to utilize amplitude and phase variations within input and output regions. Furthermore, we integrate collaborative learning among different ONN architectures to maintain accuracy. Our results indicate that Oplixnet effectively compresses ONN dimensions without compromising satisfactory results. For example, a 75.03% reduction in dimensions results in a minimal 0.33% drop in accuracy for fully connected neural networks (FCNNs), while a 74.88% reduction in dimensions causes a moderate 2.38% decrease in accuracy for ResNet-32."
        ],
        [
            "Using Swift technology, optical neural networks (ONNs) are known for their fast processing speeds, extensive data transfer capabilities, and low power consumption. The value of ONNs lies in their capacity to accelerate deep learning computations. However, the majority of existing ONN designs primarily focus on managing the intensity of incoming and outgoing light, often neglecting the importance of light phases as carriers of computational information. In this study, we suggest a novel approach called Oplixnet to upgrade ONNs by encoding input data in the amplitude and phase dimensions of light pulses. The ONN architecture is modified to exploit amplitude and phase fluctuations within input and output zones. Additionally, we implement cooperative learning among distinct ONN architectures to preserve precision. Our findings suggest that Oplixnet successfully compresses ONN dimensions without sacrificing satisfactory outcomes. For instance, a reduction of 75.03% in dimensions leads to a minimal decrease of 0.33% in accuracy for fully connected neural networks (FCNNs), while a reduction of 74.88% in dimensions results in a moderate decline of 2.38% in accuracy for ResNet-32."
        ],
        [
            "With Swift technology, optical neural networks (ONNs) are renowned for their rapid processing speeds, extensive data transfer capabilities, and minimal power consumption. The significance of ONNs stems from their ability to expedite deep learning computations. However, most current ONN designs predominantly attend to managing the intensity of incoming and outgoing light, frequently disregarding the role of light phases as conveyors of computational information. In this research, we propose a novel strategy named Oplixnet to enhance ONNs by encoding input data in the amplitude and phase dimensions of light impulses. The ONN architecture is adapted to utilize amplitude and phase variations within input and output regions. Furthermore, we incorporate cooperative learning among distinct ONN architectures to maintain precision. Our results indicate that Oplixnet effectively shrinks ONN dimensions without compromising satisfactory results. For example, a 75.03% reduction in dimensions results in a negligible 0.33% decrease in accuracy for fully connected neural networks (FCNNs), while a 74.88% reduction in dimensions causes a moderate 2.38% drop in accuracy for ResNet-32."
        ],
        [
            "Utilizing Swift technology, optical neural networks (ONNs) are recognized for their swift processing speeds, expansive data transfer capacities, and low power consumption. The value of ONNs arises from their capacity to accelerate deep learning computations. Yet, the majority of existing ONN designs primarily focus on handling the intensity of incoming and outgoing light, often neglecting the function of light phases as carriers of computational information. In our study, we suggest a new method, named Oplixnet, to boost ONNs by encoding input data in the amplitude and phase dimensions of light pulses. The ONN structure is adjusted to leverage amplitude and phase fluctuations within input and output zones. Moreover, we introduce collaborative learning among distinct ONN architectures to preserve accuracy. Our findings suggest that Oplixnet efficiently diminishes ONN dimensions without sacrificing satisfactory outcomes. For instance, a reduction of 75.03% in dimensions results in a minimal 0.33% decline in accuracy for fully connected neural networks (FCNNs), while a reduction of 74.88% in dimensions leads to a moderate 2.38% decrease in accuracy for ResNet-32."
        ],
        [
            "Employing Swift technology, optical neural networks (ONNs) are renowned for their rapid processing speeds, extensive data transfer capabilities, and minimal power consumption. The significance of ONNs lies in their ability to expedite deep learning computations. However, most existing ONN architectures predominantly concentrate on managing the intensity of incoming and outgoing light, frequently disregarding the role of light phases as vectors of computational information. In our research, we propose a novel approach, called Oplixnet, to enhance ONNs by encoding input data in the amplitude and phase dimensions of light pulses. The ONN design is optimized to exploit amplitude and phase variations within input and output regions. Additionally, we incorporate collaborative learning among distinct ONN architectures to maintain precision. Our results indicate that Oplixnet effectively reduces ONN dimensions without compromising satisfactory results. For instance, a 75.03% reduction in dimensions results in a mere 0.33% drop in accuracy for fully connected neural networks (FCNNs), whereas a 74.88% reduction in dimensions results in a moderate 2.38% decrease in accuracy for ResNet-32."
        ],
        [
            "Using Swift technology, optical neural networks (ONNs) are recognized for their quick processing speeds, vast data transfer capacities, and low power consumption. The value of ONNs comes from their capacity to accelerate deep learning calculations. However, most current ONN architectures primarily focus on handling the intensity of incoming and outgoing light, often neglecting the role of light phases as carriers of computational information. In our study, we present a new method, named Oplixnet, to improve ONNs by encoding input data in the amplitude and phase dimensions of light pulses. The ONN structure is tailored to utilize amplitude and phase fluctuations within input and output zones. Furthermore, we integrate collaborative learning among various ONN architectures to preserve precision. Our findings suggest that Oplixnet effectively decreases ONN dimensions without sacrificing satisfactory outcomes. For example, a 75.03% reduction in dimensions results in a minor 0.33% decrease in accuracy for fully connected neural networks (FCNNs), whereas a 74.88% reduction in dimensions results in a moderate 2.38% decrease in accuracy for ResNet-32."
        ],
        [
            "With Swift technology, optical neural networks (ONNs) are renowned for their fast processing speeds, large data transfer capabilities, and minimal power consumption. The advantage of ONNs lies in their ability to expedite deep learning computations. Nevertheless, existing ONN architectures predominantly concentrate on managing the intensity of incoming and outgoing light, frequently disregarding the significance of light phases as vehicles of computational information. In our research, we propose a novel approach, referred to as Oplixnet, to enhance ONNs by encoding input data in the amplitude and phase dimensions of light pulses. The ONN design is engineered to exploit amplitude and phase variations within input and output regions. Additionally, we introduce collaborative learning among distinct ONN architectures to maintain accuracy. Our results indicate that Oplixnet successfully reduces ONN dimensions without compromising satisfactory results. For instance, a 75.03% reduction in dimensions leads to a minimal 0.33% drop in accuracy for fully connected neural networks (FCNNs), whereas a 74.88% reduction in dimensions results in a moderate 2.38% decline in accuracy for ResNet-32."
        ],
        [
            "Using Swift technology, optical neural networks (ONNs) are recognized for their rapid processing speeds, substantial data transfer capacities, and low power consumption. The main benefit of ONNs is their capability to accelerate deep learning calculations. However, current ONN architectures primarily focus on managing the intensity of incoming and outgoing light, often neglecting the importance of light phases as carriers of computational information. In our study, we suggest a new method, named Oplixnet, to advance ONNs by encoding input data in the amplitude and phase dimensions of light impulses. The ONN structure is developed to leverage amplitude and phase fluctuations within input and output areas. Furthermore, we incorporate collaborative learning among various ONN architectures to preserve accuracy. Our findings demonstrate that Oplixnet effectively decreases ONN dimensions without sacrificing acceptable results. For instance, a 75.03% reduction in dimensions results in a slight 0.33% decrease in accuracy for fully connected neural networks (FCNNs), while a 74.88% reduction in dimensions leads to a moderate 2.38% decline in accuracy for ResNet-32."
        ],
        [
            "With Swift technology, optical neural networks (ONNs) are renowned for their quick processing speeds, significant data transfer capabilities, and minimal power consumption. The primary advantage of ONNs lies in their ability to expedite deep learning computations. However, existing ONN architectures mainly concentrate on handling the intensity of incoming and outgoing light, frequently disregarding the significance of light phases as conveyors of computational information. In our research, we propose a novel method, called Oplixnet, to enhance ONNs by encoding input data in the amplitude and phase dimensions of light pulses. The ONN design is engineered to capitalize on amplitude and phase variations within input and output zones. In addition, we integrate cooperative learning among different ONN architectures to maintain precision. Our results indicate that Oplixnet efficiently reduces ONN dimensions without compromising satisfactory outcomes. For example, a reduction of 75.03% in dimensions results in a minor 0.33% decrease in accuracy for fully connected neural networks (FCNNs), while a reduction of 74.88% in dimensions leads to a moderate 2.38% decline in accuracy for ResNet-32."
        ],
        [
            "Using Swift technology, optical neural networks (ONNs) are recognized for their fast processing speeds, substantial data transfer capacities, and low power consumption. The major advantage of ONNs is their capability to accelerate deep learning computations. Nevertheless, current ONN architectures primarily focus on processing the intensity of incoming and outgoing light, often neglecting the importance of light phases as carriers of computational information. In our study, we suggest a new approach, referred to as Oplixnet, to improve ONNs by encoding input data in the amplitude and phase dimensions of light pulses. The ONN structure is tailored to leverage amplitude and phase fluctuations within input and output areas. Furthermore, we incorporate cooperative learning among distinct ONN architectures to preserve precision. According to our findings, Oplixnet effectively reduces ONN dimensions without sacrificing satisfactory performance. For instance, a 75.03% reduction in dimensions results in a negligible 0.33% drop in accuracy for fully connected neural networks (FCNNs), while a 74.88% reduction in dimensions results in a moderate 2.38% decrease in accuracy for ResNet-32."
        ],
        [
            "With Swift technology, optical neural networks (ONNs) are known for their swift processing speeds, considerable data transfer capabilities, and minimal power consumption. A significant advantage of ONNs lies in their potential to expedite deep learning computations. However, existing ONN architectures mostly concentrate on handling the intensity of incoming and outgoing light, frequently disregarding the significance of light phases as vehicles of computational information. In our investigation, we propose a novel strategy, called Oplixnet, to enhance ONNs by encoding input data in the amplitude and phase dimensions of light impulses. The ONN design is engineered to capitalize on amplitude and phase variations within input and output zones. In addition, we integrate cooperative learning among various ONN architectures to maintain accuracy. According to our results, Oplixnet successfully decreases ONN dimensions without compromising acceptable performance. For example, a 75.03% reduction in dimensions leads to a negligible 0.33% decline in accuracy for fully connected neural networks (FCNNs), while a 74.88% reduction in dimensions results in a moderate 2.38% decrease in accuracy for ResNet-32."
        ],
        [
            "Using Swift technology, optical neural networks (ONNs) are recognized for their quick processing speeds, generous data transfer capacities, and low power consumption. One major advantage of ONNs is their ability to accelerate deep learning computations. However, most current ONN architectures primarily focus on processing the intensity of incoming and outgoing light, often overlooking the importance of light phases as carriers of computational information. In our study, we suggest a new approach, called Oplixnet, to improve ONNs by encoding input data in the amplitude and phase dimensions of light pulses. The ONN structure is designed to leverage amplitude and phase fluctuations within input and output regions. Furthermore, we incorporate cooperative learning among various ONN designs to preserve accuracy. Based on our findings, Oplixnet effectively reduces ONN dimensions without sacrificing satisfactory performance. For instance, a 75.03% reduction in dimensions causes a minimal 0.33% drop in accuracy for fully connected neural networks (FCNNs), while a 74.88% reduction in dimensions results in a moderate 2.38% decline in accuracy for ResNet-32."
        ],
        [
            "With Swift technology, optical neural networks (ONNs) are known for their fast processing speeds, large data transfer capabilities, and minimal power consumption. One significant advantage of ONNs lies in their capability to expedite deep learning computations. Nevertheless, the majority of existing ONN architectures predominantly concentrate on handling the intensity of incoming and outgoing light, disregarding the significance of light phases as conveyors of computational data. In our research, we propose a novel strategy, named Oplixnet, to enhance ONNs by encoding input data in the amplitude and phase dimensions of light pulses. The ONN configuration is engineered to capitalize on amplitude and phase variations within input and output zones. In addition, we implement cooperative learning among various ONN designs to maintain precision. According to our results, Oplixnet successfully diminishes ONN dimensions without compromising acceptable performance. For example, a reduction of 75.03% in dimensions results in a minimal decline of 0.33% in accuracy for fully connected neural networks (FCNNs), while a decrease of 74.88% in dimensions yields a moderate drop of 2.38% in accuracy for ResNet-32."
        ],
        [
            "Using Swift technology, optical neural networks (ONNs) are renowned for their swift processing speeds, extensive data transfer capacities, and meager power consumption. An essential advantage of ONNs is their ability to accelerate deep learning calculations. However, most current ONN architectures predominantly focus on managing the intensity of incoming and outgoing light, neglecting the importance of light phases as carriers of computational data. In our study, we suggest a new approach, named Oplixnet, to boost ONNs by encoding input data in the amplitude and phase dimensions of light pulses. The ONN design is optimized to exploit amplitude and phase fluctuations within input and output regions. Furthermore, we incorporate collaborative learning among distinct ONN designs to preserve accuracy. Based on our findings, Oplixnet effectively reduces ONN dimensions without sacrificing satisfactory performance. For instance, a 75.03% reduction in dimensions causes a minimal dip of 0.33% in accuracy for fully connected neural networks (FCNNs), while a 74.88% reduction in dimensions leads to a moderate decrease of 2.38% in accuracy for ResNet-32."
        ],
        [
            "Through the utilization of Swift technology, optical neural networks (ONNs) are celebrated for their quick processing speeds, substantial data transfer capabilities, and minimal power consumption. A significant advantage of ONNs lies in their capability to expedite deep learning computations. Nevertheless, existing ONN architectures mainly concentrate on handling the intensity of incoming and outgoing light, disregarding the significance of light phases as conveyors of computational information. In our research, we propose an innovative strategy, referred to as Oplixnet, to enhance ONNs by encoding input data in the amplitude and phase dimensions of light impulses. The ONN architecture is tailored to capitalize on amplitude and phase variations within input and output areas. Moreover, we integrate collaborative learning among separate ONN designs to maintain precision. According to our results, Oplixnet effectively diminishes ONN dimensions without compromising satisfactory performance. For illustration, a reduction of 75.03% in dimensions results in a minimal loss of 0.33% in accuracy for fully connected neural networks (FCNNs), while a reduction of 74.88% in dimensions results in a moderate decline of 2.38% in accuracy for ResNet-32."
        ],
        [
            "Using Swift technology, optical neural networks (ONNs) are known for their fast processing speeds, large data transfer capacities, and low power consumption. One major advantage of ONNs is their ability to accelerate deep learning computations. However, current ONN designs primarily focus on handling the intensity of incoming and outgoing light, ignoring the importance of light phases as carriers of computational information. In our study, we suggest a novel approach, called Oplixnet, to enhance ONNs by encoding input data in the amplitude and phase dimensions of light pulses. The ONN architecture is engineered to leverage amplitude and phase differences within input and output regions. Additionally, we incorporate collaborative learning among distinct ONN models to preserve accuracy. According to our findings, Oplixnet successfully reduces ONN dimensions without sacrificing satisfactory results. For instance, a 75.03% decrease in dimensions causes a minor loss of 0.33% in accuracy for fully connected neural networks (FCNNs), whereas a 74.88% reduction in dimensions leads to a moderate decline of 2.38% in accuracy for ResNet-32."
        ],
        [
            "Through the application of Swift technology, optical neural networks (ONNs) are renowned for their swift processing speeds, significant data transfer capabilities, and minimal power consumption. A significant advantage of ONNs lies in their capacity to expedite deep learning computations. However, existing ONN architectures predominantly concentrate on managing the intensity of incoming and outgoing light, overlooking the significance of light phases as conveyors of computational information. In our research, we propose a novel method, referred to as Oplixnet, to augment ONNs by encoding input data in the amplitude and phase dimensions of light pulses. The ONN design is tailored to harness amplitude and phase discrepancies within input and output areas. Furthermore, we integrate cooperative learning among separate ONN models to maintain precision. Based on our results, Oplixnet effectively diminishes ONN dimensions without compromising satisfactory outcomes. For example, a reduction of 75.03% in dimensions brings about a minimal loss of 0.33% in accuracy for fully connected neural networks (FCNNs), whereas a 74.88% decrease in dimensions results in a moderate decline of 2.38% in accuracy for ResNet-32."
        ],
        [
            "Using Swift technology, optical neural networks (ONNs) are recognized for their quick processing speeds, substantial data transfer abilities, and meager power consumption. A major advantage of ONNs is their ability to accelerate deep learning computations. However, current ONN architectures mainly focus on handling the intensity of incoming and outgoing light, neglecting the importance of light phases as carriers of computational information. In our study, we suggest a new approach, called Oplixnet, to enhance ONNs by encoding input data in the amplitude and phase dimensions of light pulses. The ONN architecture is crafted to exploit amplitude and phase differences within input and output regions. Additionally, we incorporate cooperative learning among distinct ONN models to uphold precision. According to our findings, Oplixnet successfully reduces ONN dimensions with no substantial sacrifice in results. For instance, a 75.03% reduction in dimensions leads to a negligible loss of 0.33% in accuracy for fully connected neural networks (FCNNs), whereas a 74.88% decrease in dimensions results in a modest decline of 2.38% in accuracy for ResNet-32."
        ],
        [
            "With Swift technology, optical neural networks (ONNs) are renowned for their fast processing speeds, significant data transfer capabilities, and minimal power consumption. A primary advantage of ONNs is their capacity to expedite deep learning calculations. However, existing ONN architectures predominantly prioritize the handling of the intensity of incoming and outgoing light, disregarding the significance of light phases as vehicles of computational information. In our research, we propose a novel method, referred to as Oplixnet, to amplify ONNs by encoding input data in the amplitude and phase dimensions of light pulses. The ONN design is engineered to leverage amplitude and phase disparities within input and output areas. Moreover, we integrate cooperative learning among separate ONN models to preserve precision. Based on our study, Oplixnet effectively reduces ONN dimensions without significant loss in performance. For example, a 75.03% reduction in dimensions results in a negligible decrease of 0.33% in accuracy for fully connected neural networks (FCNNs), whereas a 74.88% reduction in dimensions results in a modest drop of 2.38% in accuracy for ResNet-32."
        ],
        [
            "Using Swift technology, optical neural networks (ONNs) are recognized for their quick processing speeds, impressive data transfer capacities, and low power consumption. One major advantage of ONNs is their ability to accelerate deep learning computations. However, most current ONN architectures focus solely on the handling of light intensity, ignoring the importance of light phases as carriers of computational information. In our investigation, we suggest a new approach, known as Oplixnet, to enhance ONNs by encoding input data in the amplitude and phase dimensions of light pulses. The ONN framework is tailored to exploit amplitude and phase differences within input and output zones. Additionally, we incorporate cooperative learning among distinct ONN models to maintain accuracy. According to our findings, Oplixnet successfully decreases ONN dimensions without a substantial decrease in performance. For instance, a 75.03% reduction in dimensions causes a negligible drop of 0.33% in accuracy for fully connected neural networks (FCNNs), while a 74.88% reduction in dimensions results in a modest decline of 2.38% in accuracy for ResNet-32."
        ],
        [
            "Employing Swift technology, optical neural networks (ONNs) are renowned for their fast processing speeds, substantial data transfer capabilities, and minimal power consumption. A significant benefit of ONNs lies in their capacity to expedite deep learning computations. Nevertheless, the vast majority of existing ONN architectures solely concentrate on managing light intensity, disregarding the significance of light phases as conveyors of computational information. In our research, we propose an innovative strategy, named Oplixnet, to boost ONNs by encoding input data in the amplitude and phase dimensions of light pulses. The ONN architecture is designed to capitalize on amplitude and phase disparities within input and output zones. Furthermore, we integrate cooperative learning among distinct ONN models to preserve precision. Our results indicate that Oplixnet effectively lessens ONN dimensions with little impact on performance. For example, a 75.03% reduction in dimensions brings about a trivial decrease of 0.33% in accuracy for fully connected neural networks (FCNNs), while a 74.88% reduction in dimensions results in a moderate decrease of 2.38% in accuracy for ResNet-32."
        ],
        [
            "Using Swift technology, optical neural networks (ONNs) are known for their rapid processing speeds, substantial data transfer capabilities, and low power consumption. One of the major advantages of ONNs is their ability to accelerate deep learning computations. However, most existing ONN architectures focus solely on managing light intensity and ignore the importance of light phases as carriers of computational information. In our study, we suggest a new approach, called Oplixnet, to enhance ONNs by encoding input data in the amplitude and phase dimensions of light pulses. The ONN design takes advantage of amplitude and phase differences within input and output areas. Additionally, we incorporate cooperative learning among various ONN models to maintain accuracy. Our findings demonstrate that Oplixnet efficiently reduces ONN dimensions with minimal effect on performance. For instance, a 75.03% reduction in dimensions results in an insignificant 0.33% decrease in accuracy for fully connected neural networks (FCNNs), while a 74.88% reduction in dimensions causes a moderate 2.38% decrease in accuracy for ResNet-32."
        ],
        [
            "With Swift technology, optical neural networks (ONNs) are renowned for their swift processing speeds, considerable data transfer capacities, and minimal power consumption. A significant advantage of ONNs is their capacity to expedite deep learning computations. Nevertheless, current ONN architectures primarily focus on managing light intensity, disregarding the significance of light phases as conveyors of computational data. In our research, we propose a novel strategy, named Oplixnet, to strengthen ONNs by encoding input information in the amplitude and phase attributes of light impulses. The ONN blueprint exploits amplitude and phase discrepancies within input and output zones. Furthermore, we integrate cooperative learning among numerous ONN models to preserve precision. Our results indicate that Oplixnet effectively decreases ONN dimensions with minimal impact on performance. For instance, a 75.03% reduction in dimensions causes a negligible 0.33% decline in accuracy for fully connected neural networks (FCNNs), whereas a 74.88% reduction in dimensions results in a moderate 2.38% decrease in accuracy for ResNet-32."
        ],
        [
            "Using Swift technology, optical neural networks (ONNs) are recognized for their quick processing speeds, substantial data transfer capabilities, and low power consumption. A major benefit of ONNs is their ability to accelerate deep learning computations. However, existing ONN architectures mainly concentrate on handling light intensity, while neglecting the importance of light phases as carriers of computational information. In our study, we suggest a new approach, named Oplixnet, to enhance ONNs by encoding input data in the amplitude and phase characteristics of light pulses. The ONN design employs amplitude and phase differences within input and output regions. Additionally, we incorporate cooperative learning among various ONN models to maintain precision. Our findings suggest that Oplixnet successfully reduces ONN dimensions with minimal performance loss. For example, a 75.03% reduction in dimensions results in a negligible 0.33% drop in accuracy for fully connected neural networks (FCNNs), while a 74.88% reduction in dimensions leads to a moderate 2.38% decrease in accuracy for ResNet-32."
        ],
        [
            "Employing Swift technology, optical neural networks (ONNs) are renowned for their fast processing speeds, significant data transfer capacities, and minimal power consumption. A primary advantage of ONNs is their capability to expedite deep learning calculations. Nevertheless, current ONN architectures primarily focus on managing light intensity, disregarding the significance of light phases as conveyors of computational data. In our research, we propose a novel strategy, named Oplixnet, to improve ONNs by encoding input data through the amplitude and phase traits of light pulses. The ONN structure utilizes amplitude and phase distinctions within input and output zones. Furthermore, we integrate cooperative learning among various ONN models to preserve accuracy. Our results indicate that Oplixnet effectively reduces ONN dimensions with insignificant loss in performance. For instance, a 75.03% reduction in dimensions results in a negligible 0.33% drop in accuracy for fully connected neural networks (FCNNs), whereas a 74.88% reduction in dimensions causes a moderate 2.38% decline in accuracy for ResNet-32."
        ],
        [
            "Using Swift technology, optical neural networks (ONNs) are recognized for their quick processing speeds, substantial data transfer capacities, and low power consumption. One major benefit of ONNs is their ability to accelerate deep learning computations. However, existing ONN architectures mainly focus on managing light intensity, neglecting the importance of light phases as carriers of computational information. In our study, we suggest a new approach, called Oplixnet, to enhance ONNs by encoding input data through the amplitude and phase properties of light pulses. The ONN design takes advantage of amplitude and phase variations in input and output regions. Additionally, we incorporate cooperative learning among multiple ONN models to maintain accuracy. Our findings demonstrate that Oplixnet efficiently decreases ONN sizes with minimal impact on performance. For example, a 75.03% reduction in sizes leads to an insignificant 0.33% decrease in accuracy for fully connected neural networks (FCNNs), whereas a 74.88% reduction in sizes causes a moderate 2.38% decrease in accuracy for ResNet-32."
        ],
        [
            "Via Swift technology, optical neural networks (ONNs) are acknowledged for their fast processing speeds, substantial data transfer capabilities, and minimal power consumption. A primary advantage of ONNs lies in their capacity to expedite deep learning computations. Nevertheless, current ONN architectures predominantly attend to regulating light intensity, disregarding the significance of light phases as conveyors of computational data. In our research, we propose a novel method, named Oplixnet, to augment ONNs by encoding input data through the amplitude and phase attributes of light impulses. The ONN blueprint leverages amplitude and phase fluctuations in input and output zones. Moreover, we integrate collaborative learning among several ONN models to preserve precision. Our results exhibit that Oplixnet effectively reduces ONN dimensions with minimal effect on performance. For instance, a 75.03% reduction in dimensions results in a negligible 0.33% decrease in accuracy for fully connected neural networks (FCNNs), while a 74.88% reduction in dimensions induces a moderate 2.38% decrease in accuracy for ResNet-32."
        ],
        [
            "Using Swift technology, optical neural networks (ONNs) are recognized for their quick processing speeds, large data transfer capacities, and low power consumption. One major advantage of ONNs is their ability to accelerate deep learning computations. However, most existing ONN architectures mainly focus on controlling light intensity and overlook the importance of light phases as carriers of computational information. In our study, we suggest a new approach, called Oplixnet, to enhance ONNs by encoding input data using the amplitude and phase characteristics of light pulses. The ONN design employs fluctuations in amplitude and phase at input and output stages. Additionally, we incorporate collaborative learning among multiple ONN models to maintain accuracy. Our findings demonstrate that Oplixnet efficiently reduces ONN dimensions without significantly impacting performance. For instance, a 75.03% reduction in dimensions leads to a negligible 0.33% drop in accuracy for fully connected neural networks (FCNNs), and a 74.88% reduction in dimensions causes a moderate 2.38% decrease in accuracy for ResNet-32."
        ],
        [
            "Employing Swift technology, optical neural networks (ONNs) are renowned for their fast processing speeds, substantial data transfer capacities, and minimal power consumption. A significant benefit of ONNs lies in their capacity to expedite deep learning computations. Nevertheless, current ONN architectures predominantly concentrate on managing light intensity and disregard the significance of light phases as conveyors of computational information. In our research, we propose a novel method, named Oplixnet, to augment ONNs by encoding input data utilizing the amplitude and phase attributes of light impulses. The ONN architecture incorporates fluctuations in amplitude and phase at input and output stages. Furthermore, we integrate collaborative learning among numerous ONN models to uphold precision. Our results reveal that Oplixnet effectively diminishes ONN dimensions with minimal influence on performance. For instance, a 75.03% decrease in dimensions results in an insignificant 0.33% reduction in accuracy for fully connected neural networks (FCNNs), and a 74.88% reduction in dimensions causes a moderate 2.38% decline in accuracy for ResNet-32."
        ],
        [
            "Using Swift technology, optical neural networks (ONNs) are recognized for their rapid processing speeds, large data transfer capabilities, and low power consumption. One major advantage of ONNs is their ability to accelerate deep learning computations. However, existing ONN designs primarily focus on managing light intensity and overlook the importance of light phases as carriers of computational information. In our study, we suggest a new approach, named Oplixnet, to enhance ONNs by encoding input data using the amplitude and phase properties of light pulses. The ONN architecture incorporates variations in amplitude and phase at input and output points. Additionally, we incorporate cooperative learning among multiple ONN models to maintain precision. Our findings demonstrate that Oplixnet reduces ONN sizes significantly with minimal impact on performance. For instance, a 75.03% decrease in size results in a negligible 0.33% decrease in accuracy for fully connected neural networks (FCNNs), and a 74.88% reduction in size causes a moderate 2.38% drop in accuracy for ResNet-32."
        ],
        [
            "Employing Swift technology, optical neural networks (ONNs) are renowned for their quick processing speeds, substantial data transfer capacities, and minimal power consumption. One notable advantage of ONNs is their capacity to expedite deep learning computations. Nevertheless, current ONN architectures predominantly concentrate on managing light intensity, disregarding the significance of light phases as bearers of computational data. In our investigation, we propose a novel method, titled Oplixnet, to boost ONNs by encoding input data utilizing the amplitude and phase characteristics of light pulses. The ONN design integrates fluctuations in amplitude and phase at input and output junctures. Furthermore, we implement cooperative learning among several ONN models to uphold precision. Our research indicates that Oplixnet substantially diminishes ONN dimensions without adversely affecting performance. For example, a 75.03% reduction in size leads to an insignificant 0.33% decrease in accuracy for fully connected neural networks (FCNNs), and a 74.88% reduction in size incurs a moderate 2.38% decline in accuracy for ResNet-32."
        ],
        [
            "Using Swift technology, optical neural networks (ONNs) are recognized for their fast processing speeds, large data transfer capabilities, and low power consumption. One major advantage of ONNs is their ability to accelerate deep learning computations. However, existing ONN architectures primarily focus on managing light intensity, neglecting the importance of light phases as carriers of computational information. In our study, we propose a new method, named Oplixnet, to enhance ONNs by encoding input data based on the amplitude and phase properties of light pulses. The ONN architecture incorporates variations in amplitude and phase at input and output interfaces. Additionally, we employ cooperative learning among multiple ONN models to maintain accuracy. Our findings suggest that Oplixnet significantly reduces ONN dimensions without negatively impacting performance. For instance, a 75.03% reduction in size results in an insignificant 0.33% decrease in accuracy for fully connected neural networks (FCNNs), and a 74.88% reduction in size causes a moderate 2.38% decline in accuracy for ResNet-32."
        ],
        [
            "Employing Swift technology, optical neural networks (ONNs) are renowned for their swift processing speeds, substantial data transfer capacities, and minimal power consumption. A significant advantage of ONNs lies in their capability to expedite deep learning computations. Nevertheless, current ONN architectures predominantly concentrate on regulating light intensity, disregarding the significance of light phases as conveyors of computational data. In our investigation, we present a novel strategy, referred to as Oplixnet, to augment ONNs by encoding input data based on the amplitude and phase attributes of light impulses. The ONN design integrates fluctuations in amplitude and phase at input and output terminals. Furthermore, we utilize collaborative learning among several ONN models to preserve precision. Our results indicate that Oplixnet appreciably diminishes ONN dimensions without impairing performance. For example, a reduction of 75.03% in dimensions leads to an insignificant 0.33% drop in accuracy for fully connected neural networks (FCNNs), while a 74.88% reduction in dimensions causes a moderate 2.38% decrease in accuracy for ResNet-32."
        ],
        [
            "Using Swift technology, optical neural networks (ONNs) are known for their quick processing speeds, large data transfer capabilities, and low power consumption. One major advantage of ONNs is their ability to accelerate deep learning calculations. However, existing ONN architectures mainly focus on controlling light intensity, ignoring the role of light phases as carriers of computational information. In this study, we propose a new method, called Oplixnet, to enhance ONNs by encoding input data based on the amplitude and phase properties of light pulses. The ONN architecture includes variations in amplitude and phase at input and output ports. Additionally, we employ collaborative learning among multiple ONN models to maintain accuracy. Our findings demonstrate that Oplixnet substantially reduces ONN sizes without impacting performance. For instance, a 75.03% decrease in size results in a negligible 0.33% drop in accuracy for fully connected neural networks (FCNNs), while a 74.88% reduction in size causes a moderate 2.38% decline in accuracy for ResNet-32."
        ],
        [
            "With Swift technology, optical neural networks (ONNs) are renowned for their rapid processing speeds, substantial data transfer capacities, and minimal power consumption. A significant advantage of ONNs is their capability to expedite deep learning computations. Nevertheless, current ONN structures predominantly concentrate on managing light intensity, disregarding the function of light phases as conveyors of computational information. In this research, we present a novel approach, titled Oplixnet, to augment ONNs by encoding input data according to the amplitude and phase characteristics of light pulses. The ONN design features fluctuations in amplitude and phase at input and output terminals. Furthermore, we utilize cooperative learning among numerous ONN models to uphold precision. Our results reveal that Oplixnet substantially lessens ONN dimensions without influencing performance. For example, a 75.03% reduction in size brings about an insignificant 0.33% decrease in accuracy for fully connected neural networks (FCNNs), while a 74.88% reduction in size results in a moderate 2.38% drop in accuracy for ResNet-32."
        ],
        [
            "Utilizing Swift technology, optical neural networks (ONNs) are recognized for their swift processing speeds, generous data transfer capabilities, and meager power consumption. One notable advantage of ONNs is their potential to accelerate deep learning calculations. However, present ONN architectures primarily focus on regulating light intensity, neglecting the role of light phases as carriers of computational data. In our study, we introduce a novel strategy, named Oplixnet, to enhance ONNs by encoding input data based on the amplitude and phase properties of light pulses. The ONN architecture incorporates amplitude and phase fluctuations at input and output interfaces. Additionally, we employ collaborative learning among several ONN models to maintain accuracy. Our findings indicate that Oplixnet substantially reduces ONN dimensions without compromising performance. For instance, a 75.03% decrease in size leads to a negligible 0.33% reduction in accuracy for fully connected neural networks (FCNNs), whereas a 74.88% reduction in size causes a moderate 2.38% drop in accuracy for ResNet-32."
        ],
        [
            "Employing Swift technology, optical neural networks (ONNs) are known for their fast processing speeds, ample data transfer capabilities, and minimal power consumption. One significant advantage of ONNs is their capability to expedite deep learning computations. Nevertheless, current ONN architectures predominantly concentrate on controlling light intensity, disregarding the function of light phases as conveyors of computational data. In our research, we propose a new method, referred to as Oplixnet, to improve ONNs by encoding input data based on the amplitude and phase characteristics of light impulses. The ONN design integrates amplitude and phase variations at input and output ports. Furthermore, we implement cooperative learning among various ONN models to preserve precision. Our results suggest that Oplixnet significantly minimizes ONN dimensions without impairing performance. For example, a 75.03% reduction in size results in an insignificant 0.33% decrease in accuracy for fully connected neural networks (FCNNs), whereas a 74.88% reduction in size causes a moderate 2.38% decline in accuracy for ResNet-32."
        ]
    ]
}